[{
  "_id": {
    "$oid": "5bbf07a2b79d666cbb21b76d"
  },
  "message_id": "<JIRA.13155988.1525014170000.11830.1525014180069@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0740b79d666cbb21a25d"
    },
    {
      "$oid": "5bbf0740b79d666cbb21a25e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0740b79d666cbb21a25d"
  },
  "from_id": {
    "$oid": "5bbf070b57674ee1673023ab"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (KAFKA-6835) Enable topic unclean leader election\n to be enabled without controller change",
  "body": "Rajini Sivaram created KAFKA-6835:\n-------------------------------------\n\n             Summary: Enable topic unclean leader election to be enabled without controller change\n                 Key: KAFKA-6835\n                 URL: https://issues.apache.org/jira/browse/KAFKA-6835\n             Project: Kafka\n          Issue Type: Task\n          Components: core\n            Reporter: Rajini Sivaram\n\n\nDynamic update of broker's default unclean.leader.election.enable will be processed without controller change (KAFKA-6526). We should probably do the same for topic overrides as well.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-04-29T15:03:00Z"
  },
  "patches": [],
  "external_id": "KAFKA-6835"
},{
  "_id": {
    "$oid": "5bbe0e39272f7b1f6830c0c8"
  },
  "message_id": "<JIRA.13181965.1535566442000.189539.1535571000618@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e12272f7b1f6830bd59"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0e39272f7b1f6830c0c4"
    },
    {
      "$oid": "5bbe0e39272f7b1f6830c0c5"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0e39272f7b1f6830c0c4"
  },
  "from_id": {
    "$oid": "5bbe0e2857674ee167917572"
  },
  "to_ids": [
    {
      "$oid": "5bbe0e2757674ee1679173c5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PARQUET-1406) unit test fails on some cases",
  "body": "\n    [ https://issues.apache.org/jira/browse/PARQUET-1406?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16596768#comment-16596768 ] \n\nWes McKinney commented on PARQUET-1406:\n---------------------------------------\n\nCan you post the output of {{ctest -VV}}? \n\n> unit test fails on some cases\n> -----------------------------\n>\n>                 Key: PARQUET-1406\n>                 URL: https://issues.apache.org/jira/browse/PARQUET-1406\n>             Project: Parquet\n>          Issue Type: Test\n>          Components: parquet-cpp\n>    Affects Versions: 1.5.0\n>         Environment: Ubuntu Linux: 18.04\n> gcc: 7.3.0\n>            Reporter: Peter Pan\n>            Priority: Critical\n>             Fix For: 1.5.0\n>\n>\n> Hi, i'm trying to integrate parquet-cpp 1.5.0rc0 with arrow 0.10.0 and thrift 0.11.0 but experiencing unit test failure as follows:\n> {code:java}\n> Test project /mnt/tmp/parquet-cpp-apache-parquet-cpp-1.5.0-rc0/build\n>       Start  1: bloom_filter-test\n> 1/18 Test  #1: bloom_filter-test ................***Failed   18.69 sec\n>       Start  2: column_reader-test\n> 2/18 Test  #2: column_reader-test ...............   Passed    0.07 sec\n>       Start  3: column_scanner-test\n> 3/18 Test  #3: column_scanner-test ..............   Passed    0.08 sec\n>       Start  4: column_writer-test\n> 4/18 Test  #4: column_writer-test ...............***Failed    4.14 sec\n>       Start  5: file-deserialize-test\n> 5/18 Test  #5: file-deserialize-test ............***Failed    0.04 sec\n>       Start  6: file-serialize-test\n> 6/18 Test  #6: file-serialize-test ..............***Failed    0.11 sec\n>       Start  7: properties-test\n> 7/18 Test  #7: properties-test ..................   Passed    0.03 sec\n>       Start  8: statistics-test\n> 8/18 Test  #8: statistics-test ..................   Passed    0.10 sec\n>       Start  9: encoding-test\n> 9/18 Test  #9: encoding-test ....................   Passed    0.07 sec\n>       Start 10: metadata-test\n> 10/18 Test #10: metadata-test ....................   Passed    0.03 sec\n>       Start 11: public-api-test\n> 11/18 Test #11: public-api-test ..................   Passed    0.03 sec\n>       Start 12: types-test\n> 12/18 Test #12: types-test .......................   Passed    0.03 sec\n>       Start 13: reader-test\n> 13/18 Test #13: reader-test ......................***Failed    0.03 sec\n>       Start 14: schema-test\n> 14/18 Test #14: schema-test ......................   Passed    0.03 sec\n>       Start 15: arrow-schema-test\n> 15/18 Test #15: arrow-schema-test ................   Passed    0.03 sec\n>       Start 16: arrow-reader-writer-test\n> 16/18 Test #16: arrow-reader-writer-test .........***Failed    7.63 sec\n>       Start 17: comparison-test\n> 17/18 Test #17: comparison-test ..................   Passed    0.03 sec\n>       Start 18: memory-test\n> 18/18 Test #18: memory-test ......................   Passed    0.03 sec\n> 67% tests passed, 6 tests failed out of 18\n> {code}\n> Any idea to fix these failures?\n> BTW, i needed to *manually* replace references to `AllocateEmptyBitmap` with `GetEmptyBitmap` to make this rc build successfully before i could do this unit test. Can it be fixed as well?\n>  \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-08-29T19:30:00Z"
  },
  "patches": [],
  "external_id": "PARQUET-1406"
},{
  "_id": {
    "$oid": "5bea9ecc9e73d744d4130e8c"
  },
  "message_id": "<7085305.1156163475953.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9e8a9e73d744d412fe1c"
  },
  "from_id": {
    "$oid": "5bea9e8735e3ea2b7b5e2a92"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (DERBY-1559) when receiving a single EXTDTA\n object representing a BLOB, the server do not need to read it into memory\n before inserting it into the DB",
  "body": "    [ http://issues.apache.org/jira/browse/DERBY-1559?page=comments#action_12429406 ] \n            \nAndreas Korneliussen commented on DERBY-1559:\n---------------------------------------------\n\nI will wait for DERBY-1610 being fixed, to avoid the risk.\n\n\n> when receiving a single EXTDTA object representing a BLOB, the server do not need to read it into memory before inserting it into the DB\n> ----------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-1559\n>                 URL: http://issues.apache.org/jira/browse/DERBY-1559\n>             Project: Derby\n>          Issue Type: Sub-task\n>          Components: Network Server\n>    Affects Versions: 10.2.1.0, 10.3.0.0, 10.2.2.0\n>            Reporter: Andreas Korneliussen\n>         Assigned To: Andreas Korneliussen\n>         Attachments: DERBY-1559.diff, DERBY-1559.stat, DERBY-1559v2.diff, serverMemoryUsage.xls\n>\n>\n> When streaming a BLOB from the Network Client to the Network Server, the Network server currently read all the data from the stream and put it into a byte array.\n> The blob data is then inserted into the DB by using\n> PreparedStatement.setBytes(..)\n> and later\n> PreparedStatement.execute()\n> To avoid OutOfMemoryError if the size of the Blob is > than total memory in the VM, we could make the network server create a stream which reads data when doing PreparedStatement.execute().  The DB will then stream the BLOB data directly from the network inputstream into the disk.\n> I intent to make a patch which does this if there is only one EXTDTA object (BLOB) sent from the client in the statement, as it will simplify the implementation. Later this can be improved  further to include CLOBs, and possibly to include the cases where there are multiple EXTDTA objects.\n> --\n> CLOBs are more complex, as there need to be some character encoding. This can be achieved by using a InputStreamReader,  and use PreparedStatement.setCharacterStream(..). However the size of the stream is not necessarily the same as the size of the raw binary data, and to do this for CLOBs, I would need the embedded prepared statements to support the new setCharacterStream() overloads in JDBC4 (which do not include a length atribute)\n> --\n> Multiple EXTDATA objects are also more complex, since one would need to have fully read the previous object ,before it is possible to read the next.\n> --\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2006-08-21T05:31:15Z"
  },
  "patches": [],
  "external_id": "DERBY-1559"
},{
  "_id": {
    "$oid": "5bbe1048b1ffc5570d040681"
  },
  "message_id": "<JIRA.12742886.1411168765000.142560.1411940254010@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e4eb1ffc5570d03b7b4"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0fe9b1ffc5570d03f4ca"
    },
    {
      "$oid": "5bbe0fe9b1ffc5570d03f4cb"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0fe9b1ffc5570d03f4ca"
  },
  "from_id": {
    "$oid": "5bacc4c457674ee167dd6de5"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (TIKA-1420) Add Metadata Extraction to Arbitrary\n Parsers",
  "body": "\n    [ https://issues.apache.org/jira/browse/TIKA-1420?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14151232#comment-14151232 ] \n\nTyler Palsulich commented on TIKA-1420:\n---------------------------------------\n\nThanks [~gagravarr] and [~chrismattmann]. I'll move them over tomorrow, resolve this issue, and close the RB.\n\n> Add Metadata Extraction to Arbitrary Parsers\n> --------------------------------------------\n>\n>                 Key: TIKA-1420\n>                 URL: https://issues.apache.org/jira/browse/TIKA-1420\n>             Project: Tika\n>          Issue Type: Improvement\n>          Components: parser\n>            Reporter: Tyler Palsulich\n>            Priority: Minor\n>\n> Suppose you wish to extract information from arbitrary file types and add it to a Metadata Object. This type of task is best handled by a... Handler. But, Handlers do not have access to the Metadata Object passed to a Parser. \n> So, I see a few ways we could do using existing functionality.\n> 1) Make an intermediate XML representation of the desired metadata in a handler, then convert the XML to the Metadata after parsing. \n> 2) Create a second Parser which extracts the desired information.\n>      a) Assume the Handler passed to this Parser is already filled with content. So, we could simply get whatever content from the Handler and populate the Metadata directly.\n>      b) Create a new Stream in the first Parser to pass to the second, which in turn populates the Metadata.\n> None of these options seem ideal. Is there a better way to handle this scenario? Or, can we create some sort of... wrapper for a Handler which can accept a Metadata Object to populate directly? \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-09-28T21:37:34Z"
  },
  "patches": [],
  "external_id": "TIKA-1420"
},{
  "_id": {
    "$oid": "5bacc6ac56f6a00b0209cd69"
  },
  "message_id": "<226846221.1244634967651.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacc6a556f6a00b0209cb0f"
  },
  "from_id": {
    "$oid": "59677a5daff2204b3cbd1359"
  },
  "to_ids": [
    {
      "$oid": "58c11cef02ca40f8bfb1f60d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (MAHOUT-121) Speed up distance calculations for\n sparse vectors",
  "body": "\n    [ https://issues.apache.org/jira/browse/MAHOUT-121?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12718043#action_12718043 ] \n\nBenson Margulies commented on MAHOUT-121:\n-----------------------------------------\n\nTwo arrays would indeed model all the high-speed C++ sparse vectors I have met. One optimization is to allow insertion without maintaining order and then resorting for algorithms that need to do many inserts in a row without reading.\n\n\n> Speed up distance calculations for sparse vectors\n> -------------------------------------------------\n>\n>                 Key: MAHOUT-121\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-121\n>             Project: Mahout\n>          Issue Type: Improvement\n>          Components: Matrix\n>            Reporter: Shashikant Kore\n>         Attachments: mahout-121.patch\n>\n>\n> From my mail to the Mahout mailing list.\n> I am working on clustering a dataset which has thousands of sparse vectors. The complete dataset has few tens of thousands of feature items but each vector has only couple of hundred feature items. For this, there is an optimization in distance calculation, a link to which I found the archives of Mahout mailing list.\n> http://lingpipe-blog.com/2009/03/12/speeding-up-k-means-clustering-algebra-sparse-vectors/\n> I tried out this optimization.  The test setup had 2000 document  vectors with few hundred items.  I ran canopy generation with Euclidean distance and t1, t2 values as 250 and 200.\n>  \n> Current Canopy Generation: 28 min 15 sec.\n> Canopy Generation with distance optimization: 1 min 38 sec.\n> I know by experience that using Integer, Double objects instead of primitives is computationally expensive. I changed the sparse vector  implementation to used primitive collections by Trove [\n> http://trove4j.sourceforge.net/ ].\n> Distance optimization with Trove: 59 sec\n> Current canopy generation with Trove: 21 min 55 sec\n> To sum, these two optimizations reduced cluster generation time by a 97%.\n> Currently, I have made the changes for Euclidean Distance, Canopy and KMeans.  \n> Licensing of Trove seems to be an issue which needs to be addressed.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-06-10T04:56:07Z"
  },
  "patches": [],
  "external_id": "MAHOUT-121"
},{
  "_id": {
    "$oid": "5f27bc9aa9368823397d8cbe"
  },
  "message_id": "<JIRA.12706636.1396620897323.65247.1396631305323@arcas>",
  "mailing_list_id": {
    "$oid": "5f27bb47a9368823397d2ab3"
  },
  "reference_ids": [
    {
      "$oid": "5f27bc9aa9368823397d8cbb"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bc9aa9368823397d8cbb"
  },
  "from_id": {
    "$oid": "596779fcaff2204b3cbd12a7"
  },
  "to_ids": [
    {
      "$oid": "5bbe40b857674ee167452f7d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JENA-671) Filter placement of (sequence) does\n not consider last op in sequence.",
  "body": "\n    [ https://issues.apache.org/jira/browse/JENA-671?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13960133#comment-13960133 ] \n\nAndy Seaborne commented on JENA-671:\n------------------------------------\n\nIn improving this, a number of tests change to do with replacing \"filter ?x = :x\" with assignments. These look OK in the new scheme noting that {{(assign)}}, unlike {{(extend)}} can act as a filter as well when {{?x}} is bound on input.\n\n> Filter placement of (sequence) does not consider last op in sequence.\n> ---------------------------------------------------------------------\n>\n>                 Key: JENA-671\n>                 URL: https://issues.apache.org/jira/browse/JENA-671\n>             Project: Apache Jena\n>          Issue Type: Bug\n>          Components: ARQ\n>    Affects Versions: Jena 2.11.1\n>            Reporter: Andy Seaborne\n>            Assignee: Andy Seaborne\n>            Priority: Minor\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-04-04T17:08:25Z"
  },
  "patches": [],
  "external_id": "JENA-671"
},{
  "_id": {
    "$oid": "5bf68a17f36e975afa4e446b"
  },
  "message_id": "<JIRA.12683539.1386560586870.1301.1386578947932@arcas>",
  "mailing_list_id": {
    "$oid": "5bf68976f36e975afa4e36af"
  },
  "reference_ids": [
    {
      "$oid": "5bf68a17f36e975afa4e4468"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bf68a17f36e975afa4e4468"
  },
  "from_id": {
    "$oid": "5bf68a1735e3ea2b7b1c813a"
  },
  "to_ids": [
    {
      "$oid": "5bf6898e35e3ea2b7b1be76d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (GIRAPH-813) Compilation problems with Hadoop\n 2.2.0",
  "body": "\n    [ https://issues.apache.org/jira/browse/GIRAPH-813?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13842981#comment-13842981 ] \n\nPlacek commented on GIRAPH-813:\n-------------------------------\n\nThis doesn't work for me. I get the following errors:\n\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Apache Giraph Distribution 1.1.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ giraph-dist ---\n[INFO] \n[INFO] --- mavanagaiata:0.5.0:branch (git-commit) @ giraph-dist ---\n[INFO] \n[INFO] --- mavanagaiata:0.5.0:commit (git-commit) @ giraph-dist ---\n[INFO] \n[INFO] --- maven-assembly-plugin:2.4:single (assemble) @ giraph-dist ---\n[INFO] Reading assembly descriptor: src/main/assembly/bin.xml\n[WARNING] Missing POM for org.apache.giraph:giraph-hbase:jar:1.1.0-SNAPSHOT\n[WARNING] Missing POM for org.apache.giraph:giraph-accumulo:jar:1.1.0-SNAPSHOT\n[WARNING] Missing POM for org.apache.giraph:giraph-rexster:jar:1.1.0-SNAPSHOT\n[WARNING] Missing POM for org.apache.giraph:giraph-hcatalog:jar:1.1.0-SNAPSHOT\n[WARNING] Missing POM for org.apache.giraph:giraph-hive:jar:1.1.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Apache Giraph Parent .............................. SUCCESS [2.041s]\n[INFO] Apache Giraph Core ................................ SUCCESS [57.743s]\n[INFO] Apache Giraph Examples ............................ SUCCESS [17.641s]\n[INFO] Apache Giraph Distribution ........................ FAILURE [0.348s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1:18.273s\n[INFO] Finished at: Mon Dec 09 09:41:33 CET 2013\n[INFO] Final Memory: 45M/405M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single (assemble) on project giraph-dist: Failed to create assembly: Unable to resolve dependencies for assembly 'bin': Failed to resolve dependencies for assembly: Missing:\n[ERROR] ----------\n[ERROR] 1) org.apache.giraph:giraph-hbase:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] Try downloading the file manually from the project website.\n[ERROR] \n[ERROR] Then, install it using the command:\n[ERROR] mvn install:install-file -DgroupId=org.apache.giraph -DartifactId=giraph-hbase -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file\n[ERROR] \n[ERROR] Alternatively, if you host your own repository you can deploy the file there:\n[ERROR] mvn deploy:deploy-file -DgroupId=org.apache.giraph -DartifactId=giraph-hbase -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[ERROR] \n[ERROR] Path to dependency:\n[ERROR] 1) org.apache.giraph:giraph-dist:pom:1.1.0-SNAPSHOT\n[ERROR] 2) org.apache.giraph:giraph-hbase:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] 2) org.apache.giraph:giraph-rexster:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] Try downloading the file manually from the project website.\n[ERROR] \n[ERROR] Then, install it using the command:\n[ERROR] mvn install:install-file -DgroupId=org.apache.giraph -DartifactId=giraph-rexster -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file\n[ERROR] \n[ERROR] Alternatively, if you host your own repository you can deploy the file there:\n[ERROR] mvn deploy:deploy-file -DgroupId=org.apache.giraph -DartifactId=giraph-rexster -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[ERROR] \n[ERROR] Path to dependency:\n[ERROR] 1) org.apache.giraph:giraph-dist:pom:1.1.0-SNAPSHOT\n[ERROR] 2) org.apache.giraph:giraph-rexster:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] 3) org.apache.giraph:giraph-accumulo:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] Try downloading the file manually from the project website.\n[ERROR] \n[ERROR] Then, install it using the command:\n[ERROR] mvn install:install-file -DgroupId=org.apache.giraph -DartifactId=giraph-accumulo -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file\n[ERROR] \n[ERROR] Alternatively, if you host your own repository you can deploy the file there:\n[ERROR] mvn deploy:deploy-file -DgroupId=org.apache.giraph -DartifactId=giraph-accumulo -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[ERROR] \n[ERROR] Path to dependency:\n[ERROR] 1) org.apache.giraph:giraph-dist:pom:1.1.0-SNAPSHOT\n[ERROR] 2) org.apache.giraph:giraph-accumulo:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] 4) org.apache.giraph:giraph-hive:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] Try downloading the file manually from the project website.\n[ERROR] \n[ERROR] Then, install it using the command:\n[ERROR] mvn install:install-file -DgroupId=org.apache.giraph -DartifactId=giraph-hive -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file\n[ERROR] \n[ERROR] Alternatively, if you host your own repository you can deploy the file there:\n[ERROR] mvn deploy:deploy-file -DgroupId=org.apache.giraph -DartifactId=giraph-hive -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[ERROR] \n[ERROR] Path to dependency:\n[ERROR] 1) org.apache.giraph:giraph-dist:pom:1.1.0-SNAPSHOT\n[ERROR] 2) org.apache.giraph:giraph-hive:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] 5) org.apache.giraph:giraph-hcatalog:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] Try downloading the file manually from the project website.\n[ERROR] \n[ERROR] Then, install it using the command:\n[ERROR] mvn install:install-file -DgroupId=org.apache.giraph -DartifactId=giraph-hcatalog -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file\n[ERROR] \n[ERROR] Alternatively, if you host your own repository you can deploy the file there:\n[ERROR] mvn deploy:deploy-file -DgroupId=org.apache.giraph -DartifactId=giraph-hcatalog -Dversion=1.1.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[ERROR] \n[ERROR] Path to dependency:\n[ERROR] 1) org.apache.giraph:giraph-dist:pom:1.1.0-SNAPSHOT\n[ERROR] 2) org.apache.giraph:giraph-hcatalog:jar:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] ----------\n[ERROR] 5 required artifacts are missing.\n[ERROR] \n[ERROR] for artifact:\n[ERROR] org.apache.giraph:giraph-dist:pom:1.1.0-SNAPSHOT\n[ERROR] \n[ERROR] from the specified remote repositories:\n[ERROR] central (http://repo1.maven.org/maven2, releases=true, snapshots=true),\n[ERROR] cloudera cdh (https://repository.cloudera.com/artifactory/cloudera-repos, releases=true, snapshots=false),\n[ERROR] apache (https://repository.apache.org/content/groups/public/, releases=true, snapshots=true),\n[ERROR] sonatype (https://oss.sonatype.org/content/groups/public/, releases=true, snapshots=true)\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :giraph-dist\n\n\n> Compilation problems with Hadoop 2.2.0\n> --------------------------------------\n>\n>                 Key: GIRAPH-813\n>                 URL: https://issues.apache.org/jira/browse/GIRAPH-813\n>             Project: Giraph\n>          Issue Type: Bug\n>          Components: build\n>    Affects Versions: 1.1.0\n>         Environment: >hadoop version\n> Hadoop 2.2.0\n> Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768\n> Compiled by hortonmu on 2013-10-07T06:28Z\n> Compiled with protoc 2.5.0\n> From source with checksum 79e53ce7994d1628b240f09af91e1af4\n>            Reporter: Placek\n>\n> Running the code:\n> -----------------------\n> git clone git://git.apache.org/giraph.git snapshot_from_git\n> cd snapshot_from_git\n> mvn -Phadoop_yarn -Dhadoop.version=2.2.0 clean package -DskipTests\n> Produces:\n> -------------\n> Cloning into 'snaphot_from_git'...\n> remote: Counting objects: 16809, done.\n> remote: Compressing objects: 100% (7476/7476), done.\n> remote: Total 16809 (delta 9496), reused 11044 (delta 6199)\n> Receiving objects: 100% (16809/16809), 21.95 MiB | 4.39 MiB/s, done.\n> Resolving deltas: 100% (9496/9496), done.\n> [INFO] Scanning for projects...\n> [INFO] ------------------------------------------------------------------------\n> [INFO] Reactor Build Order:\n> [INFO] \n> [INFO] Apache Giraph Parent\n> [INFO] Apache Giraph Core\n> [INFO] Apache Giraph Examples\n> [INFO] Apache Giraph Distribution\n> [INFO]                                                                         \n> [INFO] ------------------------------------------------------------------------\n> [INFO] Building Apache Giraph Parent 1.1.0-SNAPSHOT\n> [INFO] ------------------------------------------------------------------------\n> [INFO] \n> [INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ giraph-parent ---\n> [INFO] \n> [INFO] --- mavanagaiata:0.5.0:branch (git-commit) @ giraph-parent ---\n> [INFO] \n> [INFO] --- mavanagaiata:0.5.0:commit (git-commit) @ giraph-parent ---\n> [INFO] \n> [INFO] --- maven-enforcer-plugin:1.2:enforce (default) @ giraph-parent ---\n> [INFO] Skipping Rule Enforcement.\n> [INFO] \n> [INFO] --- maven-dependency-versions-check-plugin:2.0.2:check (default) @ giraph-parent ---\n> [INFO] Checking dependency versions\n> [ERROR] Found a problem with the dependency commons-httpclient:commons-httpclient\n>   Resolved version is 3.0.1\n>   Version 3.1 was expected by artifact: org.apache.hadoop:hadoop-common\n> [ERROR] Found a problem with the dependency commons-logging:commons-logging\n>   Resolved version is 1.0.4\n>   Version 1.1.1 was expected by artifact: org.apache.hadoop:hadoop-common\n> [ERROR] Found a problem with the dependency io.netty:netty\n>   Resolved version is 3.5.3.Final\n>   Version 3.6.2.Final was expected by artifacts: org.apache.hadoop:hadoop-mapreduce-client-common, org.apache.hadoop:hadoop-mapreduce-client-core, org.apache.hadoop:hadoop-yarn-common, org.apache.hadoop:hadoop-yarn-server-common, org.apache.hadoop:hadoop-yarn-server-nodemanager, org.apache.hadoop:hadoop-yarn-server-resourcemanager, org.apache.hadoop:hadoop-yarn-server-tests:test-jar\n> [ERROR] Found a problem with the dependency org.apache.zookeeper:zookeeper\n>   Resolved version is 3.3.3\n>   Version 3.4.5 was expected by artifacts: org.apache.hadoop:hadoop-common, org.apache.hadoop:hadoop-mapreduce-client-common, org.apache.hadoop:hadoop-yarn-server-common, org.apache.hadoop:hadoop-yarn-server-nodemanager, org.apache.hadoop:hadoop-yarn-server-resourcemanager, org.apache.hadoop:hadoop-yarn-server-tests:test-jar\n> [INFO] ------------------------------------------------------------------------\n> [INFO] Reactor Summary:\n> [INFO] \n> [INFO] Apache Giraph Parent .............................. FAILURE [14.980s]\n> [INFO] Apache Giraph Core ................................ SKIPPED\n> [INFO] Apache Giraph Examples ............................ SKIPPED\n> [INFO] Apache Giraph Distribution ........................ SKIPPED\n> [INFO] ------------------------------------------------------------------------\n> [INFO] BUILD FAILURE\n> [INFO] ------------------------------------------------------------------------\n> [INFO] Total time: 16.684s\n> [INFO] Finished at: Mon Dec 09 04:42:08 CET 2013\n> [INFO] Final Memory: 23M/334M\n> [INFO] ------------------------------------------------------------------------\n> [ERROR] Failed to execute goal com.ning.maven.plugins:maven-dependency-versions-check-plugin:2.0.2:check (default) on project giraph-parent: Found dependency version conflicts -> [Help 1]\n> [ERROR] \n> [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n> [ERROR] Re-run Maven using the -X switch to enable full debug logging.\n> [ERROR] \n> [ERROR] For more information about the errors and possible solutions, please read the following articles:\n> [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.4#6159)\n",
  "date": {
    "$date": "2013-12-09T08:49:07Z"
  },
  "patches": [],
  "external_id": "GIRAPH-813"
},{
  "_id": {
    "$oid": "5bea982a9e73d744d411b584"
  },
  "message_id": "<JIRA.12720667.1402514935943.81508.1404232705406@arcas>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [
    {
      "$oid": "5bea982a9e73d744d411b583"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bea982a9e73d744d411b583"
  },
  "from_id": {
    "$oid": "58c9de5702ca40f8bf20c1ae"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DERBY-6611) Broken link in API docs to\n derby.drda.keepAlive documentation",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-6611?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14049036#comment-14049036 ] \n\nASF subversion and git services commented on DERBY-6611:\n--------------------------------------------------------\n\nCommit 1607135 from [~dagw] in branch 'code/trunk'\n[ https://svn.apache.org/r1607135 ]\n\nDERBY-6611 Broken link in API docs to derby.drda.keepAlive documentation\n\nJavadoc fixup for this issue: Use textual cross reference instead.\n\n> Broken link in API docs to derby.drda.keepAlive documentation\n> -------------------------------------------------------------\n>\n>                 Key: DERBY-6611\n>                 URL: https://issues.apache.org/jira/browse/DERBY-6611\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Documentation\n>    Affects Versions: 10.10.2.0\n>            Reporter: Dag H. Wanvik\n>             Fix For: 10.11.0.0\n>\n>\n> In the javadoc for NetworkServerMBean#getDrdaKeepAlive, there is a\n> @see link which is broken. It points to:\n> http://db.apache.org/derby/docs/dev/adminguide/radmindrdakeepalive.html\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-07-01T16:38:25Z"
  },
  "patches": [],
  "external_id": "DERBY-6611"
},{
  "_id": {
    "$oid": "5bbf074bb79d666cbb21a448"
  },
  "message_id": "<JIRA.13169614.1530561537000.57629.1530561540307@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0747b79d666cbb21a377"
    },
    {
      "$oid": "5bbf0747b79d666cbb21a376"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0747b79d666cbb21a376"
  },
  "from_id": {
    "$oid": "5bbf071057674ee167302b9c"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (KAFKA-7128) Lagging high watermark can lead to\n committed data loss after ISR expansion",
  "body": "Jason Gustafson created KAFKA-7128:\n--------------------------------------\n\n             Summary: Lagging high watermark can lead to committed data loss after ISR expansion\n                 Key: KAFKA-7128\n                 URL: https://issues.apache.org/jira/browse/KAFKA-7128\n             Project: Kafka\n          Issue Type: Bug\n            Reporter: Jason Gustafson\n            Assignee: Jason Gustafson\n\n\nSome model checking exposed a weakness in the ISR expansion logic. We know that the high watermark can go backwards after a leader failover, but we may not have known that this can lead to the loss of committed data. \n\nSay we have three replicas: r1, r2, and r3. Initially, the ISR consists of (r1, r2) and the leader is r1. r3 is a new replica which has not begun fetching. The data up to offset 10 has been committed to the ISR. Here is the initial state:\n\nISR: (r1, r2)\nLeader: r1\nr1: [hw=10, leo=10]\nr2: [hw=5, leo=10]\nr3: [hw=0, leo=0]\n\nReplica 1 then initiates shutdown (or fails) and leaves the ISR, which makes r2 the new leader. The high watermark is still lagging r1.\n\nISR: (r2)\nLeader: r2\nr1 (offline): [hw=10, leo=10]\nr2: [hw=5, leo=10]\nr3: [hw=0, leo=0]\n\nReplica 3 then catch up to the high watermark on r2 and joins the ISR. Perhaps it's high watermark is lagging behind r2, but this is unimportant.\n\nISR: (r2, r3)\nLeader: r2\nr1 (offline): [hw=10, leo=10]\nr2: [hw=5, leo=10]\nr3: [hw=0, leo=5]\n\nNow r2 fails and r3 is elected leader and is the only member of the ISR. The committed data from offsets 5 to 10 has been lost.\n\nISR: (r3)\nLeader: r3\nr1 (offline): [hw=10, leo=10]\nr2 (offline): [hw=5, leo=10]\nr3: [hw=0, leo=5]\n\nThe bug is the fact that we allowed r3 into the ISR after the local high watermark had been reached. Since the follower does not know the true high watermark for the previous leader's epoch, it should not allow a replica to join the ISR until it has caught up to an offset within its own epoch. \n\nNote this is related to https://cwiki.apache.org/confluence/display/KAFKA/KIP-207%3A+Offsets+returned+by+ListOffsetsResponse+should+be+monotonically+increasing+even+during+a+partition+leader+change\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-07-02T19:59:00Z"
  },
  "patches": [],
  "external_id": "KAFKA-7128"
},{
  "_id": {
    "$oid": "5c50146c6b85f47dd6c3b746"
  },
  "message_id": "<2091122226.316991263840654484.JavaMail.jira@brutus.apache.org>",
  "mailing_list_id": {
    "$oid": "5c5011fb6b85f47dd6c39f9c"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5c50146c6b85f47dd6c3b745"
  },
  "from_id": {
    "$oid": "5c50123f621a9a77b35f364f"
  },
  "to_ids": [
    {
      "$oid": "5c5012bb621a9a77b35fb11d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (JSPWIKI-538) Prepare a charter",
  "body": "\n    [ https://issues.apache.org/jira/browse/JSPWIKI-538?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12801898#action_12801898 ] \n\nHarry Metske commented on JSPWIKI-538:\n--------------------------------------\n\nDo we have to fill this template : https://svn.apache.org/repos/private/committers/board/templates/podling-tlp-resolution.txt ?\n\n\n\n> Prepare a charter\n> -----------------\n>\n>                 Key: JSPWIKI-538\n>                 URL: https://issues.apache.org/jira/browse/JSPWIKI-538\n>             Project: JSPWiki\n>          Issue Type: Task\n>            Reporter: Janne Jalkanen\n>             Fix For: Graduating\n>\n>\n> We need to prepare a charter, since JSPWiki should (probably) graduate as a top-level project.\n> http://incubator.apache.org/guides/graduation.html#tlp-resolution\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-01-18T18:50:54Z"
  },
  "patches": [],
  "external_id": "JSPWIKI-538"
},{
  "_id": {
    "$oid": "5f27b705641061285051e622"
  },
  "message_id": "<JIRA.13276518.1577405709000.378535.1577405760015@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b6f5641061285051df9f"
    },
    {
      "$oid": "5f27b6f5641061285051dfa0"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b6f5641061285051df9f"
  },
  "from_id": {
    "$oid": "5f27b6f7af02e2d6de4fd444"
  },
  "to_ids": [
    {
      "$oid": "5bacb13357674ee167d59296"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PHOENIX-5651) IndexScrutiny does not handle\n TTL/row-expiry",
  "body": "Priyank Porwal created PHOENIX-5651:\n---------------------------------------\n\n             Summary: IndexScrutiny does not handle TTL/row-expiry\n                 Key: PHOENIX-5651\n                 URL: https://issues.apache.org/jira/browse/PHOENIX-5651\n             Project: Phoenix\n          Issue Type: Bug\n    Affects Versions: 4.14.3, 4.15.1\n            Reporter: Priyank Porwal\n\n\nIf a data-table has TTL on it, it's indexes inherit the TTL too. Hence when we run IndexScrutiny on such tables and it's indexes, scrutiny's attempts to find matching index rows for near-expiry data rows results in no-matches since the index row gets expired before the read from data-region mapper. The same happens in the MR job for the other direction Index->Data.\n\nThis does not impact correctness of indexing design, but makes it very inconvenient to get a clean scrutiny run. All reported invalid rows have to be matched against the table TTL, which is non-trivial exercise.\n\nIndexScrutiny itself could detect such expired rows when the matching pair is not found and not report them as INVALID_ROWS. Perhaps a new counter for EXPIRED_ROWS should be added as well for better visibility. \n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.4#803005)\n",
  "date": {
    "$date": "2019-12-27T00:16:00Z"
  },
  "patches": [],
  "external_id": "PHOENIX-5651"
},{
  "_id": {
    "$oid": "5bacb100faaadd76f8a9a33f"
  },
  "message_id": "<JIRA.13044741.1487664131000.19904.1487695424080@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [
    {
      "$oid": "5bacb0f3faaadd76f8a99fef"
    },
    {
      "$oid": "5bacb0f3faaadd76f8a99fee"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacb0f3faaadd76f8a99fee"
  },
  "from_id": {
    "$oid": "5bacb0ba57674ee167d4aa07"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PIG-5132) Merge from trunk (5) [Spark Branch]",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-5132?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15876266#comment-15876266 ] \n\nAdam Szita commented on PIG-5132:\n---------------------------------\n\n[~kellyzly]: [~nkollar] and I have created the merge patch too, see [^PIG-5132.1.patch].\nIt is less in size because file movements are handled as moves and not as deletion/additions. It also includes binary file changes (e.g. ant-contrib.jar).\nBasically diff was generated as\n{code}\ngit diff HEAD~ --full-index --binary\n{code}\nAfter resolving the merge conflicts we saw some build problems. These along with some e2e testing fixes can be found in a subsequent patch [^PIG-5132.1_fixes.patch]\nWe're running unit and e2e tests right now.\n\n> Merge from trunk (5) [Spark Branch]\n> -----------------------------------\n>\n>                 Key: PIG-5132\n>                 URL: https://issues.apache.org/jira/browse/PIG-5132\n>             Project: Pig\n>          Issue Type: Sub-task\n>          Components: spark\n>            Reporter: liyunzhang_intel\n>            Assignee: liyunzhang_intel\n>             Fix For: spark-branch\n>\n>         Attachments: PIG-5132.1_fixes.patch, PIG-5132.1.patch, PIG-5132.patch\n>\n>\n> merge changes from trunk to branch.\n> the latest commit in trunk is\n>  92df45d - (origin/trunk, origin/HEAD, trunk) PIG-5085: Support FLATTEN of maps (szita via rohini) \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-21T16:43:44Z"
  },
  "patches": [],
  "external_id": "PIG-5132"
},{
  "_id": {
    "$oid": "5f27ced8442ab9b9860f46ae"
  },
  "message_id": "<JIRA.12710587.1398443707328.84896.1404263904723@arcas>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27cea3442ab9b9860f37db"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cea3442ab9b9860f37db"
  },
  "from_id": {
    "$oid": "5bacb1e757674ee167d6df18"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Assigned] (OOZIE-1811) Current test failures in trunk",
  "body": "\n     [ https://issues.apache.org/jira/browse/OOZIE-1811?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMona Chitnis reassigned OOZIE-1811:\n-----------------------------------\n\n    Assignee: Mona Chitnis\n\n> Current test failures in trunk\n> ------------------------------\n>\n>                 Key: OOZIE-1811\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-1811\n>             Project: Oozie\n>          Issue Type: Bug\n>    Affects Versions: trunk\n>            Reporter: Robert Kanter\n>            Assignee: Mona Chitnis\n>            Priority: Critical\n>\n> There's a bunch of test failures currently in trunk; I'm not sure what commit(s) is the cause, but I think it was somewhat recent.\n> e.g. https://builds.apache.org/job/oozie-trunk-precommit-build/1199/\n> Reproducible by running these tests, instead of having to run them all, which takes a lot longer :)\n> {noformat}\n> mvn clean test -Dtest=TestSubWorkflowActionExecutor,TestBunldeChangeXCommand,TestCoordUpdateXCommand,TestCoordJobQueryExecutor,TestStatusTransitService,TestSLAEventGeneration\n> {noformat}\n> {noformat}\n> Results :\n> Failed tests:   testCoordinatorActionCommandsSubmitAndStart(org.apache.oozie.sla.TestSLAEventGeneration): expected:<...11921-oozie-rkan-C@1[]> but was:<...11921-oozie-rkan-C@1[2]>\n>   testCoordStatusTransitServiceDoneWithError(org.apache.oozie.service.TestStatusTransitService): expected:<DONEWITHERROR> but was:<KILLED>\n>   testBundleStatusTransitRunningFromKilled(org.apache.oozie.service.TestStatusTransitService): expected:<RUNNING> but was:<KILLED>\n> Tests in error: \n>   testGetList(org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor)\n>   testInsert(org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor)\n> Tests run: 62, Failures: 3, Errors: 2, Skipped: 0\n> {noformat}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-07-02T01:18:24Z"
  },
  "patches": [],
  "external_id": "OOZIE-1811"
},{
  "_id": {
    "$oid": "60fac324d907ab79037e7d9c"
  },
  "message_id": "<JIRA.12645611.1367396227000.196808.1478308259643@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac2cdd907ab79037e5e98"
    },
    {
      "$oid": "60fac324d907ab79037e7d9b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2cdd907ab79037e5e98"
  },
  "from_id": {
    "$oid": "59bfaa3af2a4565fe90974cd"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (DIRAPI-136) Add the TLS closure alert support in\n the API",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRAPI-136?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEmmanuel Lecharny updated DIRAPI-136:\n-------------------------------------\n    Fix Version/s:     (was: 1.0.0-RC2)\n                   1.0.0-RC3\n\n> Add the TLS closure alert support in the API\n> --------------------------------------------\n>\n>                 Key: DIRAPI-136\n>                 URL: https://issues.apache.org/jira/browse/DIRAPI-136\n>             Project: Directory Client API\n>          Issue Type: New Feature\n>    Affects Versions: 1.0.0-M16\n>            Reporter: Emmanuel Lecharny\n>             Fix For: 1.0.0-RC3\n>\n>\n> We currently can start a TLS session on an existing connection using the startTls() method, but we cant terminate it. We need to add a closeTls() method.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-11-05T01:10:59Z"
  },
  "patches": [],
  "external_id": "DIRAPI-136"
},{
  "_id": {
    "$oid": "5bbf0ff6b79d666cbb2315b3"
  },
  "message_id": "<JIRA.12702972.1395435833398.122504.1395444764657@arcas>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0ff6b79d666cbb2315b0"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0ff6b79d666cbb2315b0"
  },
  "from_id": {
    "$oid": "5bbf07f757674ee16731cad0"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-1321) Kafka zookeeper path should be\n configurable to create multiple Kafka cluster in the same zookeeper cluster",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-1321?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13943721#comment-13943721 ] \n\nJay Kreps commented on KAFKA-1321:\n----------------------------------\n\nHey Jay,\n\nThe zookeeper chroot feature Guozhang describes is actually preferrable to doing it in our code as it prevents the possibility any coding error and limits visibility to just the sub-path. However not everyone knows about this feature. How do you think we could improve our documentation to make this easier to discover?\n\n> Kafka zookeeper path should be configurable to create multiple Kafka cluster in the same zookeeper cluster\n> ----------------------------------------------------------------------------------------------------------\n>\n>                 Key: KAFKA-1321\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-1321\n>             Project: Kafka\n>          Issue Type: Improvement\n>            Reporter: Jay Bae\n>\n> We want to have multiple Kafka clusters in the same zookeeper and each Kafka cluster should be physically separated. To do so, the root path of zookeeper nodes should be configurable.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-03-21T23:32:44Z"
  },
  "patches": [],
  "external_id": "KAFKA-1321"
},{
  "_id": {
    "$oid": "58bfc929aea7c7604a49da25"
  },
  "message_id": "<240504795.1255413391334.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "58bfc8a9aea7c7604a49c2e7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "58bfc923aea7c7604a49d904"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a5"
  },
  "to_ids": [
    {
      "$oid": "58bfc8c002ca40f8bf147881"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (ZOOKEEPER-541) zkpython limited to 256 handles",
  "body": "\n     [ https://issues.apache.org/jira/browse/ZOOKEEPER-541?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nHenry Robinson updated ZOOKEEPER-541:\n-------------------------------------\n\n    Status: Patch Available  (was: In Progress)\n\n> zkpython limited to 256 handles\n> -------------------------------\n>\n>                 Key: ZOOKEEPER-541\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-541\n>             Project: Zookeeper\n>          Issue Type: Bug\n>          Components: contrib-bindings\n>    Affects Versions: 3.2.1\n>            Reporter: Patrick Hunt\n>            Assignee: Henry Robinson\n>             Fix For: 3.3.0\n>\n>         Attachments: ZOOKEEPER-541.patch\n>\n>\n> zkpython is currently limited to a max of 256 total handles - not 256 open handles, but rather 256 total handles created\n> over the lifetime of the python application.\n> In general this isn't a real issue, however in the case of a long lived application which polls the cluster periodically (closing\n> the session btw calls) this is an issue.\n> it would be great if the slots could be reused? or perhaps a more complex structure, such as a linked list, which would allow\n> dynamic growth/shrinkage of the handle list.\n> Also see ZOOKEEPER-540\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-10-12T22:56:31Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-541"
},{
  "_id": {
    "$oid": "5f27d08f532b7277349c3e4c"
  },
  "message_id": "<623777826.9461268174505504.JavaMail.jira@brutus.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d08f532b7277349c3e37"
  },
  "from_id": {
    "$oid": "5f27d08faf02e2d6de930982"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (AMQNET-241) NMS Failover causes CPU utilization to\n spike if ActiveMQ is stopped.",
  "body": "\n     [ https://issues.apache.org/activemq/browse/AMQNET-241?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTed Carroll updated AMQNET-241:\n-------------------------------\n\n\nThat's the behavior that I'd like.  However, the behavior that is there is still bad.  NMS should not spin up a thread which will, if allowed, consume as much CPU as it can get until the connection is disposed.\n\n> NMS Failover causes CPU utilization to spike if ActiveMQ is stopped.\n> --------------------------------------------------------------------\n>\n>                 Key: AMQNET-241\n>                 URL: https://issues.apache.org/activemq/browse/AMQNET-241\n>             Project: ActiveMQ .Net\n>          Issue Type: Bug\n>          Components: ActiveMQ\n>    Affects Versions: 1.2.0\n>         Environment: .NET 3.5\n>            Reporter: Ted Carroll\n>            Assignee: Timothy Bish\n>             Fix For: 1.2.1, 1.3.0\n>\n>         Attachments: NMS_Failover_CPU_Spike1.patch, NmsRepro_clean.zip\n>\n>\n> 1)  Configure a simple program that attempts to connect to ActiveMQ with the url \"activemq:failover:(tcp://localhost:61616)\"\n> 2)  Make sure activemq is not running on the local host\n> 3)  Try to connect.\n> 4)  Notice that the CPU utilization in the sample program is high.\n> 5)  Start ActiveMQ.\n> 6)  Notice that the connection is successful but the CPU utilization remains high.\n> Expected:\n> I expect, potentially, an initial spike in CPU utilization but not sustained high CPU.\n> Diagnosis:\n> In FailoverTransport.DoConnect() there is the following if statement:\n>                 if(ConnectedTransport != null || disposed || connectionFailure != null) \n>                 { \n>                     return false; \n>                 } \n>                 else \n> it apears that connectionFailure is set and never reset.  Then the FailoverTask.Iterate() calls into DoConnect only to return false immediately every time.\n> I have attached a patch, but I would be uncomfortable calling it a long term solution as I've not had the time to fully understand the code in question.  However, it seems to improve the behavior.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-03-09T22:41:45Z"
  },
  "patches": [],
  "external_id": "AMQNET-241"
},{
  "_id": {
    "$oid": "5f27cd8b014d3531c6cc2556"
  },
  "message_id": "<176691865.11090.1317808895686.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "59bf938ff2a4565fe9eadde9"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (JCR-3091) Lucene Scorer implementations should\n handle the 'advance' to NO_MORE_DOCS optimisation better",
  "body": "Lucene Scorer implementations should handle the 'advance' to NO_MORE_DOCS optimisation better\n---------------------------------------------------------------------------------------------\n\n                 Key: JCR-3091\n                 URL: https://issues.apache.org/jira/browse/JCR-3091\n             Project: Jackrabbit Content Repository\n          Issue Type: Improvement\n          Components: jackrabbit-core\n            Reporter: Alex Parvulescu\n            Assignee: Alex Parvulescu\n\n\nThis is from the lucene Scorer (actually DocIdSetIterator) api:\n\"NOTE: this method may be called with NO_MORE_DOCS for efficiency by some Scorers. If your implementation cannot efficiently determine that it should exhaust, it is recommended that you check for that value in each call to this method.\"\n\nNone of the scorer implementations does that currently. Except for ChildAxisScorer thanks to JCR-3082.\n\nThis is a worthwhile effort, which can save us from bugs (JCR-3082) but also leverage some performance optimisation hints from the lucene api.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-10-05T10:01:35Z"
  },
  "patches": [],
  "external_id": "JCR-3091"
},{
  "_id": {
    "$oid": "5bac97d3c291c350402ff3cb"
  },
  "message_id": "<1981738860.35366.1329196861130.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bac9787c291c350402fecd8"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bac97d3c291c350402ff3b6"
  },
  "from_id": {
    "$oid": "5bac97d357674ee167c946bd"
  },
  "to_ids": [
    {
      "$oid": "58c11c6c02ca40f8bfb1f59b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (XERCESJ-1553) An incomplete fix for the NPE bugs\n in XSAttributeUseImpl.java",
  "body": "\n     [ https://issues.apache.org/jira/browse/XERCESJ-1553?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMichael Glavassevich resolved XERCESJ-1553.\n-------------------------------------------\n\n    Resolution: Incomplete\n\nPointing to a commit made years ago doesn't imply that there's an issue today, nor does it imply that other codepaths have an issue. Please provide an actual test case which demonstrates a problem with the current code base.\n                \n> An incomplete fix for the NPE bugs in XSAttributeUseImpl.java\n> -------------------------------------------------------------\n>\n>                 Key: XERCESJ-1553\n>                 URL: https://issues.apache.org/jira/browse/XERCESJ-1553\n>             Project: Xerces2-J\n>          Issue Type: Bug\n>          Components: Other\n>            Reporter: Guangtai Liang\n>            Priority: Critical\n>              Labels: incomplete_fix, missing_fixes\n>\n> The fix revision 320528 was aimed to remove an NPE bug on the \"this.fDefault\" and \"this.fDefault.actualValue\" in the method \"getConstraintValue\" of the file \"/xerces/java/trunk/src/org/apache/xerces/impl/xs/XSAttributeUseImpl.java\" , but it is incomplete. \n> Since the \"this.fDefault\" is a class field and also could be null during the run-time execution, it should also be null-checked before being dereferenced in other methods. \n> The buggy code locations the same fix needs to be applied at are as bellows: \n> Line 123 of the method \"getActualVC\";\n>   public Object getActualVC() {\n>         return getConstraintType() == XSConstants.VC_NONE ?\n>                null :\n>                fDefault.actualValue;\n>     }\n> Line 129 of the method \"getActualVCType\": \n>        public short getActualVCType() {\n>         return getConstraintType() == XSConstants.VC_NONE ?\n>                XSConstants.UNAVAILABLE_DT :\n>                fDefault.actualValueType;\n>     }\n> Line 135 of the method \"getItemValueTypes\" : \n>   public ShortList getItemValueTypes() {\n>         return getConstraintType() == XSConstants.VC_NONE ?\n>                null :\n>                fDefault.itemValueTypes;\n>     }\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: j-dev-unsubscribe@xerces.apache.org\nFor additional commands, e-mail: j-dev-help@xerces.apache.org\n\n",
  "date": {
    "$date": "2012-02-14T05:21:01Z"
  },
  "patches": [],
  "external_id": "XERCESJ-1553"
},{
  "_id": {
    "$oid": "5bbc73b4bef76478c709b721"
  },
  "message_id": "<2700409.1146237620204.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bbc7089bef76478c70952e8"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "59677c13aff2204b3cbd153f"
  },
  "to_ids": [
    {
      "$oid": "58c11493e4f89451f51d782e"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Moved: (SB-16) Changing the action dynamically without using\n javascript",
  "body": "     [ http://issues.apache.org/struts/browse/SB-16?page=all ]\n\nDon Brown moved STR-2547 to SB-16:\n----------------------------------\n\n      Project: Sandbox  (was: Struts Action 1)\n          Key: SB-16  (was: STR-2547)\n    Component:     (was: Sandbox)\n      Version:     (was: 1.2.7)\n    Assign To:     (was: Struts Developer Mailing List)\n\n> Changing the action dynamically without using javascript\n> --------------------------------------------------------\n>\n>          Key: SB-16\n>          URL: http://issues.apache.org/struts/browse/SB-16\n>      Project: Sandbox\n>         Type: Improvement\n\n>  Environment: Operating System: other\n> Platform: All\n>     Reporter: Nagaraj\n>     Priority: Minor\n\n>\n> Hi,\n>     I am facing the following problem in my application. First i will explain \n> the problem context.\n> My action class extending the DispatchAction.In my <html: form > tag will have \n> default action.Say i have the buttons for Create, Read, Update,delate \n> operations in my JSP.Now based on the button click , i wll change the action \n> in the JavaScript.For example \n> document.formName.action=\"/abc.do?method=read\";\n> Now the problem is we dont want to use javascript.but at the same we wants to \n> use dispatchaction. So how do you will change the action dynamically without \n> using the javascript? since button/image does not have action property. Can \n> any one of you help me to solve this problem?\n> Thank you\n> Nagaraj P\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/struts/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@struts.apache.org\nFor additional commands, e-mail: dev-help@struts.apache.org\n\n",
  "date": {
    "$date": "2006-04-28T15:20:20Z"
  },
  "patches": [],
  "external_id": "SB-16"
},{
  "_id": {
    "$oid": "5bc86f6a57a11257de56c7f8"
  },
  "message_id": "<2104186670.5007.1345721502376.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc86f6957a11257de56c7ea"
  },
  "from_id": {
    "$oid": "5bc85cb357674ee167cda7da"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-1394) Image streams are lost when adding\n new images to page",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1394?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJuraj Lonc updated PDFBOX-1394:\n-------------------------------\n\n    Attachment: PDFBOX-1394.patch\n\nfix\n                \n> Image streams are lost when adding new images to page\n> -----------------------------------------------------\n>\n>                 Key: PDFBOX-1394\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1394\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: PDModel\n>    Affects Versions: 1.7.1\n>         Environment: Oracle JDK 1.6.0_26\n>            Reporter: Juraj Lonc\n>             Fix For: 1.8.0\n>\n>         Attachments: input.pdf, output.pdf, PDFBOX-1394.patch\n>\n>\n> I open existing PDF which contains images. Then I want to insert new images and save it as new PDF.\n> The resulting PDF does not contain original images, just the new ones. Adobe Reader complains about missing object too.\n> PDDocument pdDoc=PDDocument.load(\"input.pdf\", false);\n> PDXObjectImage ximage = new PDJpeg(pdDoc, bufferedImage);\n> PDPage page=(PDPage) pdDoc.getDocumentCatalog().getAllPages().get(0);\n> PDPageContentStream content = new PDPageContentStream(pdDoc, page, true, true);\n> content.drawXObject(ximage, 100, 600, 200, 130);\n> // or content.drawImage(ximage, 0, 0);\n> content.close();\n> pdDoc.save(\"output.pdf\");\n> pdDoc.close();\n> When I look into PDF binary file I can see only 1 image stream (the one I added) and I can't see streams of original images, they simply are not there.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-08-23T22:31:42Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1394"
},{
  "_id": {
    "$oid": "58bfcec615d83644fcc4850b"
  },
  "message_id": "<JIRA.13042035.1486724921000.113535.1487365541990@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfcebc15d83644fcc482c6"
    },
    {
      "$oid": "58bfcebc15d83644fcc482c5"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfcebc15d83644fcc482c5"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-2691) recreateSocketAddresses may\n recreate the unreachable IP address",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-2691?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15872521#comment-15872521 ] \n\nASF GitHub Bot commented on ZOOKEEPER-2691:\n-------------------------------------------\n\nGithub user eribeiro commented on a diff in the pull request:\n\n    https://github.com/apache/zookeeper/pull/173#discussion_r101845264\n  \n    --- Diff: src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java ---\n    @@ -181,6 +197,33 @@ public void recreateSocketAddresses() {\n                 }\n             }\n     \n    +        /**\n    +         * Resolve the hostname to IP addresses, and find one reachable address.\n    +         *\n    +         * @param hostname the name of the host\n    +         * @param timeout the time, in millseconds, before {@link InetAddress#isReachable}\n    +         *                aborts\n    +         * @return a reachable IP address. If no such IP address can be found,\n    +         *         just return the first IP address of the hostname.\n    +         *\n    +         * @exception UnknownHostException\n    +         */\n    +        public InetAddress getReachableAddress(String hostname, int timeout) \n    +                throws UnknownHostException {\n    +            InetAddress[] addresses = InetAddress.getAllByName(hostname);\n    +            for (InetAddress a : addresses) {\n    +                try {\n    +                    if (a.isReachable(timeout)) {\n    --- End diff --\n    \n    My main problem with this PR is that call to `isReachable(timeout)` for two reasons: \n    \n    1) the most important one: `isReachable(timeout)` seems unreliable so there are plenty cases where it returns false even tough the node is reachable or vice-versa! https://bugs.openjdk.java.net/browse/JDK-8159410 (google \"InetAddress.isReachable not working\" or \"InetAddress.isReachable unreliable\" to see further cases).\n    \n    2) This timeout can add an arbitrary delay until a reachable node can be tested.\n    \n    IDK what a good compromise would be for both points above (leaving as it is today could work, so no problem, even tough I am a bit concerned), but maybe we could use a solution similar to ZOOKEEPER-2184 and return the next address in the array (using `next = ++next % addresses.length` to prevent out of bound exceptions).\n    \n\n\n\n> recreateSocketAddresses may recreate the unreachable IP address\n> ---------------------------------------------------------------\n>\n>                 Key: ZOOKEEPER-2691\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-2691\n>             Project: ZooKeeper\n>          Issue Type: Bug\n>    Affects Versions: 3.4.8\n>         Environment: Centos6.5\n> Java8\n> ZooKeeper3.4.8\n>            Reporter: JiangJiafu\n>            Priority: Minor\n>\n> The QuorumPeer$QuorumServer.recreateSocketAddress()  is used to resolved the hostname to a new IP address(InetAddress) when any exception happens to the socket. It will be very useful when a hostname can be resolved to more than one IP address.\n> But the problem is Java API InetAddress.getByName(String hostname) will always return the first IP address when the hostname can be resolved to more than one IP address, and the first IP address may be unreachable forever. For example, if a machine has two network interfaces: eth0, eth1, say eth0 has ip1, eth1 has ip2, the relationship between hostname and the IP addresses is set in /etc/hosts. When I \"close\" the eth0 by command \"ifdown eth0\", the InetAddress.getByName(String hostname)  will still return ip1, which is unreachable forever.\n> So I think it will be better to check the IP address by InetAddress.isReachable(long) and choose the reachable IP address. \n> I have modified the ZooKeeper source code, and test the new code in my own environment, and it can work very well when I turn down some network interfaces using \"ifdown\" command.\n> The original code is:\n> {code:title=QuorumPeer.java|borderStyle=solid}\n>         public void recreateSocketAddresses() {\n>             InetAddress address = null;\n>             try {\n>                 address = InetAddress.getByName(this.hostname);\n>                 LOG.info(\"Resolved hostname: {} to address: {}\", this.hostname, address);\n>                 this.addr = new InetSocketAddress(address, this.port);\n>                 if (this.electionPort > 0){\n>                     this.electionAddr = new InetSocketAddress(address, this.electionPort);\n>                 }\n>             } catch (UnknownHostException ex) {\n>                 LOG.warn(\"Failed to resolve address: {}\", this.hostname, ex);\n>                 // Have we succeeded in the past?\n>                 if (this.addr != null) {\n>                     // Yes, previously the lookup succeeded. Leave things as they are\n>                     return;\n>                 }\n>                 // The hostname has never resolved. Create our InetSocketAddress(es) as unresolved\n>                 this.addr = InetSocketAddress.createUnresolved(this.hostname, this.port);\n>                 if (this.electionPort > 0){\n>                     this.electionAddr = InetSocketAddress.createUnresolved(this.hostname,\n>                                                                            this.electionPort);\n>                 }\n>             }\n>         }\n> {code}\n> After my modification:\n> {code:title=QuorumPeer.java|borderStyle=solid}\n>         public void recreateSocketAddresses() {\n>             InetAddress address = null;\n>             try {\n>                 address = getReachableAddress(this.hostname);\n>                 LOG.info(\"Resolved hostname: {} to address: {}\", this.hostname, address);\n>                 this.addr = new InetSocketAddress(address, this.port);\n>                 if (this.electionPort > 0){\n>                     this.electionAddr = new InetSocketAddress(address, this.electionPort);\n>                 }\n>             } catch (UnknownHostException ex) {\n>                 LOG.warn(\"Failed to resolve address: {}\", this.hostname, ex);\n>                 // Have we succeeded in the past?\n>                 if (this.addr != null) {\n>                     // Yes, previously the lookup succeeded. Leave things as they are\n>                     return;\n>                 }\n>                 // The hostname has never resolved. Create our InetSocketAddress(es) as unresolved\n>                 this.addr = InetSocketAddress.createUnresolved(this.hostname, this.port);\n>                 if (this.electionPort > 0){\n>                     this.electionAddr = InetSocketAddress.createUnresolved(this.hostname,\n>                                                                            this.electionPort);\n>                 }\n>             }\n>         }\n>         public InetAddress getReachableAddress(String hostname) throws UnknownHostException {\n>             InetAddress[] addresses = InetAddress.getAllByName(hostname);\n>             for (InetAddress a : addresses) {\n>                 try {\n>                     if (a.isReachable(5000)) {\n>                         return a;\n>                     } \n>                 } catch (IOException e) {\n>                     LOG.warn(\"IP address {} is unreachable\", a);\n>                 }\n>             }\n>             // All the IP address is unreachable, just return the first one.\n>             return addresses[0];\n>         }\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-17T21:05:41Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-2691"
},{
  "_id": {
    "$oid": "5bdc014e16772b6055c7eee4"
  },
  "message_id": "<JIRA.12958218.1460488646000.221665.1460571805589@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdc014d16772b6055c7eedd"
    },
    {
      "$oid": "5bdc014d16772b6055c7eede"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdc014d16772b6055c7eedd"
  },
  "from_id": {
    "$oid": "58c9de5702ca40f8bf20c1ae"
  },
  "to_ids": [
    {
      "$oid": "5bdc002135e3ea2b7bb8a8be"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KNOX-709) HBase request URLs must not be URL\n encoded",
  "body": "\n    [ https://issues.apache.org/jira/browse/KNOX-709?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15239759#comment-15239759 ] \n\nASF subversion and git services commented on KNOX-709:\n------------------------------------------------------\n\nCommit 2eb51e601ada268c178e746478dd7945ec26ec12 in knox's branch refs/heads/master from [~kevin.minder]\n[ https://git-wip-us.apache.org/repos/asf?p=knox.git;h=2eb51e6 ]\n\n[KNOX-709] - HBase request URLs must not be URL encoded\n\n\n> HBase request URLs must not be URL encoded\n> ------------------------------------------\n>\n>                 Key: KNOX-709\n>                 URL: https://issues.apache.org/jira/browse/KNOX-709\n>             Project: Apache Knox\n>          Issue Type: Bug\n>    Affects Versions: 0.9.0\n>            Reporter: Kevin Minder\n>             Fix For: 0.9.0\n>\n>\n> A recent change to deal with URL encoding seems to have caused an issue with HBase.  Ultimately this appears to be a problem with HBase improperly decoding a URL but this needs to be accounted for in Knox.  You can see from the audit log output below that colons and commas in the URL path are being encoded and HBase appears to be decoding them improperly.\n> {code}\n> 16/04/12 13:40:24 ||20f26ac5-f2ea-4b41-8050-9bbbfc08f660|audit|WEBHBASE|guest|||dispatch|uri|http://localhost:60080/test_table/*/family1%3Arow2_col1%2Cfamily2%3A/0%2C9223372036854775807?v=1&user.name=guest|success|Response status: 500\n> 16/04/12 13:40:24 ||20f26ac5-f2ea-4b41-8050-9bbbfc08f660|audit|WEBHBASE|guest|||access|uri|/gateway/sandbox/hbase/test_table/*/family1:row2_col1,family2:/0,9223372036854775807?v=1|success|Response status: 500\n> {code}\n> This results in failures containing this message found but enabling HTTP client logging.\n> {code}\n> <h3>Caused by:</h3><pre>java.lang.NumberFormatException: For input string: \"0,9223372036854775807\"\n> \tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n> \tat java.lang.Long.parseLong(Long.java:441)\n> \tat java.lang.Long.valueOf(Long.java:540)\n> \tat org.apache.hadoop.hbase.rest.RowSpec.parseTimestamp(RowSpec.java:167)\n> \tat org.apache.hadoop.hbase.rest.RowSpec.&lt;init&gt;(RowSpec.java:62)\n> \tat org.apache.hadoop.hbase.rest.RowResource.&lt;init&gt;(RowResource.java:77)\n> \tat org.apache.hadoop.hbase.rest.TableResource.getRowResourceWithSuffixGlobbing(TableResource.java:119)\n> \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n> \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n> \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n> \tat java.lang.reflect.Method.invoke(Method.java:606)\n> \tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.invokeSubLocator(SubLocatorRule.java:180)\n> \tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:107)\n> \tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n> \tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)\n> \tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n> \tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n> \tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n> \tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n> \tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n> \tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n> \tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n> \tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n> \tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n> \tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n> \tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n> \tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n> \tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n> \tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n> \tat org.apache.hadoop.hbase.rest.filter.GzipFilter.doFilter(GzipFilter.java:76)\n> \tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n> \tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n> \tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n> \tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n> \tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n> \tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n> \tat org.mortbay.jetty.Server.handle(Server.java:326)\n> \tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n> \tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n> \tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n> \tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n> \tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n> \tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n> \tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-04-13T18:23:25Z"
  },
  "patches": [],
  "external_id": "KNOX-709"
},{
  "_id": {
    "$oid": "5f27d2a746816ce7cf504496"
  },
  "message_id": "<19160475.1128581471611.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d2a046816ce7cf504289"
  },
  "from_id": {
    "$oid": "5f27d22eaf02e2d6de9bcb22"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (MAVENUPLOAD-539) Upload StringTemplate 2.2",
  "body": "    [ http://jira.codehaus.org/browse/MAVENUPLOAD-539?page=comments#action_47940 ] \n\nWilfred Springer commented on MAVENUPLOAD-539:\n----------------------------------------------\n\nI intended to have this uploaded to the Maven 2 repository, and figured that no project.xml would have been required in that case.\n\n> Upload StringTemplate 2.2\n> -------------------------\n>\n>          Key: MAVENUPLOAD-539\n>          URL: http://jira.codehaus.org/browse/MAVENUPLOAD-539\n>      Project: maven-upload-requests\n>         Type: Task\n>     Reporter: Wilfred Springer\n\n>\n>\n> http://blogs.sun.com/roller/resources/wilfred/stringtemplate-2.2-bundle.jar\n> http://www.stringtemplate.org/\n> Version 2.2 of the template engine created by the inventor of ANTLR.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2005-10-06T01:51:11Z"
  },
  "patches": [],
  "external_id": "MAVENUPLOAD-539"
},{
  "_id": {
    "$oid": "60fd7ade36c61279ee0bab4c"
  },
  "message_id": "<JIRA.13000924.1472467719000.185068.1494494584985@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fd79fc36c61279ee0b865d"
  },
  "reference_ids": [
    {
      "$oid": "60fd7adc36c61279ee0baab0"
    },
    {
      "$oid": "60fd7adc36c61279ee0baab1"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fd7adc36c61279ee0baab0"
  },
  "from_id": {
    "$oid": "60fd7ac5f73e2aa3900008cf"
  },
  "to_ids": [
    {
      "$oid": "5f27c3fcaf02e2d6de73a42d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (FINERACT-228) As a manager, i want to get\n reports on \"Deposits to Staff Account \"",
  "body": "\n    [ https://issues.apache.org/jira/browse/FINERACT-228?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16006134#comment-16006134 ] \n\nSantosh Math commented on FINERACT-228:\n---------------------------------------\n\nHi  [~Ippez], Is this resolved? \n\n> As a manager, i want to get reports on \"Deposits to Staff Account \"\n> -------------------------------------------------------------------\n>\n>                 Key: FINERACT-228\n>                 URL: https://issues.apache.org/jira/browse/FINERACT-228\n>             Project: Apache Fineract\n>          Issue Type: Improvement\n>            Reporter: Ippez Roberts\n>            Assignee: Markus Geiss\n>             Fix For: 1.0.0\n>\n>\n> During Client creation, Include \"Is Staff\" as a Boolean so that i can filter loan and savings accounts that belong to a staff and any deposit activities associated with it. This is to avoid malicious staff from posting transactions into their accounts.\n> Design a report to fetch \"Deposit to Staff Account\". This includes both Loan Repayment, Disbursements and Savings Deposit to a staff client for a range or period (FROM DATE.... TO DATE ...)\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-05-11T09:23:04Z"
  },
  "patches": [],
  "external_id": "FINERACT-228"
},{
  "_id": {
    "$oid": "5bc867d857a11257de568b8d"
  },
  "message_id": "<JIRA.12635060.1362356866000.1821.1409552841255@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8649157a11257de566c8e"
    },
    {
      "$oid": "5bc8649157a11257de566c8d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8649157a11257de566c8d"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-1527) Added table cells, page breaking,\n line, breaking, and translieration",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-1527?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14117072#comment-14117072 ] \n\nTilman Hausherr commented on PDFBOX-1527:\n-----------------------------------------\n\nDon't know, we're just small amount of people, and sometimes, no one makes a decision for or against. Me, I mostly focus on rendering. And while PDFBox does have PDF generation, these are low level commands. I thought people use Apache FOP for higher level stuff.\n\nHowever I wonder: why would people \"move from itext to pdfbox\"? They paid for itext.\n\n> Added table cells, page breaking, line, breaking, and translieration\n> --------------------------------------------------------------------\n>\n>                 Key: PDFBOX-1527\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1527\n>             Project: PDFBox\n>          Issue Type: New Feature\n>          Components: Writing\n>    Affects Versions: 1.8.0\n>            Reporter: Glen Peterson\n>              Labels: features\n>         Attachments: GlenPdfTest.java, test.pdf\n>\n>\n> PlanBase and I would like to contribute our line-breaking, page-breaking, table cell, and transliteration code to the PDFBox project.  I think someone on the mailing list asked me to open an issue here and attach a patch, so that is what I'm doing.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-09-01T06:27:21Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1527"
},{
  "_id": {
    "$oid": "5bacb29ffaaadd76f8aa0693"
  },
  "message_id": "<JIRA.12633537.1361500315221.331229.1361765292987@arcas>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [
    {
      "$oid": "5bacb269faaadd76f8a9fa46"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacb269faaadd76f8a9fa46"
  },
  "from_id": {
    "$oid": "5bacb29d57674ee167d838a2"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PIG-3210) Pig fails to start when it cannot write\n log to log files",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-3210?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMeng-Sung Wu updated PIG-3210:\n------------------------------\n\n    Affects Version/s:     (was: 0.10.0)\n               Status: Patch Available  (was: Open)\n    \n> Pig fails to start when it cannot write log to log files\n> --------------------------------------------------------\n>\n>                 Key: PIG-3210\n>                 URL: https://issues.apache.org/jira/browse/PIG-3210\n>             Project: Pig\n>          Issue Type: Bug\n>          Components: impl\n>            Reporter: Meng-Sung Wu\n>            Priority: Minor\n>         Attachments: PIG-3210.patch\n>\n>\n> Pig will check whether logFileName is null or not before setting to pig.logfile property in some places. But forget to check in other places.\n> {code:xml} \n> 381         pigContext.getProperties().setProperty(\"pig.logfile\", (logFileName == null? \"\": logFileName));\n> ...\n> 451         pigContext.getProperties().setProperty(\"pig.logfile\", logFileName);\n> {code} \n> {code:xml} \n> 12/12/25 16:38:00 WARN pig.Main: Need write permission in the directory: /opt/trend/hadooppet/sanity-tm-6/result to create log file.\n>  14 2012-12-25 16:38:00,372 [main] INFO org.apache.pig.Main - Apache Pig version 0.10.1.tm6 (rexported) compiled Oct 22 2012, 11:11:02\n>  15 2012-12-25 16:38:01,712 [main] WARN org.apache.pig.Main - Cannot write to log file: /opt/trend/hadooppet/sanity-tm-6/result//akamai.pig1356453481712.log\n>  16 2012-12-25 16:38:01,727 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. null\n>  17 2012-12-25 16:38:01,727 [main] WARN org.apache.pig.Main - There is no log file to write to.\n>  18 2012-12-25 16:38:01,727 [main] ERROR org.apache.pig.Main - java.lang.NullPointerException\n>  19 at java.util.Hashtable.put(Hashtable.java:394)\n>  20 at java.util.Properties.setProperty(Properties.java:143)\n>  21 at org.apache.pig.Main.run(Main.java:542)\n>  22 at org.apache.pig.Main.main(Main.java:115)\n>  23 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n>  24 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n>  25 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n>  26 at java.lang.reflect.Method.invoke(Method.java:597)\n>  27 at org.apache.hadoop.util.RunJar.main(RunJar.java:208)\n> {code} \n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-02-25T04:08:12Z"
  },
  "patches": [],
  "external_id": "PIG-3210"
},{
  "_id": {
    "$oid": "5bbe0ed5272f7b1f6830de4a"
  },
  "message_id": "<JIRA.12935718.1454353808000.267240.1454353839894@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e12272f7b1f6830bd59"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0ec7272f7b1f6830dbd9"
    },
    {
      "$oid": "5bbe0ec7272f7b1f6830dbda"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0ec7272f7b1f6830dbd9"
  },
  "from_id": {
    "$oid": "5bbe0e2857674ee167917572"
  },
  "to_ids": [
    {
      "$oid": "5bbe0e2757674ee1679173c5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PARQUET-498) Add a ColumnChunk builder\n abstraction as part of creating new row groups",
  "body": "Wes McKinney created PARQUET-498:\n------------------------------------\n\n             Summary: Add a ColumnChunk builder abstraction as part of creating new row groups\n                 Key: PARQUET-498\n                 URL: https://issues.apache.org/jira/browse/PARQUET-498\n             Project: Parquet\n          Issue Type: New Feature\n          Components: parquet-cpp\n            Reporter: Wes McKinney\n\n\nNecessary for PARQUET-452, but we should treat as an independent task.\n\nThis class will be responsible for encapsulating creating a serialized sequence of data pages. This way, users on the write path need only specify the desired data page size, then write arrays of values, repetition, and definiton levels\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-02-01T19:10:39Z"
  },
  "patches": [],
  "external_id": "PARQUET-498"
},{
  "_id": {
    "$oid": "5bbdf5f4e8113566f664c379"
  },
  "message_id": "<1781195668.49200.1347005887861.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbdf5f4e8113566f664c378"
  },
  "from_id": {
    "$oid": "5bacc58a57674ee167df4cce"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd1e4f89451f55cdfd2"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (NUTCH-1459) Remove dead code (phase2) from\n InjectorJob",
  "body": "\n     [ https://issues.apache.org/jira/browse/NUTCH-1459?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nFerdy Galema updated NUTCH-1459:\n--------------------------------\n\n    Attachment: nutch-1459.txt\n    \n> Remove dead code (phase2) from InjectorJob\n> ------------------------------------------\n>\n>                 Key: NUTCH-1459\n>                 URL: https://issues.apache.org/jira/browse/NUTCH-1459\n>             Project: Nutch\n>          Issue Type: Improvement\n>            Reporter: Ferdy Galema\n>             Fix For: 2.1\n>\n>         Attachments: nutch-1459.txt\n>\n>\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-09-07T19:18:07Z"
  },
  "patches": [],
  "external_id": "NUTCH-1459"
},{
  "_id": {
    "$oid": "5bea9a2b9e73d744d41217da"
  },
  "message_id": "<140800243.7529.1311691269750.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9a259e73d744d41216a8"
  },
  "from_id": {
    "$oid": "5bea981135e3ea2b7b4e616e"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DERBY-5333) Intermittent assert failure in\n testInterruptShutdown: thread's interrupted flag lost after shutdown",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-5333?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13071126#comment-13071126 ] \n\nDag H. Wanvik commented on DERBY-5333:\n--------------------------------------\n\nBackported to 10.8 branch at 1151118, closing.\n\n> Intermittent assert failure in testInterruptShutdown: thread's interrupted flag lost after shutdown\n> ---------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-5333\n>                 URL: https://issues.apache.org/jira/browse/DERBY-5333\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: JDBC\n>            Reporter: Dag H. Wanvik\n>            Assignee: Dag H. Wanvik\n>            Priority: Minor\n>             Fix For: 10.8.1.6, 10.9.0.0\n>\n>         Attachments: derby-5333-repro.diff, derby-5333a.diff\n>\n>\n> Saw this while running trunk's InterruptResilienceTest 800 times back-to-back with JDK1.7 b147 looking for DERBY-5312. It is independent of that issue, though: I saw it with both proposed patches for that issue also.\n> 1) testInterruptShutdown(org.apache.derbyTesting.functionTests.tests.store.InterruptResilienceTest)junit.f\\\n> ramework.AssertionFailedError\n>         at org.apache.derbyTesting.functionTests.tests.store.InterruptResilienceTest.testInterruptShutdown\\\n> (InterruptResilienceTest.java:667)\n>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n>         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n>         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n>         at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:112)\n>         at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)\n>         at junit.extensions.TestSetup$1.protect(TestSetup.java:21)\n>         at junit.extensions.TestSetup.run(TestSetup.java:25)\n>         at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)\n>         at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)\n>         at junit.extensions.TestSetup$1.protect(TestSetup.java:21)\n>         at junit.extensions.TestSetup.run(TestSetup.java:25)\n>         at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)\n>         at junit.extensions.TestSetup$1.protect(TestSetup.java:21)\n>         at junit.extensions.TestSetup.run(TestSetup.java:25)\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-07-26T14:41:09Z"
  },
  "patches": [],
  "external_id": "DERBY-5333"
},{
  "_id": {
    "$oid": "60fac332d907ab79037e82aa"
  },
  "message_id": "<JIRA.12979408.1466006748000.6307.1466006769422@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac31ad907ab79037e79ea"
    },
    {
      "$oid": "60fac332d907ab79037e82a9"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac31ad907ab79037e79ea"
  },
  "from_id": {
    "$oid": "5bbe12a057674ee1679b1512"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (DIRKRB-584) NPE if the token issuers value is not\n specified",
  "body": "Colm O hEigeartaigh created DIRKRB-584:\n------------------------------------------\n\n             Summary: NPE if the token issuers value is not specified\n                 Key: DIRKRB-584\n                 URL: https://issues.apache.org/jira/browse/DIRKRB-584\n             Project: Directory Kerberos\n          Issue Type: Bug\n    Affects Versions: 1.0.0-RC2\n            Reporter: Colm O hEigeartaigh\n            Assignee: Colm O hEigeartaigh\n             Fix For: 1.0.0-GA\n\n\nThere is a NPE if the the token issuers value is not specified.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-06-15T16:06:09Z"
  },
  "patches": [],
  "external_id": "DIRKRB-584"
},{
  "_id": {
    "$oid": "5bea9fb39e73d744d4133db5"
  },
  "message_id": "<181185229.1140040510711.JavaMail.jira@ajax.apache.org>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9fb39e73d744d4133db4"
  },
  "from_id": {
    "$oid": "5bea9f9135e3ea2b7b60dce6"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (DERBY-608) Documentation will require changes\n once dynamic parameters are supported for unary minus and unary plus.",
  "body": "    [ http://issues.apache.org/jira/browse/DERBY-608?page=comments#action_12366546 ] \n\nEric Radzinski commented on DERBY-608:\n--------------------------------------\n\nCan somebody confirm that the only doc change related to this issue is removing the following note from the Reference Guide.  It's in the topic titled \"Where dynamic parameters are allowed\"\n\n    13. A dynamic parameter is not allowed as the operand of the unary operators - or +.\n\nAre there any other doc hits?\n\n> Documentation will require changes once dynamic parameters are supported for unary minus and unary plus.\n> --------------------------------------------------------------------------------------------------------\n>\n>          Key: DERBY-608\n>          URL: http://issues.apache.org/jira/browse/DERBY-608\n>      Project: Derby\n>         Type: Sub-task\n>   Components: Documentation\n>     Versions: 10.2.0.0\n>     Reporter: Mamta A. Satoor\n>      Fix For: 10.2.0.0\n\n>\n> Once Derby-582 is resolved, the documentation for unary -/+ should be updated to reflect support for dynamic parameters for unary +/-.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-02-15T22:55:10Z"
  },
  "patches": [],
  "external_id": "DERBY-608"
},{
  "_id": {
    "$oid": "58c120cd6d2aba458ddfc2b8"
  },
  "message_id": "<8691283.1121612353410.JavaMail.jira@ajax.apache.org>",
  "mailing_list_id": {
    "$oid": "58c117176d2aba458dde60bb"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "58c11e69e4f89451f51d7dcf"
  },
  "to_ids": [
    {
      "$oid": "58c11420e4f89451f51d76f1"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Created: (JELLY-214) Static namespaced attributes wrongly managed",
  "body": "Static namespaced attributes wrongly managed\n--------------------------------------------\n\n         Key: JELLY-214\n         URL: http://issues.apache.org/jira/browse/JELLY-214\n     Project: jelly\n        Type: Bug\n  Components: core / taglib.core  \n    Versions: 1.0    \n    Reporter: Paul Libbrecht\n Assigned to: Paul Libbrecht \n     Fix For: 1.1-beta-1\n\n\nThe following script:\n <j:jelly xmlns:j=\"jelly:core\">\n  <foo xmlns:xx=\"lolo\" xx:att2=\"lala\"/>\n </j:jelly>\nYields a very weird error which, after tracing, indicates that StaticTagScript handles attributes without namespace.\n\npaul\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: commons-dev-unsubscribe@jakarta.apache.org\nFor additional commands, e-mail: commons-dev-help@jakarta.apache.org\n\n",
  "date": {
    "$date": "2005-07-17T16:59:13Z"
  },
  "patches": [],
  "external_id": "JELLY-214"
},{
  "_id": {
    "$oid": "5bc8696657a11257de569ab1"
  },
  "message_id": "<JIRA.12728423.1405838750966.3821.1405845518715@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8685f57a11257de568fcd"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8685f57a11257de568fcd"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Comment Edited] (PDFBOX-2228) LZW EarlyChange parameter\n isn't supported",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-2228?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14067845#comment-14067845 ] \n\nTilman Hausherr edited comment on PDFBOX-2228 at 7/20/14 8:38 AM:\n------------------------------------------------------------------\n\nFixed in rev 1612039 for the trunk and in rev 1612055 for the 1.8 branch.\n\n\nwas (Author: tilman):\nFixed in rev 1612039 for the trunk.\n\n> LZW EarlyChange parameter isn't supported\n> -----------------------------------------\n>\n>                 Key: PDFBOX-2228\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-2228\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Parsing\n>    Affects Versions: 1.8.6, 1.8.7, 2.0.0\n>            Reporter: Tilman Hausherr\n>            Assignee: Tilman Hausherr\n>              Labels: LZW\n>             Fix For: 1.8.6, 1.8.7, 2.0.0\n>\n>         Attachments: PDFBOX-2228-031207.pdf\n>\n>\n> The image in the attached PDF can't be decoded (IndexOutOfBoundsException), The reason is that the EarlyChange decode param is set to 0:\n> {quote}\n> An indication of when to increase the code length. If the value of this entry is 0, code length increases are postponed as long as possible. If the value is 1, code length increases occur one code early. This parameter is included because LZW sample code distributed by some vendors increases the code length one code earlier than necessary. Default value: 1.\n> {quote}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-07-20T08:38:38Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2228"
},{
  "_id": {
    "$oid": "5bacb221faaadd76f8a9e9ba"
  },
  "message_id": "<JIRA.12697736.1393519832219.119000.1393523480657@arcas>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [
    {
      "$oid": "5bacb20dfaaadd76f8a9e568"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacb20dfaaadd76f8a9e568"
  },
  "from_id": {
    "$oid": "58bfc95002ca40f8bf14799b"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PIG-3782) PushDownForEachFlatten +\n ColumnMapKeyPrune with user defined schema failing due to incorrect UID\n assignment",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-3782?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKoji Noguchi updated PIG-3782:\n------------------------------\n\n    Attachment: pig-3782-v01.patch\n\nI lack the understanding of why we need the copying of userDefinedSchema. \nThis patch takes out the copying step so that UID of the userDefinedSchema would be saved.\n\n[~daijy], can you check if I'm not messing up with this change?\n\n> PushDownForEachFlatten + ColumnMapKeyPrune with user defined schema failing due to incorrect UID assignment\n> -----------------------------------------------------------------------------------------------------------\n>\n>                 Key: PIG-3782\n>                 URL: https://issues.apache.org/jira/browse/PIG-3782\n>             Project: Pig\n>          Issue Type: Bug\n>            Reporter: Koji Noguchi\n>            Assignee: Koji Noguchi\n>         Attachments: pig-3782-v01.patch\n>\n>\n> {noformat}\n> a = load '1.txt' as (a0:int, a1, a2:bag{});\n> b = load '2.txt' as (b0:int, b1);\n> c = foreach a generate a0, flatten(a2) as (q1, q2);\n> d = join c by a0, b by b0;\n> e = foreach d generate a0, q1, q2;\n> f = foreach e generate a0, (int)q1, (int)q2;\n> store f into 'output';\n> {noformat}\n> This pig script fails with \n> 2014-02-27 11:49:45,657 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 13 Input: 0 Column: 1)\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-27T17:51:20Z"
  },
  "patches": [],
  "external_id": "PIG-3782"
},{
  "_id": {
    "$oid": "5bea9c4a9e73d744d4128989"
  },
  "message_id": "<1826285942.1213171725019.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9c4a9e73d744d4128987"
  },
  "from_id": {
    "$oid": "5bea9c4a35e3ea2b7b5873f5"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-3688) multiple db instance and ERROR XSDG3:\n Meta-data for Container\n org.apache.derby.impl.store.raw.data.RAFContainer4@d1cfbb could not be\n accessed",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-3688?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nSe Youn Ban updated DERBY-3688:\n-------------------------------\n\n          Environment: Windows 2000, JDK1.4.2, derby 10.3.3.0,10.4.1.3 and others  (was: Windows 2000, JDK1.4.2)\n    Affects Version/s:     (was: 10.3.3.0)\n\n> multiple db instance and ERROR XSDG3: Meta-data for Container org.apache.derby.impl.store.raw.data.RAFContainer4@d1cfbb could not be accessed\n> ---------------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-3688\n>                 URL: https://issues.apache.org/jira/browse/DERBY-3688\n>             Project: Derby\n>          Issue Type: Bug\n>         Environment: Windows 2000, JDK1.4.2, derby 10.3.3.0,10.4.1.3 and others\n>            Reporter: Se Youn Ban\n>            Priority: Critical\n>\n> I got this error when I took queries for multiple db instances. \n> I make db instance for every day and access the db instance of the day when I need to query.\n> The size of one day db instance is about 6G.\n> When I try to select query for a week, the ERROR XSDG3 occurs and derby is goning to be crashed.\n> --------------------------------\n> ERROR XSDG3: Meta-data for Container org.apache.derby.impl.store.raw.data.RAFContainer4@d1cfbb could not be accessed\n> \tat org.apache.derby.iapi.error.StandardException.newException(StandardException.java:296)\n> \tat org.apache.derby.impl.store.raw.data.RAFContainer.run(RAFContainer.java:1451)\n> \tat java.security.AccessController.doPrivileged(Native Method)\n> \tat org.apache.derby.impl.store.raw.data.RAFContainer.openContainer(RAFContainer.java:854)\n> \tat org.apache.derby.impl.store.raw.data.RAFContainer4.openContainer(RAFContainer4.java:146)\n> \tat org.apache.derby.impl.store.raw.data.FileContainer.setIdent(FileContainer.java:365)\n> \tat org.apache.derby.impl.store.raw.data.RAFContainer.setIdentity(RAFContainer.java:134)\n> \tat org.apache.derby.impl.services.cache.CachedItem.takeOnIdentity(CachedItem.java:222)\n> \tat org.apache.derby.impl.services.cache.Clock.addEntry(Clock.java:800)\n> \tat org.apache.derby.impl.services.cache.Clock.find(Clock.java:303)\n> \tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.openContainer(BaseDataFileFactory.java:629)\n> \tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.openContainer(BaseDataFileFactory.java:559)\n> \tat org.apache.derby.impl.store.raw.xact.Xact.openContainer(Xact.java:1283)\n> \tat org.apache.derby.impl.store.access.heap.HeapConglomerateFactory.readConglomerate(HeapConglomerateFactory.java:237)\n> \tat org.apache.derby.impl.store.access.RAMAccessManager.conglomCacheFind(RAMAccessManager.java:482)\n> \tat org.apache.derby.impl.store.access.RAMTransaction.findExistingConglomerate(RAMTransaction.java:397)\n> \tat org.apache.derby.impl.store.access.RAMTransaction.openStoreCost(RAMTransaction.java:1664)\n> \tat org.apache.derby.impl.sql.compile.CompilerContextImpl.getStoreCostController(CompilerContextImpl.java:460)\n> \tat org.apache.derby.impl.sql.compile.FromBaseTable.getStoreCostController(FromBaseTable.java:4471)\n> \tat org.apache.derby.impl.sql.compile.FromBaseTable.estimateCost(FromBaseTable.java:946)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.estimateTotalCost(OptimizerImpl.java:2623)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costBasedCostOptimizable(OptimizerImpl.java:2171)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costOptimizable(OptimizerImpl.java:1984)\n> \tat org.apache.derby.impl.sql.compile.FromBaseTable.optimizeIt(FromBaseTable.java:520)\n> \tat org.apache.derby.impl.sql.compile.ProjectRestrictNode.optimizeIt(ProjectRestrictNode.java:316)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.SelectNode.optimize(SelectNode.java:1766)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:894)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:255)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.TableOperatorNode.optimizeSource(TableOperatorNode.java:886)\n> \tat org.apache.derby.impl.sql.compile.UnionNode.optimizeIt(UnionNode.java:249)\n> \tat org.apache.derby.impl.sql.compile.ProjectRestrictNode.optimizeIt(ProjectRestrictNode.java:316)\n> \tat org.apache.derby.impl.sql.compile.OptimizerImpl.costPermutation(OptimizerImpl.java:1938)\n> \tat org.apache.derby.impl.sql.compile.SelectNode.optimize(SelectNode.java:1766)\n> \tat org.apache.derby.impl.sql.compile.DMLStatementNode.optimizeStatement(DMLStatementNode.java:305)\n> \tat org.apache.derby.impl.sql.compile.CursorNode.optimizeStatement(CursorNode.java:515)\n> \tat org.apache.derby.impl.sql.GenericStatement.prepMinion(GenericStatement.java:365)\n> \tat org.apache.derby.impl.sql.GenericStatement.prepare(GenericStatement.java:88)\n> \tat org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(GenericLanguageConnectionContext.java:768)\n> \tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(EmbedPreparedStatement.java:128)\n> \tat org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(EmbedPreparedStatement20.java:82)\n> \tat org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(EmbedPreparedStatement30.java:63)\n> \tat org.apache.derby.jdbc.Driver30.newEmbedPreparedStatement(Driver30.java:99)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(EmbedConnection.java:1475)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(EmbedConnection.java:1356)\n> \tat org.apache.derby.impl.drda.DRDAStatement.prepare(DRDAStatement.java:635)\n> \tat org.apache.derby.impl.drda.DRDAStatement.explicitPrepare(DRDAStatement.java:596)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.parsePRPSQLSTT(DRDAConnThread.java:3618)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.processCommands(DRDAConnThread.java:789)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.run(DRDAConnThread.java:278)\n> Caused by: java.io.IOException: 파일이 이미 있으므로 만들 수 없습니다 (can not create file)\n> \tat java.io.RandomAccessFile.open(Native Method)\n> \tat java.io.RandomAccessFile.<init>(RandomAccessFile.java:204)\n> \tat org.apache.derby.impl.io.DirRandomAccessFile.<init>(DirRandomAccessFile.java:57)\n> \tat org.apache.derby.impl.io.DirRandomAccessFile4.<init>(DirRandomAccessFile4.java:57)\n> \tat org.apache.derby.impl.io.DirFile4.getRandomAccessFile(DirFile4.java:250)\n> \tat org.apache.derby.impl.store.raw.data.RAFContainer.run(RAFContainer.java:1392)\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-06-11T01:08:45Z"
  },
  "patches": [],
  "external_id": "DERBY-3688"
},{
  "_id": {
    "$oid": "5f27bca8a9368823397d90e7"
  },
  "message_id": "<JIRA.12684326.1386874614961.31316.1386929227180@arcas>",
  "mailing_list_id": {
    "$oid": "5f27bb47a9368823397d2ab3"
  },
  "reference_ids": [
    {
      "$oid": "5f27bca8a9368823397d90e4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bca8a9368823397d90e4"
  },
  "from_id": {
    "$oid": "596779fcaff2204b3cbd12a7"
  },
  "to_ids": [
    {
      "$oid": "5bbe40b857674ee167452f7d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JENA-610) Remote ASK queries do not close the\n connection.",
  "body": "\n    [ https://issues.apache.org/jira/browse/JENA-610?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13847355#comment-13847355 ] \n\nAndy Seaborne commented on JENA-610:\n------------------------------------\n\nI have failed to reproduce this - the test was a loop of 1000's of execAsk.  The server did not create any more threads nor lock up in thread congestion.\n\nResponses to execAsk are small (about 400 bytes in the SPARQL XML result form including HTTP response header), and fits in TCP buffering.  Therefore, the server will write the result and complete the HTTP request regardless of what the client does.\n\n\n> Remote ASK queries do not close the connection.\n> -----------------------------------------------\n>\n>                 Key: JENA-610\n>                 URL: https://issues.apache.org/jira/browse/JENA-610\n>             Project: Apache Jena\n>          Issue Type: Bug\n>          Components: ARQ\n>    Affects Versions: Jena 2.10.1\n>            Reporter: Andy Seaborne\n>            Assignee: Andy Seaborne\n>\n> Reported in \n> http://mail-archives.apache.org/mod_mbox/jena-users/201312.mbox/%3CCAHYYBPKkLtPh9n0Ue-S9h0TgwVteWx9fgy0ewOoUWxrbGnPFSA%40mail.gmail.com%3E\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.4#6159)\n",
  "date": {
    "$date": "2013-12-13T10:07:07Z"
  },
  "patches": [],
  "external_id": "JENA-610"
},{
  "_id": {
    "$oid": "5c5015146b85f47dd6c3bd31"
  },
  "message_id": "<397564390.1234792143961.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5c5011fb6b85f47dd6c39f9c"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5c5015146b85f47dd6c3bd2e"
  },
  "from_id": {
    "$oid": "59bf949cf2a4565fe9f07fb0"
  },
  "to_ids": [
    {
      "$oid": "5c5012bb621a9a77b35fb11d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (JSPWIKI-506) Click on \"More...\"  leads to page\n MoreMenu",
  "body": "\n    [ https://issues.apache.org/jira/browse/JSPWIKI-506?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12673877#action_12673877 ] \n\nJanne Jalkanen commented on JSPWIKI-506:\n----------------------------------------\n\nCreate the MoreMenu page, but make it empty.  That solves your issue.\n\nMoreMenu is one of the pages which should be installed by default, but obviously when you are upgrading, it's sometimes missing.\n\nMoreMenu works logically exactly the same way as LeftMenu and LeftMenuFooter, and it would be illogical to have it function differently.  The idea is that if the page is missing, the users are empowered to go and create one.\n\nPropose WONTFIX.\n\n> Click on \"More...\"  leads to page MoreMenu\n> ------------------------------------------\n>\n>                 Key: JSPWIKI-506\n>                 URL: https://issues.apache.org/jira/browse/JSPWIKI-506\n>             Project: JSPWiki\n>          Issue Type: Bug\n>    Affects Versions: 2.8.1\n>            Reporter: Bruno Peeters\n>            Priority: Minor\n>\n> Clicking on the \"More...\" tab on a wiki page currently leads to a page MoreMenu (eg http://www.jspwiki.org/wiki/MoreMenu). From a user perspective this is not logical.  In our environment there isn't even such a page. Confronting the user with a choice \"This page does not exist. Why don't you go and create it?\" creates unnecessary confusion.\n> It would be more userfriendly if the user would remain on the current page. This can be easily achieved by adapting pageactionstop.jsp by removing the hyperlink to MoreMenu.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-02-16T05:49:03Z"
  },
  "patches": [],
  "external_id": "JSPWIKI-506"
},{
  "_id": {
    "$oid": "5f27b9e2641061285052d6e4"
  },
  "message_id": "<JIRA.12731936.1407220632346.26579.1407344594549@arcas>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b9e2641061285052d6ba"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b9e2641061285052d6ba"
  },
  "from_id": {
    "$oid": "58bfc8c302ca40f8bf14789e"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PHOENIX-1146) Detect stale client region cache\n on server and retry scans in split regions",
  "body": "\n    [ https://issues.apache.org/jira/browse/PHOENIX-1146?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14087903#comment-14087903 ] \n\nHudson commented on PHOENIX-1146:\n---------------------------------\n\nSUCCESS: Integrated in Phoenix | Master | Hadoop1 #317 (See [https://builds.apache.org/job/Phoenix-master-hadoop1/317/])\nPHOENIX-1146 Detect stale client region cache on server and retry scans in split regions (jtaylor: rev 1d6f072cd135bb7f96f7342934f364258a79e867)\n* phoenix-core/src/main/java/org/apache/phoenix/schema/StaleRegionBoundaryCacheException.java\n* phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java\n* phoenix-core/src/it/java/org/apache/phoenix/end2end/SkipScanAfterManualSplitIT.java\n* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java\n* phoenix-core/src/main/java/org/apache/phoenix/util/ServerUtil.java\n* phoenix-core/src/main/java/org/apache/phoenix/util/SchemaUtil.java\n* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java\n* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java\n* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java\n* phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java\n\n\n> Detect stale client region cache on server and retry scans in split regions\n> ---------------------------------------------------------------------------\n>\n>                 Key: PHOENIX-1146\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-1146\n>             Project: Phoenix\n>          Issue Type: Bug\n>    Affects Versions: 5.0.0, 3.1, 4.1\n>            Reporter: James Taylor\n>            Assignee: James Taylor\n>             Fix For: 5.0.0, 3.1, 4.1\n>\n>         Attachments: PHOENIX-1146.patch\n>\n>\n> HBase cannot recover correctly from an aggregate scan run on the coprocessor side (see HBASE-116670). This can lead to incorrect query results the first time a query is run after a split occurs (due to the region boundary cache being stale). Phoenix can work around this by:\n> - detecting on server before the scan starts that the region cache used by the client is out-of-date. This can be done up-front because the start/stop row of the scan should never span across a region boundary. In this case, a DoNotRetryIOException is thrown with some embedded information to cause a StaleRegionBoundaryCacheException to be thrown on the client.\n> - catching this exception on the client (in ParallelIterators), refreshing the region boundary cache, and re-running the necessary scans based on the new region boundaries.\n> - detecting if this happens more than N times to prevent any kind of excessive looping due to splits occurring over and over again.\n> Phoenix has additional requirements above and beyond standard HBase clients, so even if HBase could recover from this situation, Phoenix would likely need this workaround to ensure that a scan does not span across region boundaries. This is required when the client is doing a merge sort on the results of the parallel scans, mainly in ORDER BY (including topN) and local indexing, and potentially GROUP BY if we move toward sorting the distinct groups on the server side.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-08-06T17:03:14Z"
  },
  "patches": [],
  "external_id": "PHOENIX-1146"
},{
  "_id": {
    "$oid": "5bea9d9d9e73d744d412c810"
  },
  "message_id": "<16012749.1181652206241.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9d8f9e73d744d412c595"
  },
  "from_id": {
    "$oid": "5bea9b8235e3ea2b7b565eba"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-2763) In the Network Client InputStreams and\n Readers returned from LOB's should be sensitive to underlying LOB data\n changes.",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-2763?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nV.Narayanan updated DERBY-2763:\n-------------------------------\n\n    Attachment: UpdateSensitiveStreamsForClient_v1.stat\n                UpdateSensitiveStreamsForClient_v1.diff\n\nI concur with the support voiced for Approach 2 since it is being supported\nby a very valid use case and seems a very logical approach at this\njuncture.\n\nI highly appreciate the guidance offered and thank Rick and Oystein\nfor their invaluable inputs in this patch.\n\nAttached here is the patch that is a extension of the already attached\nApproach_2.txt\n\nIn this patch I have enabled\n\njdbcapi/BlobUpdaetableStreamTest\njdbcapi/ClobupdateableReaderTest\n\nfor the Network Client.\n\nI have added tests for the case when a stream\nis taken from a empty Clob or a empty Blob\n(con.createClob or con.createBlob) to\n\njdbc4/BlobTest\njdbc4/ClobTest\n\nPls find a detailed explanation for this approach in my\nprevious attachment Approach_2.txt\n\n\n\n> In the Network Client InputStreams and Readers returned from LOB's should be sensitive to underlying LOB data changes.\n> ----------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-2763\n>                 URL: https://issues.apache.org/jira/browse/DERBY-2763\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Network Client\n>    Affects Versions: 10.3.0.0\n>            Reporter: V.Narayanan\n>            Assignee: V.Narayanan\n>             Fix For: 10.3.0.0\n>\n>         Attachments: Approach_2.diff, Approach_2.stat, Approach_2.txt, Approach_3.diff, Approach_3.stat, Approach_4.diff, Approach_4.stat, LOBLengthPersists.java, UpdateSensitiveStreamsForClient_v1.diff, UpdateSensitiveStreamsForClient_v1.stat\n>\n>\n> Currently the Embedded and Network Client would differ \n> in behaviour when the following series of steps is \n> followed.\n> a) Create an empty Blob\n> b) get an InputStream using Blob.getBinaryStream()\n> c) write data into this Blob\n>    c.1) Get an OutputStream\n>    c.2) Use OutputStream.write(byte [] b) to write\n>         into this Blob.\n> d) Now read from the InputStream obtained in step b)\n>    and print the number of bytes read as output.\n> The output of step d) differs in the client and in the Embedded side.\n> In the Client\n> -------------\n> The number of bytes read would always be -1.\n> In the Embedded\n> ---------------\n> The number of bytes would be the number of bytes we\n> reflected.\n> The above behaviour in the NetworkClient is because\n> the length of the Blob is read once and stored in the \n> constructor of the locator Stream returned (in the \n> attribute maxPos).\n> This instead should be read each time we use the streams.\n> A similar issue exists for Clobs also.\n> I will raise a seperate JIRA issue for this.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-06-12T05:43:26Z"
  },
  "patches": [],
  "external_id": "DERBY-2763"
},{
  "_id": {
    "$oid": "5bdc024716772b6055c802d3"
  },
  "message_id": "<JIRA.12693731.1391719698697.331.1391722219605@arcas>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdc024716772b6055c802ce"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdc024716772b6055c802ce"
  },
  "from_id": {
    "$oid": "58c9de5702ca40f8bf20c1ae"
  },
  "to_ids": [
    {
      "$oid": "5bdc002135e3ea2b7bb8a8be"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KNOX-248) XML configuration file to describe\n how to launch Knox as Windows service",
  "body": "\n    [ https://issues.apache.org/jira/browse/KNOX-248?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13893827#comment-13893827 ] \n\nASF subversion and git services commented on KNOX-248:\n------------------------------------------------------\n\nCommit 8ec228fa863c594a6222de00b9b3c5600f6db6e8 in branch refs/heads/master from [~kevin.minder]\n[ https://git-wip-us.apache.org/repos/asf?p=incubator-knox.git;h=8ec228f ]\n\nKNOX-248: XML configuration file to describe how to launch Knox as Windows service\n\n\n> XML configuration file to describe how to launch Knox as Windows service\n> ------------------------------------------------------------------------\n>\n>                 Key: KNOX-248\n>                 URL: https://issues.apache.org/jira/browse/KNOX-248\n>             Project: Apache Knox\n>          Issue Type: New Feature\n>    Affects Versions: 0.3.0\n>            Reporter: Yura Tolochkevich\n>             Fix For: 0.4.0\n>\n>         Attachments: KNOX-248.patch\n>\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-06T21:30:19Z"
  },
  "patches": [],
  "external_id": "KNOX-248"
},{
  "_id": {
    "$oid": "5f27d06a532b7277349c3482"
  },
  "message_id": "<14259085.30031285275041595.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d05f532b7277349c30c8"
  },
  "from_id": {
    "$oid": "5f27cf90af02e2d6de8d717f"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (AMQCPP-316) Unable to receive any messages after\n re-starting message broker",
  "body": "\n    [ https://issues.apache.org/activemq/browse/AMQCPP-316?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=62053#action_62053 ] \n\nHelen Huang commented on AMQCPP-316:\n------------------------------------\n\nThank you very much for the fix! I got the code in the 3.2.x branch today and tested it. The MessageSender can reconnect without crashing using the failover transport. \n\n\n\n\n> Unable to receive any messages after re-starting message broker\n> ---------------------------------------------------------------\n>\n>                 Key: AMQCPP-316\n>                 URL: https://issues.apache.org/activemq/browse/AMQCPP-316\n>             Project: ActiveMQ C++ Client\n>          Issue Type: Bug\n>          Components: CMS Impl\n>    Affects Versions: 3.2.0, 3.2.1, 3.2.2, 3.2.3\n>         Environment: Windows xp service pack 3, ActiveMQ broker 5.3.1, apr 1.4.2, apr-util 1.3.9, apr iconv 1.2.1\n>            Reporter: Helen Huang\n>            Assignee: Timothy Bish\n>            Priority: Critical\n>             Fix For: 3.2.4, 3.3.0\n>\n>         Attachments: ReconnectionTest-new.zip, ReconnectionTest.zip\n>\n>\n> We developed two applications that use CmsTemplate to send and receive messages. The sender application is called MessageSender, and the receiver application is called MessageListener. We found that the MessageListener is unable to receive any messages after we re-start the message broker.\n> The followings are the steps to recreate the problem:\n> (1) start the activemq message broker,\n> (2) start MessageListener and MessageSender, and observe that messages are being sent and received successfully.\n> (3) stop the message broker without stopping MessageListener and MessageSender. Wait for a while (for about a minute or two)\n> (4) start the message broker again.\n> We expect we can send and receive messages successfully after step (4), but the MessageListener can never receive any messages any more. Also from the activemq admin page, we find that the consumer of the topic is gone. We did the test with url \"?keepAlive=true&wireFormat.maxInactivityDuration=0\", but it did not work.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-09-23T16:50:41Z"
  },
  "patches": [],
  "external_id": "AMQCPP-316"
},{
  "_id": {
    "$oid": "5f27ceae442ab9b9860f3af0"
  },
  "message_id": "<JIRA.12750398.1414171774000.440864.1415322633763@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ce8b442ab9b9860f30a7"
    },
    {
      "$oid": "5f27ce8b442ab9b9860f30a8"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ce8b442ab9b9860f30a7"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a6"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (OOZIE-2045) Symlink support for sharelib",
  "body": "\n    [ https://issues.apache.org/jira/browse/OOZIE-2045?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14201346#comment-14201346 ] \n\nHadoop QA commented on OOZIE-2045:\n----------------------------------\n\nTesting JIRA OOZIE-2045\n\nCleaning local git workspace\n\n----------------------------\n\n{color:green}+1 PATCH_APPLIES{color}\n{color:green}+1 CLEAN{color}\n{color:green}+1 RAW_PATCH_ANALYSIS{color}\n.    {color:green}+1{color} the patch does not introduce any @author tags\n.    {color:green}+1{color} the patch does not introduce any tabs\n.    {color:green}+1{color} the patch does not introduce any trailing spaces\n.    {color:green}+1{color} the patch does not introduce any line longer than 132\n.    {color:green}+1{color} the patch does adds/modifies 1 testcase(s)\n{color:green}+1 RAT{color}\n.    {color:green}+1{color} the patch does not seem to introduce new RAT warnings\n{color:green}+1 JAVADOC{color}\n.    {color:green}+1{color} the patch does not seem to introduce new Javadoc warnings\n{color:red}-1 COMPILE{color}\n.    {color:green}+1{color} HEAD compiles\n.    {color:green}+1{color} patch compiles\n.    {color:red}-1{color} the patch seems to introduce 1 new javac warning(s)\n{color:green}+1 BACKWARDS_COMPATIBILITY{color}\n.    {color:green}+1{color} the patch does not change any JPA Entity/Colum/Basic/Lob/Transient annotations\n.    {color:green}+1{color} the patch does not modify JPA files\n{color:green}+1 TESTS{color}\n.    Tests run: 1552\n{color:green}+1 DISTRO{color}\n.    {color:green}+1{color} distro tarball builds with the patch \n\n----------------------------\n{color:red}*-1 Overall result, please check the reported -1(s)*{color}\n\n\nThe full output of the test-patch run is available at\n\n.   https://builds.apache.org/job/oozie-trunk-precommit-build/2082/\n\n> Symlink support for sharelib\n> ----------------------------\n>\n>                 Key: OOZIE-2045\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-2045\n>             Project: Oozie\n>          Issue Type: Bug\n>            Reporter: Purshotam Shah\n>            Assignee: Purshotam Shah\n>         Attachments: OOZIE-2045-V2.patch, OOZIE-2045-V3.patch, OOZIE-2045-V4.patch\n>\n>\n> With metafile configuration, one can configure sharelib (eg. pig_latest) to latest version available.\n> If new version is available, admin will just change the pig_latest sysmlink to new version. \n> There will be no change to Oozie or metafile configuration.\n> Oozie should be able to detect symlink change and reloads the sharelib.\n> metafile configuration can look like\n> oozie.pig_latest=hdfs:///tmp/pig_latest\n> where /tmp/pig_latest is a symlink pointing to latest available version.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-11-07T01:10:33Z"
  },
  "patches": [],
  "external_id": "OOZIE-2045"
},{
  "_id": {
    "$oid": "5f27b99d641061285052c075"
  },
  "message_id": "<JIRA.12754077.1415606823000.65143.1421104054722@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b8be6410612850527855"
    },
    {
      "$oid": "5f27b8be6410612850527856"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b8be6410612850527855"
  },
  "from_id": {
    "$oid": "5bbd8d2257674ee167ceb0ba"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Issue Comment Deleted] (PHOENIX-1428) Keep scanner open on\n server and pace by client instead of spooling",
  "body": "\n     [ https://issues.apache.org/jira/browse/PHOENIX-1428?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJames Taylor updated PHOENIX-1428:\n----------------------------------\n    Comment: was deleted\n\n(was: Argh. Sorry about that. Anyway to correct that?)\n\n> Keep scanner open on server and pace by client instead of spooling\n> ------------------------------------------------------------------\n>\n>                 Key: PHOENIX-1428\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-1428\n>             Project: Phoenix\n>          Issue Type: Bug\n>            Reporter: James Taylor\n>            Assignee: Dave Hacker\n>\n> Instead of spooling a batch of results for all chunked scans to the client, keep the scan open and pace it through pre-fetching. This will perform much better for a full table scan with a LIMIT = 1, as this case currently will run a scan for every guidepost, each returning a single row.\n> [~lhofhansl]\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-01-12T23:07:34Z"
  },
  "patches": [],
  "external_id": "PHOENIX-1428"
},{
  "_id": {
    "$oid": "5f27d22146816ce7cf501ebe"
  },
  "message_id": "<101974805.1136939701784.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d22046816ce7cf501e77"
  },
  "from_id": {
    "$oid": "5bbe410157674ee1674bf19b"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Reopened: (MNG-1860) maven.* project references no longer\n resolve in ant tasks.",
  "body": "     [ http://jira.codehaus.org/browse/MNG-1860?page=all ]\n     \nBrett Porter reopened MNG-1860:\n-------------------------------\n\n\n> maven.* project references no longer resolve in ant tasks.\n> ----------------------------------------------------------\n>\n>          Key: MNG-1860\n>          URL: http://jira.codehaus.org/browse/MNG-1860\n>      Project: Maven 2\n>         Type: Bug\n\n>   Components: Ant tasks\n>     Versions: 2.0.1\n>     Reporter: John Casey\n>     Assignee: John Casey\n>     Priority: Blocker\n>      Fix For: 2.0.2\n\n>\n>\n> from original email:\n> The following reference in ant run used to work. Now it resolves to\n> null.null. Did something fundamental change or is this a bug?\n>               <unzip src=\"${maven.build.finalName}.${maven.packaging}\"\n> dest=\"target/classes\"></unzip>       \n>   \n> From follow-up email:\n> changing to pom.* fixes the problem\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2006-01-10T18:35:01Z"
  },
  "patches": [],
  "external_id": "MNG-1860"
},{
  "_id": {
    "$oid": "5bbdf641e8113566f664cf65"
  },
  "message_id": "<229089892.10268.1314211109731.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "58bfcbd1e4f89451f55cdfd2"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "subject": "[jira] [Created] (NUTCH-1092) overhaul FAQ's and publish to Nutch\n site",
  "date": {
    "$date": "2011-08-24T18:38:29Z"
  },
  "from_id": {
    "$oid": "58c9e0c402ca40f8bf20c547"
  },
  "body": "overhaul FAQ's and publish to Nutch site\n----------------------------------------\n\n                 Key: NUTCH-1092\n                 URL: https://issues.apache.org/jira/browse/NUTCH-1092\n             Project: Nutch\n          Issue Type: Sub-task\n          Components: documentation\n    Affects Versions: 1.4, 2.0\n            Reporter: Lewis John McGibbney\n             Fix For: 1.4, 2.0\n\n\nWe require a complete overhaul of the FAQ's on the Wiki. Once this is accomplished they need to be pushed into the Nutch site. \n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "external_id": "NUTCH-1092"
},{
  "_id": {
    "$oid": "60fac2cbd907ab79037e5e01"
  },
  "message_id": "<JIRA.13274408.1576241124000.318029.1576251900362@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac2c7d907ab79037e5c33"
    },
    {
      "$oid": "60fac2c7d907ab79037e5c34"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2c7d907ab79037e5c33"
  },
  "from_id": {
    "$oid": "60fac2cbf73e2aa390c801e6"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DIRSTUDIO-1244) Build failure on trunk/master:\n Missing requirement: org.apache.directory.api.ldap.client.api\n 2.0.0.AM5-SNAPSHOT requires 'java.package; org.apache.commons.pool2 2.7.0'\n but it could not be found",
  "body": "\n    [ https://issues.apache.org/jira/browse/DIRSTUDIO-1244?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16995714#comment-16995714 ] \n\nGraham Leggett commented on DIRSTUDIO-1244:\n-------------------------------------------\n\nBizarrely, upgrading directory-studio's pom to machine the commons-pool2 version on the directory-api's pom fixes this.\n\nhttps://github.com/apache/directory-studio/pull/25\n\n> Build failure on trunk/master: Missing requirement: org.apache.directory.api.ldap.client.api 2.0.0.AM5-SNAPSHOT requires 'java.package; org.apache.commons.pool2 2.7.0' but it could not be found\n> -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DIRSTUDIO-1244\n>                 URL: https://issues.apache.org/jira/browse/DIRSTUDIO-1244\n>             Project: Directory Studio\n>          Issue Type: Bug\n>          Components: studio-build\n>            Reporter: Graham Leggett\n>            Priority: Major\n>\n> When attempting to build trunk/master of Apache Directory Studio, the build fails with reference to non-existent dependencies:\n> {noformat}\n> [ERROR] Cannot resolve target definition:\n> [ERROR]   Software being installed: org.apache.directory.api.ldap.client.api 2.0.0.AM5-SNAPSHOT\n> [ERROR]   Missing requirement: org.apache.directory.api.ldap.client.api 2.0.0.AM5-SNAPSHOT requires 'java.package; org.apache.commons.pool2 2.7.0' but it could not be found\n> [ERROR] \n> [ERROR] Failed to resolve target definition /Users/minfrin/src/apache/sandbox/directory/directory-studio-trunk/eclipse-trgt-platform/org.apache.directory.studio.eclipse-trgt-platform.target: See log for details -> [Help 1]\n> [ERROR] \n> [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n> [ERROR] Re-run Maven using the -X switch to enable full debug logging.\n> [ERROR] \n> [ERROR] For more information about the errors and possible solutions, please read the following articles:\n> [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MavenExecutionException\n> {noformat}\n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.4#803005)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@directory.apache.org\nFor additional commands, e-mail: dev-help@directory.apache.org\n\n",
  "date": {
    "$date": "2019-12-13T15:45:00Z"
  },
  "patches": [],
  "external_id": "DIRSTUDIO-1244"
},{
  "_id": {
    "$oid": "60fac402d907ab79037ec308"
  },
  "message_id": "<2136867321.52354.1329516777295.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac402d907ab79037ec2f5"
  },
  "from_id": {
    "$oid": "5f27c228af02e2d6de6ec6fb"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (DIRSERVER-1696) Creation of an entry like\n cn=test1+cn=test2, ou=system should not be allowed",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSERVER-1696?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEmmanuel Lecharny resolved DIRSERVER-1696.\n------------------------------------------\n\n    Resolution: Fixed\n\nFixed with http://svn.apache.org/viewvc?rev=1245708&view=rev\n                \n> Creation of an entry like cn=test1+cn=test2, ou=system should not be allowed\n> ----------------------------------------------------------------------------\n>\n>                 Key: DIRSERVER-1696\n>                 URL: https://issues.apache.org/jira/browse/DIRSERVER-1696\n>             Project: Directory ApacheDS\n>          Issue Type: Bug\n>    Affects Versions: 2.0.0-M5\n>            Reporter: Emmanuel Lecharny\n>            Priority: Minor\n>             Fix For: 2.0.0-M6\n>\n>\n> Currently, we can create an entry like :\n> dn: cn=test1+cn=test2,ou=system\n> objectClass: person\n> cn: test1\n> cn: test2\n> sn: Test\n> This is not allowed by X501, as explained in par 9.3 :\n> \"The set that forms an RDN contains exactly one AttributeTypeAndDistinguishedValue for each attribute which contains distinguished values in the entry; that is, a given attribute type cannot appear twice in the same RDN.\"\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-02-17T22:12:57Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-1696"
},{
  "_id": {
    "$oid": "58c11b6c6d2aba458dded525"
  },
  "message_id": "<20120207125740.GV26355@dusk.harfang.homelinux.org>",
  "mailing_list_id": {
    "$oid": "58c117176d2aba458dde60bb"
  },
  "reference_ids": [
    {
      "$oid": "58c11b6b6d2aba458dded51c"
    },
    {
      "$oid": "58c11b6b6d2aba458dded518"
    },
    {
      "$oid": "58c11b6b6d2aba458dded519"
    },
    {
      "$oid": "58c11b6b6d2aba458dded51e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58c11b6b6d2aba458dded51e"
  },
  "from_id": {
    "$oid": "58c1136702ca40f8bfb1e460"
  },
  "to_ids": [
    {
      "$oid": "58c1132fe4f89451f51d7403"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [math] Major speed improvement to 1D FFT (MATH-732)",
  "body": "Hi.\n\n> [...]\n> >> >\n> >> > I remember we recently had this conversation on the ML: one of Gilles\n> >> > or Luc argued that for very low-level algorithms, it didn't really\n> >> > matter if the parameters were passed as \"very crude\" objects, since it\n> >> > was most likely that the user would have to write an adapter to its\n> >> > own data format. So I would suggest we give up Complex[] altogether in\n> >> > the interface of FFT, and replace it with double[] arrays, with the\n> >> > following layout :\n> >> > data[2 * i] = real part\n> >> > data[2 * i + 1] = imaginary part.\n> >> >\n> >> > What do you think?\n> >>\n> >> I agree with this view (it may be me who expressed this). A double array\n> >> as an interface for our library seems good to me.\n> >\n> > I'm wary of this sort of \"optimization\" (as it decreases the code clarity).\n> >\n> > -1 until it has been proven that it brings a _significant_ performance\n> >   improvement.\n> > At the moment, I would of course keep the internal switch to arrays of\n> > primitives but also the API that takes and returns \"Complex[]\" objects.\n> >\n> Well, in fact this change was not really thought of as an\n> optimization. It seemed to me that this data layout was more\n> natural... But it's probably because I'm used to it. I should mention\n> that having to create an array of Complex is the very reason why I do\n> not use the transform package for my own calcs. Indeed, my data comes\n> in double[] arrays. So I would have to turn this data into Complex[],\n> which would internally be converted back to double[] arrays...\n\nWherever it makes sense from a user perspective[1], nobody prevents you from\nadding new methods to the API, e.g.:\n---CUT---\npublic double[][] transform(double[][] dataRI) { /* ... */ }\n---CUT---\nAnd you can pass your data to the CM code in a matter of two array reference\nassignments.\n\n> [...]\n\n\nBest,\nGilles\n\n[1] As long as the code remains self-documenting: I'd prefer to avoid, as\n    much as possible, \"out-of-band\" conventions, like striding over a\n    one-dimensional array to get different kinds of data (cf. real vs\n    imaginary part: Another \"natural\" convention could be that the first\n    half of the array contains all the real parts and the second half all\n    the imaginary parts).\n    Of course the \"double[][]\" also poses the question of the data ordering,\n    but I have the impression that it is more flexible (and it could even be\n    faster ;-).\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@commons.apache.org\nFor additional commands, e-mail: dev-help@commons.apache.org\n\n",
  "date": {
    "$date": "2012-02-07T13:57:40Z"
  },
  "patches": [],
  "external_id": "MATH-732"
},{
  "_id": {
    "$oid": "5bc851176e373d4fe81c31c3"
  },
  "message_id": "<JIRA.12642862.1366177441000.113277.1421638475154@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [
    {
      "$oid": "5bc851176e373d4fe81c31c1"
    },
    {
      "$oid": "5bc851176e373d4fe81c31c2"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc851176e373d4fe81c31c1"
  },
  "from_id": {
    "$oid": "58c11d1d02ca40f8bfb1f631"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1da"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FLUME-2006) in Avro the batch size is called\n batch-size, in all other sources batchSize",
  "body": "\n     [ https://issues.apache.org/jira/browse/FLUME-2006?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAshish Paliwal updated FLUME-2006:\n----------------------------------\n    Attachment: FLUME-2006-1.patch\n\nUpdated patch with review comments incorporated\n\n> in Avro the batch size is called batch-size, in all other sources batchSize\n> ---------------------------------------------------------------------------\n>\n>                 Key: FLUME-2006\n>                 URL: https://issues.apache.org/jira/browse/FLUME-2006\n>             Project: Flume\n>          Issue Type: Bug\n>          Components: Sinks+Sources\n>    Affects Versions: v1.4.0, v1.3.1\n>            Reporter: Alexander Alten-Lorenz\n>            Assignee: Ashish Paliwal\n>            Priority: Trivial\n>         Attachments: FLUME-2006-0.patch, FLUME-2006-1.patch\n>\n>\n> http://mail-archives.apache.org/mod_mbox/flume-user/201304.mbox/%3c746AE600-7783-40F4-9817-25617370C89A@gmail.com%3e\n> The mismatch is with Avro Sink as well as Thrift sink. Other sinks use batchSize as param name\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-01-19T03:34:35Z"
  },
  "patches": [],
  "external_id": "FLUME-2006"
},{
  "_id": {
    "$oid": "5bacb42dfaaadd76f8aa6387"
  },
  "message_id": "<522989733.1257664112506.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb42bfaaadd76f8aa62ec"
  },
  "from_id": {
    "$oid": "5bacb0be57674ee167d4b157"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (PIG-1038) Optimize nested distinct/sort to use\n secondary key",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-1038?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDaniel Dai updated PIG-1038:\n----------------------------\n\n    Status: Patch Available  (was: Open)\n\n> Optimize nested distinct/sort to use secondary key\n> --------------------------------------------------\n>\n>                 Key: PIG-1038\n>                 URL: https://issues.apache.org/jira/browse/PIG-1038\n>             Project: Pig\n>          Issue Type: Improvement\n>          Components: impl\n>    Affects Versions: 0.4.0\n>            Reporter: Olga Natkovich\n>            Assignee: Daniel Dai\n>             Fix For: 0.6.0\n>\n>         Attachments: PIG-1038-1.patch\n>\n>\n> If nested foreach plan contains sort/distinct, it is possible to use hadoop secondary sort instead of SortedDataBag and DistinctDataBag to optimize the query. \n> Eg1:\n> A = load 'mydata';\n> B = group A by $0;\n> C = foreach B {\n>     D = order A by $1;\n>     generate group, D;\n> }\n> store C into 'myresult';\n> We can specify a secondary sort on A.$1, and drop \"order A by $1\".\n> Eg2:\n> A = load 'mydata';\n> B = group A by $0;\n> C = foreach B {\n>     D = A.$1;\n>     E = distinct D;\n>     generate group, E;\n> }\n> store C into 'myresult';\n> We can specify a secondary sort key on A.$1, and simplify \"D=A.$1; E=distinct D\" to a special version of distinct, which does not do the sorting.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-11-08T07:08:32Z"
  },
  "patches": [],
  "external_id": "PIG-1038"
},{
  "_id": {
    "$oid": "5bacc5c056f6a00b02098828"
  },
  "message_id": "<847745895.43284.1323128320362.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacc5a956f6a00b020981c5"
  },
  "from_id": {
    "$oid": "5bacc5ab57674ee167dfa015"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Issue Comment Edited] (MAHOUT-817) Add PCA options to SSVD\n code",
  "body": "\n    [ https://issues.apache.org/jira/browse/MAHOUT-817?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13163172#comment-13163172 ] \n\nDmitriy Lyubimov edited comment on MAHOUT-817 at 12/5/11 11:38 PM:\n-------------------------------------------------------------------\n\nSo i did an R simulation of column-wise mean and it seems to work , so i think this verifies the math.\n\nI still need to finish the doc (it also has a little typo in it), i will be finishing it from home as i don't seem to have the doc source on me here. \n\nI guess it clears the implementation on existing ssvd solver.\n\ntest results comparing \"brute forced\" svd with \"median propagated\" version: \n{code}\n\n\n> respci$svalues\n [1] 9.9995227 8.9992220 7.9907894 6.9860235 5.9786348 4.9866553 3.9853651\n [8] 2.9735904 1.9999941 0.9971344\n> ressvd$svalues\n [1] 9.9995227 8.9992220 7.9907894 6.9860235 5.9786348 4.9866553 3.9853651\n [8] 2.9735904 1.9999941 0.9971344\n> \n{code}\n                \n      was (Author: dlyubimov):\n    So i did an R simulation of column-wise mean and it seems to work , so i think this verifies the math.\n\nI still need to finish the doc (it also has a little typo in it), i will be finishing it from home as i don't seem to have the doc source on me here. \n\nI guess it clears the implementation on existing ssvd solver.\n                  \n> Add PCA options to SSVD code\n> ----------------------------\n>\n>                 Key: MAHOUT-817\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-817\n>             Project: Mahout\n>          Issue Type: New Feature\n>    Affects Versions: 0.6\n>            Reporter: Dmitriy Lyubimov\n>            Assignee: Dmitriy Lyubimov\n>             Fix For: Backlog\n>\n>         Attachments: SSVD-PCA options.pdf, ssvd-tests.R, ssvd.R, ssvd.m\n>\n>\n> It seems that a simple solution should exist to integrate PCA mean subtraction into SSVD algorithm without making it a pre-requisite step and also avoiding densifying the big input. \n> Several approaches were suggested:\n> 1) subtract mean off B\n> 2) propagate mean vector deeper into algorithm algebraically where the data is already collapsed to smaller matrices\n> 3) --?\n> It needs some math done first . I'll take a stab at 1 and 2 but thoughts and math are welcome.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-12-05T23:38:40Z"
  },
  "patches": [],
  "external_id": "MAHOUT-817"
},{
  "_id": {
    "$oid": "5c580766e078b00ec4e76aa6"
  },
  "message_id": "<JIRA.12955817.1459786650000.147243.1459921945473@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c580763e078b00ec4e76a6b"
    },
    {
      "$oid": "5c580763e078b00ec4e76a6c"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c580763e078b00ec4e76a6b"
  },
  "from_id": {
    "$oid": "5c5802f5621a9a77b3bef360"
  },
  "to_ids": [
    {
      "$oid": "5c58018a621a9a77b3bd9c56"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (RANGER-912) Ranger Admin UI to support datamask\n & row-filter policies",
  "body": "\n    [ https://issues.apache.org/jira/browse/RANGER-912?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15227776#comment-15227776 ] \n\nGautam Borad commented on RANGER-912:\n-------------------------------------\n\nCommitted to master by Madhan : https://github.com/apache/incubator-ranger/commit/19c3744b1ece45be9b33a6075472bfc37bc9bd28\n\n> Ranger Admin UI to support datamask & row-filter policies\n> ---------------------------------------------------------\n>\n>                 Key: RANGER-912\n>                 URL: https://issues.apache.org/jira/browse/RANGER-912\n>             Project: Ranger\n>          Issue Type: Bug\n>          Components: admin\n>    Affects Versions: 0.6.0\n>            Reporter: Madhan Neethiraj\n>            Assignee: Gautam Borad\n>         Attachments: RANGER-912.patch\n>\n>\n> Ranger policy model has been updated to support data-masking and row-filtering policy types for components like Hive. Policy UI should be updated to support these new types of policies.\n> References:\n> RANGER-873: Ranger Policy model to support data masking\n> RANGER-908: Ranger policy model to support row-filtering\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-04-06T05:52:25Z"
  },
  "patches": [],
  "external_id": "RANGER-912"
},{
  "_id": {
    "$oid": "5bbdac62c764eb6c7a264235"
  },
  "message_id": "<JIRA.12832197.1432312221000.11343.1432312277798@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdac1dc764eb6c7a263a83"
    },
    {
      "$oid": "5bbdac1cc764eb6c7a263a82"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdac1cc764eb6c7a263a82"
  },
  "from_id": {
    "$oid": "5bbdac1c57674ee167da556a"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FALCON-1236) Tests with radio buttons on Mirror\n wizard page",
  "body": "\n     [ https://issues.apache.org/jira/browse/FALCON-1236?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRuslan Ostafiychuk updated FALCON-1236:\n---------------------------------------\n    Attachment: FALCON-1236.patch\n\n> Tests with radio buttons on Mirror wizard page\n> ----------------------------------------------\n>\n>                 Key: FALCON-1236\n>                 URL: https://issues.apache.org/jira/browse/FALCON-1236\n>             Project: Falcon\n>          Issue Type: Test\n>          Components: merlin\n>    Affects Versions: 0.7\n>            Reporter: Ruslan Ostafiychuk\n>            Assignee: Ruslan Ostafiychuk\n>             Fix For: 0.7\n>\n>         Attachments: FALCON-1236.patch\n>\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-05-22T16:31:17Z"
  },
  "patches": [],
  "external_id": "FALCON-1236"
},{
  "_id": {
    "$oid": "5bea9a469e73d744d4121d3b"
  },
  "message_id": "<1818489022.49039.1306514147354.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea99d19e73d744d4120645"
  },
  "from_id": {
    "$oid": "5bea981135e3ea2b7b4e616e"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Issue Comment Edited] (DERBY-5248) Java Process Crash\n Causes Corrupt DB",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-5248?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13040309#comment-13040309 ] \n\nDag H. Wanvik edited comment on DERBY-5248 at 5/27/11 4:35 PM:\n---------------------------------------------------------------\n\nRunning recovery with -Dderby.debug.true=LogTrace I see the last operations before the crash:\n\n:\nDEBUG LogTrace OUTPUT: About to call undo(), transaction table =\n**************************\norg.apache.derby.impl.store.raw.xact.TransactionTable@a9255c\nTransaction Table: size = 2 largestUpdateXactId = 10946\nXid=10946 gid=null firstLog=(1,30637908) lastLog=null transactionStatus=16 myxact=10944 update=true recovery=true prepare=false needExclusion=true\nXid=10945 gid=null firstLog=(1,30637791) lastLog=null transactionStatus=0 myxact=10944 update=true recovery=true prepare=false needExclusion=true\n---------------------------\nDEBUG LogTrace OUTPUT: In recovery undo, rollback inflight transactions\nDEBUG LogTrace OUTPUT: \nUndo transaction: 10946start at end of log stop at (1,30637908)\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30696447) : Page Operation: Page(1,Container(0, 1409)) pageVersion 27734 : Purge : 1 slots starting at 1 (recordId=8038)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30696783) undoinstant (1,30696447)\n:\n:\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30638074) : Page Operation: Page(1,Container(0, 1409)) pageVersion 26835 : Purge : 1 slots starting at 925 (recordId=9123)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30736339) undoinstant (1,30638074)\n\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30638009) : Page Operation: Page(1,Container(0, 1409)) pageVersion 26834 : Purge : 1 slots starting at 926 (recordId=9124)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30736383) undoinstant (1,30638009)\n\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30637944) : Page Operation: Page(1,Container(0, 1409)) pageVersion 26833 : Purge : 1 slots starting at 928 (recordId=9126)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30736427) undoinstant (1,30637944)\n\nException trace: \norg.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED incorrect deleted row count.  Should be: 903, instead got: 902, maxSlot = 1255, recordCount = 1255\n:\n\n\n      was (Author: dagw):\n    Running recovery with -Dderby.debug.true=LogTrace I see the last operations before the crash:\n\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30638074) : Page Operation: Page(1,Container(0, 1409)) pageVersion 26835 : Purge : 1 slots starting at 925 (recordId=9123)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30736339) undoinstant (1,30638074)\n\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30638009) : Page Operation: Page(1,Container(0, 1409)) pageVersion 26834 : Purge : 1 slots starting at 926 (recordId=9124)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30736383) undoinstant (1,30638009)\n\nDEBUG LogTrace OUTPUT: Rollback log record at instant (1,30637944) : Page Operation: Page(1,Container(0, 1409)) pageVersion 26833 : Purge : 1 slots starting at 928 (recordId=9126)\nDEBUG LogTrace OUTPUT: Write CLR: Xact: 10946clrinstant: (1,30736427) undoinstant (1,30637944)\n\nException trace: \norg.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED incorrect deleted row count.  Should be: 903, instead got: 902, maxSlot = 1255, recordCount = 1255\n:\n  \n> Java Process Crash Causes Corrupt DB\n> ------------------------------------\n>\n>                 Key: DERBY-5248\n>                 URL: https://issues.apache.org/jira/browse/DERBY-5248\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Store\n>    Affects Versions: 10.8.1.2\n>         Environment: Red Hat Enterprise Linux ES release 4 (Nahant Update 7)\n> java version \"1.6.0_01\"\n> Java(TM) SE Runtime Environment (build 1.6.0_01-b06)\n> Java HotSpot(TM) Server VM (build 1.6.0_01-b06, mixed mode)\n> Derby 10.8.1.2\n>            Reporter: Tim Wu\n>             Fix For: 10.8.1.3\n>\n>         Attachments: log.ctrl.xml, log1.dat.xml.gz, objectdb.zip\n>\n>\n> During some crash tests on our product, we hit an issue where after a crash, the derby db is stuck in a corrupted state. The db fails to boot with the following error when started up with the debug jar:\n> org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED incorrect deleted row count. Should be: 903, instead got: 902, maxSlot = 1255, recordCount = 1255 \n> at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:162) \n> at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:147) \n> at org.apache.derby.impl.store.raw.data.BasePage.internalNonDeletedRecordCount(BasePage.java:1432) \n> at org.apache.derby.impl.store.raw.data.CachedPage.releaseExclusive(CachedPage.java:488) \n> at org.apache.derby.impl.store.raw.data.StoredPage.releaseExclusive(StoredPage.java:1066) \n> at org.apache.derby.impl.store.raw.data.BasePage.unlatch(BasePage.java:1370) \n> at org.apache.derby.impl.store.raw.data.PageBasicOperation.releaseResource(PageBasicOperation.java:195) \n> at org.apache.derby.impl.store.raw.data.PhysicalUndoOperation.releaseResource(PhysicalUndoOperation.java:177) \n> at org.apache.derby.impl.store.raw.log.FileLogger.undo(FileLogger.java:1055) \n> at org.apache.derby.impl.store.raw.xact.Xact.abort(Xact.java:952) \n> at org.apache.derby.impl.store.raw.xact.XactFactory.rollbackAllTransactions(XactFactory.java:547) \n> at org.apache.derby.impl.store.raw.log.LogToFile.recover(LogToFile.java:1229) \n> at org.apache.derby.impl.store.raw.RawStore.boot(RawStore.java:339) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:531) \n> at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:427) \n> at org.apache.derby.impl.store.access.RAMAccessManager.boot(RAMAccessManager.java:1019) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:531) \n> at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:427) \n> at org.apache.derby.impl.db.BasicDatabase.bootStore(BasicDatabase.java:749) \n> at org.apache.derby.impl.db.BasicDatabase.boot(BasicDatabase.java:177) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(BaseMonitor.java:1816) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(BaseMonitor.java:1682) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(BaseMonitor.java:1560) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(BaseMonitor.java:979) \n> at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Monitor.java:550) \n> at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(EmbedConnection.java:2697) \n> at org.apache.derby.impl.jdbc.EmbedConnection.<init>(EmbedConnection.java:385) \n> at org.apache.derby.impl.jdbc.EmbedConnection30.<init>(EmbedConnection30.java:73) \n> at org.apache.derby.impl.jdbc.EmbedConnection40.<init>(EmbedConnection40.java:51) \n> at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Driver40.java:70) \n> at org.apache.derby.jdbc.InternalDriver.connect(InternalDriver.java:248) \n> at org.apache.derby.jdbc.AutoloadedDriver.connect(AutoloadedDriver.java:144) \n> at java.sql.DriverManager.getConnection(DriverManager.java:582) \n> at java.sql.DriverManager.getConnection(DriverManager.java:154) \n> at OpenDerby.main(OpenDerby.java:17) \n> --------------- \n> Stack traces for all live threads: \n> Thread name=main id=1 priority=5 state=RUNNABLE isdaemon=false \n> java.lang.Thread.dumpThreads(Native Method) \n> java.lang.Thread.getAllStackTraces(Thread.java:1554) \n> org.apache.derby.shared.common.sanity.ThreadDump.getStackDumpString(ThreadDump.java:34) \n> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \n> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) \n> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) \n> java.lang.reflect.Method.invoke(Method.java:597) \n> org.apache.derby.shared.common.sanity.AssertFailure$1.run(AssertFailure.java:165) \n> java.security.AccessController.doPrivileged(Native Method) \n> org.apache.derby.shared.common.sanity.AssertFailure.dumpThreads(AssertFailure.java:159) \n> org.apache.derby.shared.common.sanity.AssertFailure.<init>(AssertFailure.java:72) \n> org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:162) \n> org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:147) \n> org.apache.derby.impl.store.raw.data.BasePage.internalNonDeletedRecordCount(BasePage.java:1432) \n> org.apache.derby.impl.store.raw.data.CachedPage.releaseExclusive(CachedPage.java:488) \n> org.apache.derby.impl.store.raw.data.StoredPage.releaseExclusive(StoredPage.java:1066) \n> org.apache.derby.impl.store.raw.data.BasePage.unlatch(BasePage.java:1370) \n> org.apache.derby.impl.store.raw.data.PageBasicOperation.releaseResource(PageBasicOperation.java:195) \n> org.apache.derby.impl.store.raw.data.PhysicalUndoOperation.releaseResource(PhysicalUndoOperation.java:177) \n> org.apache.derby.impl.store.raw.log.FileLogger.undo(FileLogger.java:1055) \n> org.apache.derby.impl.store.raw.xact.Xact.abort(Xact.java:952) \n> org.apache.derby.impl.store.raw.xact.XactFactory.rollbackAllTransactions(XactFactory.java:547) \n> org.apache.derby.impl.store.raw.log.LogToFile.recover(LogToFile.java:1229) \n> org.apache.derby.impl.store.raw.RawStore.boot(RawStore.java:339) \n> org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:531) \n> org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:427) \n> org.apache.derby.impl.store.access.RAMAccessManager.boot(RAMAccessManager.java:1019) \n> org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:531) \n> org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:427) \n> org.apache.derby.impl.db.BasicDatabase.bootStore(BasicDatabase.java:749) \n> org.apache.derby.impl.db.BasicDatabase.boot(BasicDatabase.java:177) \n> org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> org.apache.derby.impl.services.monitor.BaseMonitor.bootService(BaseMonitor.java:1816) \n> org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(BaseMonitor.java:1682) \n> org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(BaseMonitor.java:1560) \n> org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(BaseMonitor.java:979) \n> org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Monitor.java:550) \n> org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(EmbedConnection.java:2697) \n> org.apache.derby.impl.jdbc.EmbedConnection.<init>(EmbedConnection.java:385) \n> org.apache.derby.impl.jdbc.EmbedConnection30.<init>(EmbedConnection30.java:73) \n> org.apache.derby.impl.jdbc.EmbedConnection40.<init>(EmbedConnection40.java:51) \n> org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Driver40.java:70) \n> org.apache.derby.jdbc.InternalDriver.connect(InternalDriver.java:248) \n> org.apache.derby.jdbc.AutoloadedDriver.connect(AutoloadedDriver.java:144) \n> java.sql.DriverManager.getConnection(DriverManager.java:582) \n> java.sql.DriverManager.getConnection(DriverManager.java:154) \n> OpenDerby.main(OpenDerby.java:17) \n> Thread name=derby.rawStoreDaemon id=12 priority=5 state=TIMED_WAITING isdaemon=true \n> java.lang.Object.wait(Native Method) \n> org.apache.derby.impl.services.daemon.BasicDaemon.rest(BasicDaemon.java:576) \n> org.apache.derby.impl.services.daemon.BasicDaemon.run(BasicDaemon.java:390) \n> java.lang.Thread.run(Thread.java:680) \n> Thread name=Finalizer id=3 priority=8 state=WAITING isdaemon=true \n> java.lang.Object.wait(Native Method) \n> java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118) \n> java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134) \n> java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159) \n> Thread name=Reference Handler id=2 priority=10 state=WAITING isdaemon=true \n> java.lang.Object.wait(Native Method) \n> java.lang.Object.wait(Object.java:485) \n> java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116) \n> Thread name=Signal Dispatcher id=6 priority=9 state=RUNNABLE isdaemon=true \n> Thread name=Timer-0 id=10 priority=5 state=WAITING isdaemon=true \n> java.lang.Object.wait(Native Method) \n> java.lang.Object.wait(Object.java:485) \n> java.util.TimerThread.mainLoop(Timer.java:483) \n> java.util.TimerThread.run(Timer.java:462) \n> --------------- \n> Exception in thread \"main\" java.sql.SQLException: Failed to start database '/Users/tim/Downloads/LinkedBlockingQueueCrashTest/objectdb/datadb' with class loader sun.misc.Launcher$AppClassLoader@40affc70, see the next exception for details. \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:98) \n> at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Util.java:142) \n> at org.apache.derby.impl.jdbc.Util.seeNextException(Util.java:278) \n> at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(EmbedConnection.java:2736) \n> at org.apache.derby.impl.jdbc.EmbedConnection.<init>(EmbedConnection.java:385) \n> at org.apache.derby.impl.jdbc.EmbedConnection30.<init>(EmbedConnection30.java:73) \n> at org.apache.derby.impl.jdbc.EmbedConnection40.<init>(EmbedConnection40.java:51) \n> at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Driver40.java:70) \n> at org.apache.derby.jdbc.InternalDriver.connect(InternalDriver.java:248) \n> at org.apache.derby.jdbc.AutoloadedDriver.connect(AutoloadedDriver.java:144) \n> at java.sql.DriverManager.getConnection(DriverManager.java:582) \n> at java.sql.DriverManager.getConnection(DriverManager.java:154) \n> at OpenDerby.main(OpenDerby.java:17) \n> Caused by: java.sql.SQLException: Failed to start database '/Users/tim/Downloads/LinkedBlockingQueueCrashTest/objectdb/datadb' with class loader sun.misc.Launcher$AppClassLoader@40affc70, see the next exception for details. \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(SQLExceptionFactory.java:45) \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(SQLExceptionFactory40.java:122) \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:71) \n> ... 12 more \n> Caused by: java.sql.SQLException: Java exception: 'ASSERT FAILED incorrect deleted row count. Should be: 903, instead got: 902, maxSlot = 1255, recordCount = 1255: org.apache.derby.shared.common.sanity.AssertFailure'. \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(SQLExceptionFactory.java:45) \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(SQLExceptionFactory40.java:122) \n> at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:71) \n> at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Util.java:142) \n> at org.apache.derby.impl.jdbc.Util.javaException(Util.java:299) \n> at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(EmbedConnection.java:2732) \n> ... 9 more \n> Caused by: org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED incorrect deleted row count. Should be: 903, instead got: 902, maxSlot = 1255, recordCount = 1255 \n> at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:162) \n> at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:147) \n> at org.apache.derby.impl.store.raw.data.BasePage.internalNonDeletedRecordCount(BasePage.java:1432) \n> at org.apache.derby.impl.store.raw.data.CachedPage.releaseExclusive(CachedPage.java:488) \n> at org.apache.derby.impl.store.raw.data.StoredPage.releaseExclusive(StoredPage.java:1066) \n> at org.apache.derby.impl.store.raw.data.BasePage.unlatch(BasePage.java:1370) \n> at org.apache.derby.impl.store.raw.data.PageBasicOperation.releaseResource(PageBasicOperation.java:195) \n> at org.apache.derby.impl.store.raw.data.PhysicalUndoOperation.releaseResource(PhysicalUndoOperation.java:177) \n> at org.apache.derby.impl.store.raw.log.FileLogger.undo(FileLogger.java:1055) \n> at org.apache.derby.impl.store.raw.xact.Xact.abort(Xact.java:952) \n> at org.apache.derby.impl.store.raw.xact.XactFactory.rollbackAllTransactions(XactFactory.java:547) \n> at org.apache.derby.impl.store.raw.log.LogToFile.recover(LogToFile.java:1229) \n> at org.apache.derby.impl.store.raw.RawStore.boot(RawStore.java:339) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:531) \n> at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:427) \n> at org.apache.derby.impl.store.access.RAMAccessManager.boot(RAMAccessManager.java:1019) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:531) \n> at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:427) \n> at org.apache.derby.impl.db.BasicDatabase.bootStore(BasicDatabase.java:749) \n> at org.apache.derby.impl.db.BasicDatabase.boot(BasicDatabase.java:177) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1982) \n> at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:334) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(BaseMonitor.java:1816) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(BaseMonitor.java:1682) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(BaseMonitor.java:1560) \n> at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(BaseMonitor.java:979) \n> at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Monitor.java:550) \n> at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(EmbedConnection.java:2697) \n> ... 9 more \n> It also fails with a slightly different error using the non-debug jar:\n> Exception in thread \"main\" java.sql.SQLException: Failed to start database '/Users/tim/Downloads/LinkedBlockingQueueCrashTest/objectdb/datadb' with class loader sun.misc.Launcher$AppClassLoader@40affc70, see the next exception for details.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\n> \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\n> \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n> \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n> \tat java.sql.DriverManager.getConnection(DriverManager.java:582)\n> \tat java.sql.DriverManager.getConnection(DriverManager.java:154)\n> \tat OpenDerby.main(OpenDerby.java:16)\n> Caused by: java.sql.SQLException: Failed to start database '/Users/tim/Downloads/LinkedBlockingQueueCrashTest/objectdb/datadb' with class loader sun.misc.Launcher$AppClassLoader@40affc70, see the next exception for details.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n> \t... 13 more\n> Caused by: java.sql.SQLException: An exception was thrown during transaction abort.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n> \t... 10 more\n> Caused by: ERROR XSTB0: An exception was thrown during transaction abort.\n> \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.xact.Xact.abort(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.xact.XactFactory.rollbackAllTransactions(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.log.LogToFile.recover(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n> \tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n> \tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n> \tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n> \tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n> \tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n> \tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n> \tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n> \t... 10 more\n> Caused by: ERROR XSLA8: Cannot rollback transaction 10946, trying to compensate null operation with null\n> \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.log.FileLogger.undo(Unknown Source)\n> \t... 32 more\n> Caused by: ERROR XSLA1: Log Record has been sent to the stream, but it cannot be applied to the store (Object null).  This may cause recovery problems also.\n> \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.log.FileLogger.logAndUndo(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.xact.Xact.logAndUndo(Unknown Source)\n> \t... 33 more\n> Caused by: ERROR XSDB0: Unexpected exception on in-memory page Page(1,Container(0, 1409))\n> \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.data.StoredPage.storeRecordForInsert(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.data.StoredPage.storeRecord(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.data.PurgeOperation.undoMe(Unknown Source)\n> \tat org.apache.derby.impl.store.raw.data.PhysicalUndoOperation.doMe(Unknown Source)\n> \t... 35 more\n> I have the zipped up db that causes the problem. Will attach it to this bug report.\n>  \n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-05-27T16:35:47Z"
  },
  "patches": [],
  "external_id": "DERBY-5248"
},{
  "_id": {
    "$oid": "5f27ce91014d3531c6cc7016"
  },
  "message_id": "<15107309.1190310811174.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27ce8d014d3531c6cc6ed5"
  },
  "from_id": {
    "$oid": "58c11e3f02ca40f8bfb1f764"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (JCR-905) Clustering: race condition may cause\n duplicate entries in search index",
  "body": "\n     [ https://issues.apache.org/jira/browse/JCR-905?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJukka Zitting updated JCR-905:\n------------------------------\n\n    Affects Version/s: 1.2.1\n                       1.2.2\n                       1.2.3\n                       1.3.1\n        Fix Version/s:     (was: 1.4)\n                       1.3.2\n\nMerged to the 1.3 branch in revision 577835.\n\n> Clustering: race condition may cause duplicate entries in search index\n> ----------------------------------------------------------------------\n>\n>                 Key: JCR-905\n>                 URL: https://issues.apache.org/jira/browse/JCR-905\n>             Project: Jackrabbit\n>          Issue Type: Bug\n>          Components: clustering\n>    Affects Versions: 1.2.1, 1.2.2, 1.2.3, 1.3, 1.3.1\n>            Reporter: Martijn Hendriks\n>             Fix For: 1.3.2\n>\n>         Attachments: JCR-905.patch, log1.txt, log2.txt, SearchManager.patch\n>\n>\n> There seems to be a race condition that may cause duplicate search index entries. It is reproducible as follows (Jackrabbit 1.3):\n> 1) Start clusternode 1 that just adds a single node of node type clustering:test.\n> 2) Shutdown clusternode 1.\n> 3) Start clusternode 2 with an empty search index.\n> 4) Execute the query  //element(*, clustering:test).\n> 4) Print the result of the query (UUIDs of nodes in the result set).\n> When I just run clusternode 2, then there is one node in the resultset, as expected. However, when I debug clusternode 2 and have a breakpoint (i.e., a pause of a few seconds at line 306 of RepositoryImpl.java - just before the clusternode is started), then the resultset contains two results, both with the same UUID.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-09-20T10:53:31Z"
  },
  "patches": [],
  "external_id": "JCR-905"
},{
  "_id": {
    "$oid": "5bdc011a16772b6055c7ea5a"
  },
  "message_id": "<JIRA.13009795.1475652895000.741413.1475665400511@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdc00ac16772b6055c7dfeb"
    },
    {
      "$oid": "5bdc011a16772b6055c7ea57"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdc00ac16772b6055c7dfeb"
  },
  "from_id": {
    "$oid": "59677a36aff2204b3cbd12ff"
  },
  "to_ids": [
    {
      "$oid": "5bdc002135e3ea2b7bb8a8be"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KNOX-754) curl requests fail when dealing with\n special characters",
  "body": "\n    [ https://issues.apache.org/jira/browse/KNOX-754?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15548400#comment-15548400 ] \n\nLarry McCay commented on KNOX-754:\n----------------------------------\n\nYikes - [~BigDataOrange]!\nI just said that I knew of no incompatibilities between 0.7.0 and 0.9.1.\n\nJust to clarify, you mean special characters within the URL not within files themselves - correct?\n\nHave you by chance tried it direct to WebHDFS instead of through Knox to see if it is reproducible without going through the gateway?\nWe can turn on wire level DEBUG in gateway-log4j.properties to see the traffic between gateway and webhdfs as well.\n\n> curl requests fail when dealing with special characters\n> -------------------------------------------------------\n>\n>                 Key: KNOX-754\n>                 URL: https://issues.apache.org/jira/browse/KNOX-754\n>             Project: Apache Knox\n>          Issue Type: Bug\n>          Components: ClientDSL, Server\n>    Affects Versions: 0.9.1\n>         Environment: Apache Knox 0.9.1, Apache Hadoop 2.7.2\n>            Reporter: Alexandre Linte\n>            Priority: Critical\n>\n> Since Knox 0.9.1, Knox can't work with files which contain special characters as : é, ù, ü, è, etc... This is reproducible at 100%. It was working well with Knox 0.7.0 so it's a regression. \n> This happens when doing a GET or a PUT of a file of this type, and more particularly at the \"location\" process of the request. You can find an example below:\n> {noformat}\n> [shfs3453@uabigspark01 Pig]$ curl -Iikv -u shfs3453 -X GET 'https://knox-gateway.fr/gateway/bigdata/webhdfs/v1/user/shfs3453/WORK/datasets/test_électronique_embarqué.pdf?OP=OPEN'\n> Enter host password for user 'shfs3453':\n> * About to connect() to knox-gateway port 443 (#0)\n> *   Trying 10.117.41.12... connected\n> * Connected to knox-gateway (10.117.41.12) port 443 (#0)\n> * Initializing NSS with certpath: sql:/etc/pki/nssdb\n> * warning: ignoring value of ssl.verifyhost\n> * skipping SSL peer certificate verification\n> * SSL connection using TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\n> * Server certificate:\n> *       subject: E=********@*****,CN=knox-gateway,OU=*****,O=****,L=*****,ST=*****,C=***\n> *       start date: Nov 07 11:33:05 2014 GMT\n> *       expire date: Nov 06 11:33:05 2019 GMT\n> *       common name: knox-gateway\n> *       issuer: CN=***************,OU=*******,OU=********,O=******,C=***\n> * Server auth using Basic with user 'shfs3453'\n> > GET /gateway/bigdata/webhdfs/v1/user/shfs3453/WORK/datasets/test_électronique_embarqué.pdf?OP=OPEN HTTP/1.1\n> > Authorization: Basic c2hmczM0NTM6UGIxOTkxMTAh\n> > User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.19.1 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\n> > Host: knox-gateway\n> > Accept: */*\n> >\n> < HTTP/1.1 307 Temporary Redirect\n> HTTP/1.1 307 Temporary Redirect\n> < Date: Wed, 05 Oct 2016 07:19:55 GMT\n> Date: Wed, 05 Oct 2016 07:19:55 GMT\n> < Set-Cookie: JSESSIONID=4zv7v1911q5vvcg6r1tqxe77;Path=/gateway/bigdata;Secure;HttpOnly\n> Set-Cookie: JSESSIONID=4zv7v1911q5vvcg6r1tqxe77;Path=/gateway/bigdata;Secure;HttpOnly\n> < Expires: Thu, 01 Jan 1970 00:00:00 GMT\n> Expires: Thu, 01 Jan 1970 00:00:00 GMT\n> < Set-Cookie: rememberMe=deleteMe; Path=/gateway/bigdata; Max-Age=0; Expires=Tue, 04-Oct-2016 07:19:56 GMT\n> Set-Cookie: rememberMe=deleteMe; Path=/gateway/bigdata; Max-Age=0; Expires=Tue, 04-Oct-2016 07:19:56 GMT\n> < Cache-Control: no-cache\n> Cache-Control: no-cache\n> < Expires: Wed, 05 Oct 2016 07:19:56 GMT\n> Expires: Wed, 05 Oct 2016 07:19:56 GMT\n> < Date: Wed, 05 Oct 2016 07:19:56 GMT\n> Date: Wed, 05 Oct 2016 07:19:56 GMT\n> < Pragma: no-cache\n> Pragma: no-cache\n> < Expires: Wed, 05 Oct 2016 07:19:56 GMT\n> Expires: Wed, 05 Oct 2016 07:19:56 GMT\n> < Date: Wed, 05 Oct 2016 07:19:56 GMT\n> Date: Wed, 05 Oct 2016 07:19:56 GMT\n> < Pragma: no-cache\n> Pragma: no-cache\n> < Location: https://knox-gateway/gateway/bigdata/webhdfs/data/v1/webhdfs/v1/user/shfs3453/WORK/datasets/test_▒lectronique_embarqu▒.pdf?_=AAAACAAAABAAAAEAl_jkRL_c3Tzm7hoXMR1KPge4OClEqM4hfs3eslFzfdY5CBbrfaMzOa--NXb08Xjw2O11CkOtyUX5kXwh2IgZmxjw_TNHqQUvVAfFkeXiMDiBXxpbhulsVx3o_NLn9pCLsp09xJ9r1utCHrueYOAvuxY_ksQWuHld2WWGEPyWRubcgb4e6xO2F4jo96NSZhuAP8iarY5LiCtTydLPBXcEbbD146jLD7S83Mhij4VS5sO1asESNH5y8_5Z2PvLcZE11WiTS9alu-9AUqXNixw1t9Y5Em6xDle7s8-oiF3nPVM80RIdbJel4LoeCZuB2zgddLaJAYx5tSb03-QGNzupOPQ5UQ0_7ybPwmAsgiFfFNuvMbj9sKgxLg\n> Location: https://knox-gateway/gateway/bigdata/webhdfs/data/v1/webhdfs/v1/user/shfs3453/WORK/datasets/test_▒lectronique_embarqu▒.pdf?_=AAAACAAAABAAAAEAl_jkRL_c3Tzm7hoXMR1KPge4OClEqM4hfs3eslFzfdY5CBbrfaMzOa--NXb08Xjw2O11CkOtyUX5kXwh2IgZmxjw_TNHqQUvVAfFkeXiMDiBXxpbhulsVx3o_NLn9pCLsp09xJ9r1utCHrueYOAvuxY_ksQWuHld2WWGEPyWRubcgb4e6xO2F4jo96NSZhuAP8iarY5LiCtTydLPBXcEbbD146jLD7S83Mhij4VS5sO1asESNH5y8_5Z2PvLcZE11WiTS9alu-9AUqXNixw1t9Y5Em6xDle7s8-oiF3nPVM80RIdbJel4LoeCZuB2zgddLaJAYx5tSb03-QGNzupOPQ5UQ0_7ybPwmAsgiFfFNuvMbj9sKgxLg\n> < Content-Type: application/octet-stream\n> Content-Type: application/octet-stream\n> < Server: Jetty(6.1.26)\n> Server: Jetty(6.1.26)\n> < Content-Length: 0\n> Content-Length: 0\n> <\n> * Connection #0 to host knox-gateway left intact\n> * Issue another request to this URL: 'https://knox-gateway/gateway/bigdata/webhdfs/data/v1/webhdfs/v1/user/shfs3453/WORK/datasets/test_▒lectronique_embarqu▒.pdf?_=AAAACAAAABAAAAEAl_jkRL_c3Tzm7hoXMR1KPge4OClEqM4hfs3eslFzfdY5CBbrfaMzOa--NXb08Xjw2O11CkOtyUX5kXwh2IgZmxjw_TNHqQUvVAfFkeXiMDiBXxpbhulsVx3o_NLn9pCLsp09xJ9r1utCHrueYOAvuxY_ksQWuHld54aYH365vWu3V6u-_BaX3E1Ax5puYXZkEypdB2SOKVFW5bqIi5JFkgUr_XV8bXdcFTcdbohr82pKVBqmK-OvSZnCAVSdy4Yjyf51fSLo_n07ElHK84zqsXEMLU1zF5DHbSKC_jwpGahsm5VlYK7H5Ppwt0SNFHIx50O9yBpPLHYNe-ALlqTOlq6UT3ifFufhmKsY6chP6IxLw7ZyCHrBSA'\n> * Re-using existing connection! (#0) with host knox-gateway\n> * Connected to knox-gateway (10.117.41.12) port 443 (#0)\n> * Server auth using Basic with user 'shfs3453'\n> > GET /gateway/bigdata/webhdfs/data/v1/webhdfs/v1/user/shfs3453/WORK/datasets/test_▒lectronique_embarqu▒.pdf?_=AAAACAAAABAAAAEAl_jkRL_c3Tzm7hoXMR1KPge4OClEqM4hfs3eslFzfdY5CBbrfaMzOa--NXb08Xjw2O11CkOtyUX5kXwh2IgZmxjw_TNHqQUvVAfFkeXiMDiBXxpbhulsVx3o_NLn9pCLsp09xJ9r1utCHrueYOAvuxY_ksQWuHld54aYH365vWu3V6u-_BaX3E1Ax5puYXZkEypdB2SOKVFW5bqIi5JFkgUr_XV8bXdcFTcdbohr82pKVBqmK-OvSZnCAVSdy4Yjyf51fSLo_n07ElHK84zqsXEMLU1zF5DHbSKC_jwpGahsm5VlYK7H5Ppwt0SNFHIx50O9yBpPLHYNe-ALlqTOlq6UT3ifFufhmKsY6chP6IxLw7ZyCHrBSA HTTP/1.1\n> > Authorization: Basic c2hmczM0NTM6UGIxOTkxMTAh\n> > User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.19.1 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\n> > Host: knox-gateway\n> > Accept: */*\n> >\n> < HTTP/1.1 404 Not Found\n> HTTP/1.1 404 Not Found\n> < Date: Wed, 05 Oct 2016 07:32:07 GMT\n> Date: Wed, 05 Oct 2016 07:32:07 GMT\n> < Set-Cookie: JSESSIONID=1kp671ikau2cieuzdlw84yeh;Path=/gateway/bigdata;Secure;HttpOnly\n> Set-Cookie: JSESSIONID=1kp671ikau2cieuzdlw84yeh;Path=/gateway/bigdata;Secure;HttpOnly\n> < Expires: Thu, 01 Jan 1970 00:00:00 GMT\n> Expires: Thu, 01 Jan 1970 00:00:00 GMT\n> < Set-Cookie: rememberMe=deleteMe; Path=/gateway/bigdata; Max-Age=0; Expires=Tue, 04-Oct-2016 07:32:07 GMT\n> Set-Cookie: rememberMe=deleteMe; Path=/gateway/bigdata; Max-Age=0; Expires=Tue, 04-Oct-2016 07:32:07 GMT\n> < Content-Type: application/json; charset=utf-8\n> Content-Type: application/json; charset=utf-8\n> < Connection: close\n> Connection: close\n> < Server: Jetty(9.2.15.v20160210)\n> Server: Jetty(9.2.15.v20160210)\n> <\n> * Closing connection #0\n> {noformat}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-10-05T11:03:20Z"
  },
  "patches": [],
  "external_id": "KNOX-754"
},{
  "_id": {
    "$oid": "5bea9e9c9e73d744d41302e6"
  },
  "message_id": "<27651560.1158679282863.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9e499e73d744d412ee49"
  },
  "from_id": {
    "$oid": "5bea9e7135e3ea2b7b5df3a6"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-1606) derbyrunjartest hangs with wctme5.7 &\n always uses j9_13; hangs with _foundation",
  "body": "     [ http://issues.apache.org/jira/browse/DERBY-1606?page=all ]\n\nRick Hillegas updated DERBY-1606:\n---------------------------------\n\n    Fix Version/s: 10.2.2.0\n                       (was: 10.2.1.0)\n\nMoving to 10.2.2.0.\n\n> derbyrunjartest hangs with wctme5.7 & always uses j9_13; hangs with _foundation\n> -------------------------------------------------------------------------------\n>\n>                 Key: DERBY-1606\n>                 URL: http://issues.apache.org/jira/browse/DERBY-1606\n>             Project: Derby\n>          Issue Type: Test\n>          Components: Test\n>    Affects Versions: 10.0.2.0\n>            Reporter: Myrna van Lunteren\n>            Priority: Minor\n>             Fix For: 10.2.2.0\n>\n>         Attachments: DERBY-1606_20060730_skip.diff, DERBY-1606_20060731_skipcomment.diff\n>\n>\n> derbyrunjartest hangs with wctme5.7.\n> I am not sure what the hook up is but if I do this:\n> j9 -jcl:max -Xbootclasspath/p:c:/wctme5.7/ive/lib/jclMax/classes.zip;c:/wctme5.7/ive/lib/charconv.zip;c:/wctme5.7/ive/lib/database_enabler.jar -jar c:/derbyt/svn2/trunk/jars/sane/derbyrun.jar ij --help\n> I get successfully:\n> Usage: java org.apache.derby.tools.ij [-p propertyfile] [-ca connectionAttribute\n> PropertyFile] [inputfile]\n> Also, when looking at the code of this test, it forces the jvm class to be used to be j9_13 whenever the jvm as set in RunTest starts with J9. However, wctme5.7 is j9_22 and this also prevents running with foundation (assuming that's supported).\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2006-09-19T08:21:22Z"
  },
  "patches": [],
  "external_id": "DERBY-1606"
},{
  "_id": {
    "$oid": "5f27cf8d532b7277349bf6e7"
  },
  "message_id": "<JIRA.12638006.1363794803565.20111.1363795878514@arcas>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [
    {
      "$oid": "5f27cf8d532b7277349bf6e2"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cf8d532b7277349bf6e2"
  },
  "from_id": {
    "$oid": "5f27ceb5af02e2d6de8602e0"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (AMQ-4393) Web console do not show connections\n info",
  "body": "\n     [ https://issues.apache.org/jira/browse/AMQ-4393?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTimothy Bish updated AMQ-4393:\n------------------------------\n\n    Description: \nThe url:http://localhost:8161/admin/connections.jsp\nActiveMQ5.5.0, this page will show then connections info. \nActiveMQ5.8.0, this page will show nothing. \n\nBecause the objectname of MBean have changed. \n5.5.0: \norg.apache.activemq:BrokerName=localhost,Type=Connector,ConnectorName=openwire \n5.5.8: \norg.apache.activemq:type=Broker,brokerName=localhost,connector=clientConnectors,connectorName=openwire \n\nSo, to fix this, org.apache.activemq.web.BrokerFacadeSupport: \n\n{code}\n\n    public Collection<String> getConnections(String connectorName) throws Exception { \n        String brokerName = getBrokerName(); \n//        ObjectName query = new ObjectName(\"org.apache.activemq:type=Broker,brokerName=\" + brokerName \n//                + \",connector=clientConnectors,connectorName=\" + connectorName + \",connectionName=*\"); \n\n      ObjectName query = new ObjectName(\"org.apache.activemq:type=Broker,brokerName=\" + brokerName \n      + \",connector=clientConnectors,connectorName=\" + connectorName + \",connectionViewType=clientId\" + \",connectionName=*\");   \n    ... \n} \n\n{code}\n\n  was:\nThe url:http://localhost:8161/admin/connections.jsp\nActiveMQ5.5.0, this page will show then connections info. \nActiveMQ5.8.0, this page will show nothing. \n\nBecause the objectname of MBean have changed. \n5.5.0: \norg.apache.activemq:BrokerName=localhost,Type=Connector,ConnectorName=openwire \n5.5.8: \norg.apache.activemq:type=Broker,brokerName=localhost,connector=clientConnectors,connectorName=openwire \n\nSo, to fix this, org.apache.activemq.web.BrokerFacadeSupport: \n    public Collection<String> getConnections(String connectorName) throws Exception { \n        String brokerName = getBrokerName(); \n//        ObjectName query = new ObjectName(\"org.apache.activemq:type=Broker,brokerName=\" + brokerName \n//                + \",connector=clientConnectors,connectorName=\" + connectorName + \",connectionName=*\"); \n\n      ObjectName query = new ObjectName(\"org.apache.activemq:type=Broker,brokerName=\" + brokerName \n      + \",connector=clientConnectors,connectorName=\" + connectorName + \",connectionViewType=clientId\" + \",connectionName=*\");   \n    ... \n} \n\n    \n> Web console do not show connections info\n> ----------------------------------------\n>\n>                 Key: AMQ-4393\n>                 URL: https://issues.apache.org/jira/browse/AMQ-4393\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: JMX\n>    Affects Versions: 5.8.0\n>            Reporter: hengyunabc\n>            Priority: Trivial\n>\n> The url:http://localhost:8161/admin/connections.jsp\n> ActiveMQ5.5.0, this page will show then connections info. \n> ActiveMQ5.8.0, this page will show nothing. \n> Because the objectname of MBean have changed. \n> 5.5.0: \n> org.apache.activemq:BrokerName=localhost,Type=Connector,ConnectorName=openwire \n> 5.5.8: \n> org.apache.activemq:type=Broker,brokerName=localhost,connector=clientConnectors,connectorName=openwire \n> So, to fix this, org.apache.activemq.web.BrokerFacadeSupport: \n> {code}\n>     public Collection<String> getConnections(String connectorName) throws Exception { \n>         String brokerName = getBrokerName(); \n> //        ObjectName query = new ObjectName(\"org.apache.activemq:type=Broker,brokerName=\" + brokerName \n> //                + \",connector=clientConnectors,connectorName=\" + connectorName + \",connectionName=*\"); \n>       ObjectName query = new ObjectName(\"org.apache.activemq:type=Broker,brokerName=\" + brokerName \n>       + \",connector=clientConnectors,connectorName=\" + connectorName + \",connectionViewType=clientId\" + \",connectionName=*\");   \n>     ... \n> } \n> {code}\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-03-20T16:11:18Z"
  },
  "patches": [],
  "external_id": "AMQ-4393"
},{
  "_id": {
    "$oid": "5bea9a5e9e73d744d4122207"
  },
  "message_id": "<1895892174.15998.1300005599475.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9a5a9e73d744d4122168"
  },
  "from_id": {
    "$oid": "5bea97a135e3ea2b7b4d611b"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-5113) Intermittent failure in\n BlobSetMethodsTest on Java 7: Unable to set stream: 'Reached EOF\n prematurely; expected 1,024, got 0.'",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-5113?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKnut Anders Hatlen updated DERBY-5113:\n--------------------------------------\n\n    Attachment: derby-5113-1a-test.diff\n\nThe attached patch adds a tearDown() method to AccessTest. The database properties modified by the test are now cleared after each test case. DatabasePropertyTestSetup couldn't be used for this since the properties are set to different values in each test case, and not globally for the whole test which is how that decorator is normally used.\n\nCommitted revision 1081059.\n\n> Intermittent failure in BlobSetMethodsTest on Java 7: Unable to set stream: 'Reached EOF prematurely; expected 1,024, got 0.'\n> -----------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-5113\n>                 URL: https://issues.apache.org/jira/browse/DERBY-5113\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: JDBC, Test\n>    Affects Versions: 10.8.0.0\n>         Environment: Solaris 10 and Windows\n> Java(TM) SE Runtime Environment (build 1.7.0-ea-b131)\n>            Reporter: Knut Anders Hatlen\n>            Assignee: Knut Anders Hatlen\n>         Attachments: derby-5113-1a-test.diff, reporting.diff\n>\n>\n> Not sure if this is a test issue, a product issue or a JVM issue. It started happening when JDK 7 was upgraded from b116 to b131 in the nightly tests. I haven't been able to reproduce the failure in my environment, but it happens frequently in the nightly testing. For example here:\n> http://dbtg.foundry.sun.com/derby/test/Daily/jvm1.7/testing/testlog/sol/1078053-suitesAll_diff.txt\n> 19) testSetBytesLargeBlob(org.apache.derbyTesting.functionTests.tests.jdbc4.BlobSetMethodsTest)java.sql.SQLException: Unable to set stream: 'Reached EOF prematurely; expected 1,024, got 0.'.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.setStreamFailure(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedBlob.truncate(Unknown Source)\n> \tat org.apache.derbyTesting.functionTests.tests.jdbc4.BlobSetMethodsTest.testSetBytesLargeBlob(BlobSetMethodsTest.java:102)\n> \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n> \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n> \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n> \tat org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:112)\n> \tat junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)\n> \tat junit.extensions.TestSetup$1.protect(TestSetup.java:21)\n> \tat junit.extensions.TestSetup.run(TestSetup.java:25)\n> \tat org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)\n> Caused by: java.sql.SQLException: Unable to set stream: 'Reached EOF prematurely; expected 1,024, got 0.'.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n> The problem does not appear to be the reordering of the test cases that we've seen in many other tests when running on Java 7 b131. According to the traces printed to the test log (for example in the above mentioned URL), this failure is also seen in the cases where the test cases run in the same order as on most other platforms.\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-03-13T08:39:59Z"
  },
  "patches": [],
  "external_id": "DERBY-5113"
},{
  "_id": {
    "$oid": "60fac30bd907ab79037e7452"
  },
  "message_id": "<JIRA.13117711.1510366893000.211216.1510366921716@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac2d2d907ab79037e606d"
    },
    {
      "$oid": "60fac2d2d907ab79037e606e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2d2d907ab79037e606d"
  },
  "from_id": {
    "$oid": "60fac30bf73e2aa390c9da6d"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (DIR-326) Documentation bugs for\n LdapConnectionTemplate user guide",
  "body": "Jarmod created DIR-326:\n--------------------------\n\n             Summary: Documentation bugs for LdapConnectionTemplate user guide\n                 Key: DIR-326\n                 URL: https://issues.apache.org/jira/browse/DIR-326\n             Project: Directory\n          Issue Type: Bug\n          Components: sitedocs\n            Reporter: Jarmod\n            Assignee: Emmanuel Lecharny\n            Priority: Minor\n\n\nThe [#2.10 - Why use the LdapConnectionTemplate|http://directory.apache.org/api/user-guide/2.10-ldap-connection-template.html] docs have a couple of bugs:\n\n1. The code \n{code:java}\nprivate static final EntryMapper muppetEntryMapper\n{code}\n should read as follows: \n{code:java}\nprivate static final EntryMapper<Muppet> muppetEntryMapper\n{code}\n\n2. The code \n{code:java}\npublic String map( Entry entry ) throws LdapException\n{code}\n should read as follows: \n{code:java}\npublic Muppet map( Entry entry ) throws LdapException\n{code}\n\nThe code, as it stands, will not compile (one warning, one error).\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-11-11T02:22:01Z"
  },
  "patches": [],
  "external_id": "DIR-326"
},{
  "_id": {
    "$oid": "5bacb45dfaaadd76f8aa6f70"
  },
  "message_id": "<1612543485.1248033195008.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb43efaaadd76f8aa67bc"
  },
  "from_id": {
    "$oid": "5bacb29a57674ee167d831d8"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (PIG-858) Order By followed by \"replicated\" join\n fails while compiling MR-plan from physical plan",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-858?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12733057#action_12733057 ] \n\nAshutosh Chauhan commented on PIG-858:\n--------------------------------------\n\nWhile POFRJoin is getting compiled in MRCompiler, it needs to identify for each of its \npredecessor in physical plan of which compiled MROperator they are part of. Currently, it is\nassumed to be one of the compiledInputs(an array of MRoper which are immediate predecessor of current MROper in MROper DAG). \nMostly this is true, but in cases where one physical operator results in two or more MR operator, this may not be true, as is the\ncase here. When there is an order-by before FRJoin; one of the inputs of POFRJoin will be\nPOSort, but POSort operator will be in the first MROper of the two generated MROperator\nand thus will not be found in compiledInputs (which contains second MROper). Thus,\ncurrent way of identifying corresponding MRoper of a physical operator is unreliable.\nThis bug also affects the implementation of merge-sort join \nhttps://issues.apache.org/jira/browse/PIG-845 . Since POMergeJoin needs to know which MROper\ncorresponds to its left input and which one corresponds to its right. It can do so by looking\ninto compiledInputs as long as there is no order-by (or similiar PO which results in\nmultiple MROper) as its predecessors. Doing order-by before using merge\njoin is however a natural use-case there.\n\nProposal is to introduce a new private member variable in MRCompiler phyToMROperMap \n(similiar to logToPhyMap) using which leaf MROper for a given\nphysical operator can be identified. Thoughts?\n  \n\n> Order By followed by \"replicated\" join fails while compiling MR-plan from physical plan\n> ---------------------------------------------------------------------------------------\n>\n>                 Key: PIG-858\n>                 URL: https://issues.apache.org/jira/browse/PIG-858\n>             Project: Pig\n>          Issue Type: Bug\n>          Components: impl\n>    Affects Versions: 0.3.0\n>            Reporter: Ashutosh Chauhan\n>             Fix For: 0.4.0\n>\n>\n> Consider the query:\n> {code}\n> A = load 'a';\n> B = order A by $0;\n> C = join A by $0, B by $0;\n> explain C;\n> {code}\n> works. But if replicated join is used instead\n> {code}\n> A = load 'a';\n> B = order A by $0;\n> C = join A by $0, B by $0 using \"replicated\";\n> explain C;\n> {code}\n> this fails with ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2034: Error compiling operator POFRJoin\n> relevant stacktrace:\n> {code}\n> Caused by: java.lang.RuntimeException: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompilerException: ERROR 2034: Error compiling operator POFRJoin\n>         at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.explain(HExecutionEngine.java:306)\n>         at org.apache.pig.PigServer.explain(PigServer.java:574)\n>         ... 8 more\n> Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompilerException: ERROR 2034: Error compiling operator POFRJoin\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:942)\n>         at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.visit(POFRJoin.java:173)\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:342)\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:327)\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:233)\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:301)\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.explain(MapReduceLauncher.java:278)\n>         at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.explain(HExecutionEngine.java:303)\n>         ... 9 more\n> Caused by: java.lang.ArrayIndexOutOfBoundsException: -1\n>         at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:901)\n>         ... 16 more\n> {code}\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-07-19T12:53:15Z"
  },
  "patches": [],
  "external_id": "PIG-858"
},{
  "_id": {
    "$oid": "5bbf0dddb79d666cbb22d191"
  },
  "message_id": "<JIRA.12819182.1428468176000.20736.1429896400723@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0c47b79d666cbb2293ce"
    },
    {
      "$oid": "5bbf0c47b79d666cbb2293cf"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0c47b79d666cbb2293ce"
  },
  "from_id": {
    "$oid": "59677a78aff2204b3cbd137d"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (KAFKA-2105) NullPointerException in client on\n MetadataRequest",
  "body": "\n     [ https://issues.apache.org/jira/browse/KAFKA-2105?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDaneel Yaitskov updated KAFKA-2105:\n-----------------------------------\n    Attachment: guard-from-null.patch\n\nSorry, First time I did patch with --color option.\nAs for me patch files are legacy technique, a github reference for a pull request looks much much better, but I didn't find any ticket resolution in a such way. \n\n> NullPointerException in client on MetadataRequest\n> -------------------------------------------------\n>\n>                 Key: KAFKA-2105\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-2105\n>             Project: Kafka\n>          Issue Type: Bug\n>          Components: clients\n>    Affects Versions: 0.8.2.1\n>            Reporter: Roger Hoover\n>            Priority: Minor\n>         Attachments: guard-from-null.patch\n>\n>\n> With the new producer, if you accidentally pass null to KafkaProducer.partitionsFor(null), it will cause the IO thread to throw NPE.\n> Uncaught error in kafka producer I/O thread: \n> java.lang.NullPointerException\n> \tat org.apache.kafka.common.utils.Utils.utf8Length(Utils.java:174)\n> \tat org.apache.kafka.common.protocol.types.Type$5.sizeOf(Type.java:176)\n> \tat org.apache.kafka.common.protocol.types.ArrayOf.sizeOf(ArrayOf.java:55)\n> \tat org.apache.kafka.common.protocol.types.Schema.sizeOf(Schema.java:81)\n> \tat org.apache.kafka.common.protocol.types.Struct.sizeOf(Struct.java:218)\n> \tat org.apache.kafka.common.requests.RequestSend.serialize(RequestSend.java:35)\n> \tat org.apache.kafka.common.requests.RequestSend.<init>(RequestSend.java:29)\n> \tat org.apache.kafka.clients.NetworkClient.metadataRequest(NetworkClient.java:369)\n> \tat org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:391)\n> \tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:188)\n> \tat org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191)\n> \tat org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122)\n> \tat java.lang.Thread.run(Thread.java:745)\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-04-24T17:26:40Z"
  },
  "patches": [],
  "external_id": "KAFKA-2105"
},{
  "_id": {
    "$oid": "5bbdad85c764eb6c7a266ac4"
  },
  "message_id": "<JIRA.12693294.1391582426698.3490.1391767519067@arcas>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdad82c764eb6c7a266a6c"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdad82c764eb6c7a266a6c"
  },
  "from_id": {
    "$oid": "5bacb14557674ee167d5b2e1"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FALCON-297) Validations on update with effective\n time",
  "body": "\n     [ https://issues.apache.org/jira/browse/FALCON-297?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nShwetha G S updated FALCON-297:\n-------------------------------\n\n    Attachment: FALCON-297.patch\n\n> Validations on update with effective time\n> -----------------------------------------\n>\n>                 Key: FALCON-297\n>                 URL: https://issues.apache.org/jira/browse/FALCON-297\n>             Project: Falcon\n>          Issue Type: Bug\n>            Reporter: Shwetha G S\n>            Assignee: Shwetha G S\n>             Fix For: 0.5\n>\n>         Attachments: FALCON-297.patch\n>\n>\n> Effective time should be validated against start and end of entity. Also, when new bundle is not created because effectiveTime = endTime throws exception:\n> 2014-02-03 07:08:26,011 ERROR - [1451544186@qtp-1790118163-0:samarth.gupta:POST//sync/update/process/agregator-coord16-47c507d2-7ac0-4f11-88f7-94653de28b56-36dac363-8dca-48ab-aa55\n> -34fcafafc8d7 c6bbe757-e13f-4f95-a14a-d6e1cd0108aa] ~ Updation failed (AbstractEntityManager:269)\n> java.lang.IllegalArgumentException: jobId cannot be null\n> at org.apache.oozie.client.OozieClient.notEmpty(OozieClient.java:1443)\n> at org.apache.oozie.client.OozieClient$BundleJobInfo.<init>(OozieClient.java:907)\n> at org.apache.oozie.client.OozieClient.getBundleJobInfo(OozieClient.java:953)\n> at org.apache.falcon.workflow.engine.OozieWorkflowEngine.getBundleInfo(OozieWorkflowEngine.java:1029)\n> at org.apache.falcon.workflow.engine.OozieWorkflowEngine.updateInternal(OozieWorkflowEngine.java:975)\n> at org.apache.falcon.workflow.engine.OozieWorkflowEngine.update(OozieWorkflowEngine.java:827)\n> Thanks [~samarthg] for pointing these\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-07T10:05:19Z"
  },
  "patches": [],
  "external_id": "FALCON-297"
},{
  "_id": {
    "$oid": "5bac97eac291c350402ff98e"
  },
  "message_id": "<377122099.1260735438281.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bac9787c291c350402fecd8"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bac97e9c291c350402ff95b"
  },
  "from_id": {
    "$oid": "5bac97bc57674ee167c914b6"
  },
  "to_ids": [
    {
      "$oid": "58c11c6c02ca40f8bfb1f59b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (XERCESJ-1407) renameNode creates wrong Node\n Implementation with PSVI, HTML and WML DOM",
  "body": "\n    [ https://issues.apache.org/jira/browse/XERCESJ-1407?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12789935#action_12789935 ] \n\nMichael Glavassevich commented on XERCESJ-1407:\n-----------------------------------------------\n\nLudger, there seems to be a lot more going on in your new patch than what you originally posted. I'm not sure of what other issues you're trying to address here but may make it easier to review if you separated them. I was pretty close to committing your first patch which seemed to address the two issues listed here, but can't view it anymore since you removed it.\n\n> renameNode creates wrong Node Implementation with PSVI, HTML and WML DOM\n> ------------------------------------------------------------------------\n>\n>                 Key: XERCESJ-1407\n>                 URL: https://issues.apache.org/jira/browse/XERCESJ-1407\n>             Project: Xerces2-J\n>          Issue Type: Bug\n>          Components: DOM (Level 3 Core)\n>    Affects Versions: 2.9.1\n>            Reporter: Ludger Bünger\n>            Assignee: Michael Glavassevich\n>         Attachments: RenameNodePatch.txt\n>\n>\n> I stumbled across an issue when using the DOM Level 3 renameNode method but this issue is actually more than only related to renameNode:\n> Depending on parameters the DOM Level 3 renameNode method analyses whether renaming a node would cause a change of node implementation type, i.e. whether an instance of ElementImpl or AttributeImpl will be renamed such that it aquires a Namespace and thus needs to be converted to their respective NS counterparts (ElementNSImpl, AttrNSImpl).\n> Depending on the ourcome of this, there are two issues:\n> Issue 1:\n> If the to-be-renamed node not an NS aware type (i.e. ElementImpl or AttrImpl) and a namespace shall be set, xerces instantiates a new ElementNSImpl/AttNSImpl by calling the class constructor for these hardcoded.\n> However, when using the PSVI-aware DOM, this is the wrong class type! It should be PSVIElementNSImpl instead!\n> Xerces should call document.createElement instead so the correct class will be instanciated.\n> Actually I think it is a general problem that xerces sometimes call node constructors hard coded instead of using the document.create methods and suggest changing this.\n> Issue 2:\n> If the to-be-renamed node is of an NS-implementation-type or the namespace is null, an internal rename method will be called upon the element/attribute implementation and the same node object will be returned.\n> This is fine for the standard implementation, however in sometimes wrong for the HTML and WML DOM.\n> The HTML and WML-DOM use specific element implementation classes i.e. HTMLHeadingElementImpl or HTMLParagraphElementImpl.\n> In these cases, instead of calling the internal rename method, the element should be re-created using the createElement method of it's document implementation.\n> The solution here is the same as for issue 1:\n> use the document.create methods for renaming an element.\n> However we need to query whether the used DOM implementation allows element instances to be renamed (general XML) or not (HTML, WML).\n> Please find attached a patch that:\n> 1) replaces every call to Node imlementation constructors (except instances of DocumentImpls) by calling the respective document.create method\n> 2) queries whether a DOM implementation permits node renaming and if not, re-created elements upon calling rename.\n> I attached a patch that fixes these two issues.\n> I'd be pleased if someone could review whether the proposed solution is ok.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: j-dev-unsubscribe@xerces.apache.org\nFor additional commands, e-mail: j-dev-help@xerces.apache.org\n\n",
  "date": {
    "$date": "2009-12-13T20:17:18Z"
  },
  "patches": [],
  "external_id": "XERCESJ-1407"
},{
  "_id": {
    "$oid": "5bacc4e056f6a00b020946b0"
  },
  "message_id": "<JIRA.12755602.1416057106000.175218.1422300215014@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc49356f6a00b02092f7d"
    },
    {
      "$oid": "5bacc49356f6a00b02092f7e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc49356f6a00b02092f7d"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (MAHOUT-1626) Support for required\n quasi-algebraic operations and starting with aggregating rows/blocks",
  "body": "\n    [ https://issues.apache.org/jira/browse/MAHOUT-1626?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14292274#comment-14292274 ] \n\nASF GitHub Bot commented on MAHOUT-1626:\n----------------------------------------\n\nGithub user dlyubimov commented on the pull request:\n\n    https://github.com/apache/mahout/pull/62#issuecomment-71520531\n  \n    what's the status?\n    \n    Also, what happens to H20 support of this? it'll just be failing? \n\n\n> Support for required quasi-algebraic operations and starting with aggregating rows/blocks\n> -----------------------------------------------------------------------------------------\n>\n>                 Key: MAHOUT-1626\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-1626\n>             Project: Mahout\n>          Issue Type: New Feature\n>          Components: Math\n>    Affects Versions: 1.0\n>            Reporter: Gokhan Capan\n>             Fix For: 1.0\n>\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-01-26T19:23:35Z"
  },
  "patches": [],
  "external_id": "MAHOUT-1626"
},{
  "_id": {
    "$oid": "5bbdf2dde8113566f6647d59"
  },
  "message_id": "<JIRA.13122666.1512418449000.381745.1512418500143@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [
    {
      "$oid": "5bbdf2dde8113566f6647d57"
    },
    {
      "$oid": "5bbdf2dde8113566f6647d58"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdf2dde8113566f6647d57"
  },
  "from_id": {
    "$oid": "5bbdf2ad57674ee16779d239"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd1e4f89451f55cdfd2"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (NUTCH-2470) CrawlDbReader -stats to show\n quantiles of score",
  "body": "Sebastian Nagel created NUTCH-2470:\n--------------------------------------\n\n             Summary: CrawlDbReader -stats to show quantiles of score\n                 Key: NUTCH-2470\n                 URL: https://issues.apache.org/jira/browse/NUTCH-2470\n             Project: Nutch\n          Issue Type: Improvement\n          Components: crawldb\n    Affects Versions: 1.13\n            Reporter: Sebastian Nagel\n            Priority: Minor\n             Fix For: 1.14\n\n\nThe command \"readdb -stats\" shows for the CrawlDatum score min., max. and average. Median and quartiles (quantiles, in general) would complete the statistics to get an impression how scores are distributed.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-12-04T20:15:00Z"
  },
  "patches": [],
  "external_id": "NUTCH-2470"
},{
  "_id": {
    "$oid": "5f27d3f546816ce7cf50977e"
  },
  "message_id": "<11352191.1098965528852.JavaMail.orion@beaver.codehaus.org>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d3ba46816ce7cf5091c8"
  },
  "from_id": {
    "$oid": "58c123f102ca40f8bfb1fc2a"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (MPANNOUNCEMENT-15) A way to override the team name in \"The ${pom.groupId} team\"",
  "body": "The following comment has been added to this issue:\n\n     Author: Shinobu Kawai\n    Created: Thu, 28 Oct 2004 8:11 AM\n       Body:\nHi Felipe,\n\nOops, I overlooked this guy:\n  maven.announcement.stylesheet.path\n\nI guess I can live with that.  I don't like copy and pasting, though.  ;)\n\n# And if you do decide to add the new property, sure I can make the full patch.\n\nBest regards,\n-- Shinobu Kawai\n---------------------------------------------------------------------\nView this comment:\n  http://jira.codehaus.org/browse/MPANNOUNCEMENT-15?page=comments#action_25839\n\n---------------------------------------------------------------------\nView the issue:\n  http://jira.codehaus.org/browse/MPANNOUNCEMENT-15\n\nHere is an overview of the issue:\n---------------------------------------------------------------------\n        Key: MPANNOUNCEMENT-15\n    Summary: A way to override the team name in \"The ${pom.groupId} team\"\n       Type: Wish\n\n     Status: Unassigned\n   Priority: Minor\n\n Original Estimate: Unknown\n Time Spent: Unknown\n  Remaining: Unknown\n\n    Project: maven-announcement-plugin\n   Versions:\n             1.4\n\n   Assignee: \n   Reporter: Shinobu Kawai\n\n    Created: Wed, 27 Oct 2004 9:46 PM\n    Updated: Thu, 28 Oct 2004 8:11 AM\n\nDescription:\nIt would be cool if there was a way to override the team name in the announcement.jsl.\n\nSomething like\n\nmaven.announcement.teamname = ${pom.groupId}\n\nand use this instead of ${pom.groupId} in the jsl.\n\n\n---------------------------------------------------------------------\nJIRA INFORMATION:\nThis message is automatically generated by JIRA.\n\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n\nIf you want more information on JIRA, or have a bug to report see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2004-10-28T08:12:08Z"
  },
  "patches": [],
  "external_id": "MPANNOUNCEMENT-15"
},{
  "_id": {
    "$oid": "5bbf0d2e35bc96443481e1f4"
  },
  "message_id": "<JIRA.12910018.1446563573000.16523.1447083370957@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf090e35bc964434814f5a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0d0f35bc96443481dd34"
    },
    {
      "$oid": "5bbf0d0f35bc96443481dd33"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0d0f35bc96443481dd33"
  },
  "from_id": {
    "$oid": "5bbf077457674ee16730d85e"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdff0"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (STORM-1161) Several source files lack an Apache\n License",
  "body": "\n    [ https://issues.apache.org/jira/browse/STORM-1161?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14996724#comment-14996724 ] \n\nSriharsha Chintalapani commented on STORM-1161:\n-----------------------------------------------\n\n[~dossett] after this patch went in I am unable to run mvn clean install and it gets stuck here\n{code}\nWarning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.\nCompiler warnings:\n  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'\nWarning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.\nWarning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.\nWarning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.\n{code}\nCan you please take a look.\n\n> Several source files lack an Apache License\n> -------------------------------------------\n>\n>                 Key: STORM-1161\n>                 URL: https://issues.apache.org/jira/browse/STORM-1161\n>             Project: Apache Storm\n>          Issue Type: Bug\n>            Reporter: Aaron Dossett\n>            Assignee: Aaron Dossett\n>             Fix For: 0.11.0\n>\n>\n> Several source files lack an Apache License.  This should be fixed and apache rat should verify license files as a standard part of full build.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-11-09T15:36:10Z"
  },
  "patches": [],
  "external_id": "STORM-1161"
},{
  "_id": {
    "$oid": "5bbe108db1ffc5570d041150"
  },
  "message_id": "<JIRA.12684031.1386768524540.17523.1386771787575@arcas>",
  "mailing_list_id": {
    "$oid": "5bbe0e4eb1ffc5570d03b7b4"
  },
  "reference_ids": [
    {
      "$oid": "5bbe106fb1ffc5570d040cb1"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe106fb1ffc5570d040cb1"
  },
  "from_id": {
    "$oid": "59bfa242f2a4565fe9fa1469"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (TIKA-1205) Allow PDFParser to fallback to other\n parser if there is an exception",
  "body": "\n    [ https://issues.apache.org/jira/browse/TIKA-1205?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13845429#comment-13845429 ] \n\nTim Allison commented on TIKA-1205:\n-----------------------------------\n\nThank you for your feedback!  TIKA-456 is the existing issue for general timeout capability.  I agree that it would be great to add.  TIKA-1205 is a very narrowly defined improvement for PDFParser.\n\n> Allow PDFParser to fallback to other parser if there is an exception\n> --------------------------------------------------------------------\n>\n>                 Key: TIKA-1205\n>                 URL: https://issues.apache.org/jira/browse/TIKA-1205\n>             Project: Tika\n>          Issue Type: Improvement\n>          Components: parser\n>            Reporter: Tim Allison\n>            Assignee: Tim Allison\n>            Priority: Trivial\n>             Fix For: 1.5\n>\n>\n> With TIKA-1201, there is now an option to use PDFBox's NonSequentialPDFParser instead of the traditional parser for parsing PDF files.  Following the description in PDFBOX-1199, it would be useful to allow fallback to the classic parser if NonSequentialPDFParser throws an IOException.  For the sake of symmetry, I propose a boolean useParserFallbackOnException parameter.  If this parameter is true, and if Tika's PDFParser is using the classic parser, Tika will fallback to the NonSequentialPDFParser if there is an IOException; if this parameter is true and if Tika's PDFParser is using the NonSequentialPDFParser it will fallback to the classic parser if there is an IOException.\n> Many thanks to Hong-Thai for championing the addition of the added NonSequentialPDFParser capability in TIKA-1201, and many thanks to Timo for PDFBox's NonSequentialPDFParser (PDFBOX-1199)!\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.4#6159)\n",
  "date": {
    "$date": "2013-12-11T14:23:07Z"
  },
  "patches": [],
  "external_id": "TIKA-1205"
},{
  "_id": {
    "$oid": "5bea9de59e73d744d412d67f"
  },
  "message_id": "<13917766.1175608652732.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9c1d9e73d744d4128033"
  },
  "from_id": {
    "$oid": "59bfb647f2a4565fe9259fce"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (DERBY-2514) convert lang/closed.java to junit",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-2514?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12486362 ] \n\nDaniel John Debrunner commented on DERBY-2514:\n----------------------------------------------\n\nNot code coverage, but test coverage. closed.java says \"Test execution of closed JDBC objects\". There is an existing JUnit test jdbcapi.ClosedObjectTest that seems to do the same thing.\n\n\n\n> convert lang/closed.java to junit\n> ---------------------------------\n>\n>                 Key: DERBY-2514\n>                 URL: https://issues.apache.org/jira/browse/DERBY-2514\n>             Project: Derby\n>          Issue Type: Test\n>          Components: Test\n>         Environment: convert lang/closed.java to junit\n>            Reporter: Ramandeep Kaur\n>         Assigned To: Ramandeep Kaur\n>            Priority: Minor\n>             Fix For: 10.3.0.0\n>\n>\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-04-03T06:57:32Z"
  },
  "patches": [],
  "external_id": "DERBY-2514"
},{
  "_id": {
    "$oid": "5bbf29bf30623e2888add004"
  },
  "message_id": "<JIRA.12993986.1470063306000.197255.1470129920625@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf295630623e2888adcbde"
    },
    {
      "$oid": "5bbf295630623e2888adcbdd"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf295630623e2888adcbdd"
  },
  "from_id": {
    "$oid": "5bbdaabc57674ee167d03561"
  },
  "to_ids": [
    {
      "$oid": "5bbf261257674ee1674f913a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (LENS-1250) Error while initializing saved query\n service on server start",
  "body": "\n     [ https://issues.apache.org/jira/browse/LENS-1250?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRajat Khandelwal updated LENS-1250:\n-----------------------------------\n    Status: Patch Available  (was: Open)\n\n> Error while initializing saved query service on server start\n> ------------------------------------------------------------\n>\n>                 Key: LENS-1250\n>                 URL: https://issues.apache.org/jira/browse/LENS-1250\n>             Project: Apache Lens\n>          Issue Type: Bug\n>            Reporter: Archana H\n>            Assignee: Rajat Khandelwal\n>         Attachments: LENS-1250.2016-08-02_14:55:02.patch\n>\n>\n> Error:\n> {CODE}\n> java.lang.RuntimeException: Cannot initialize saved query service\n>         at org.apache.lens.server.query.save.SavedQueryServiceImpl.init(SavedQueryServiceImpl.java:75) ~[classes/:na]\n>         at org.apache.hive.service.CompositeService.init(CompositeService.java:59) ~[hive-service-2.1.3-inm-SNAPSHOT.jar:2.1.3-inm-SNAPSHOT]\n>         at org.apache.lens.server.LensServices.init(LensServices.java:252) ~[classes/:na]\n>         at org.apache.lens.server.LensServer.startServices(LensServer.java:117) ~[classes/:na]\n>         at org.apache.lens.server.LensServer.<init>(LensServer.java:84) ~[classes/:na]\n>         at org.apache.lens.server.LensServer.createLensServer(LensServer.java:73) ~[classes/:na]\n>         at org.apache.lens.server.LensServer.main(LensServer.java:173) ~[classes/:na]\n> Caused by: org.apache.lens.server.api.error.LensException: Error initializing saved query dao\n>         at org.apache.lens.server.query.save.SavedQueryDao.<init>(SavedQueryDao.java:72) ~[classes/:na]\n>         at org.apache.lens.server.query.save.SavedQueryServiceImpl.init(SavedQueryServiceImpl.java:72) ~[classes/:na]\n>         ... 6 common frames omitted\n> Caused by: org.apache.lens.server.api.error.LensException: Cannot create saved query table!\n>         at org.apache.lens.server.query.save.SavedQueryDao.createSavedQueryTableIfNotExists(SavedQueryDao.java:85) ~[classes/:na]\n>         at org.apache.lens.server.query.save.SavedQueryDao.<init>(SavedQueryDao.java:70) ~[classes/:na]\n>         ... 7 common frames omitted\n> Caused by: java.sql.SQLException: Incorrect table definition; there can be only one TIMESTAMP column with CURRENT_TIMESTAMP in DEFAULT or ON UPDATE clause Query: CREATE TABLE IF NOT EXISTS saved_query (id int(11) NOT NULL AUTO_INCREMENT,name varchar(255) NOT NULL,description varchar(255) DEFAULT NULL,query longtext,params_json longtext,created_at timestamp DEFAULT CURRENT_TIMESTAMP,updated_at timestamp DEFAULT CURRENT_TIMESTAMP,  PRIMARY KEY (id)) Parameters: []\n>         at org.apache.commons.dbutils.AbstractQueryRunner.rethrow(AbstractQueryRunner.java:363) ~[commons-dbutils-1.5.jar:1.5]\n>         at org.apache.commons.dbutils.QueryRunner.update(QueryRunner.java:490) ~[commons-dbutils-1.5.jar:1.5]\n>         at org.apache.commons.dbutils.QueryRunner.update(QueryRunner.java:420) ~[commons-dbutils-1.5.jar:1.5]\n>         at org.apache.lens.server.query.save.SavedQueryDao.createSavedQueryTableIfNotExists(SavedQueryDao.java:83) ~[classes/:na]\n> {CODE}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-08-02T09:25:20Z"
  },
  "patches": [],
  "external_id": "LENS-1250"
},{
  "_id": {
    "$oid": "58bfc976aea7c7604a49eadb"
  },
  "message_id": "<1640920039.1224203084263.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "58bfc8a9aea7c7604a49c2e7"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "58bfc8c202ca40f8bf14788c"
  },
  "to_ids": [
    {
      "$oid": "58bfc8c002ca40f8bf147881"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (ZOOKEEPER-5) Upgrade Feature in Zookeeper server.",
  "body": "\n     [ https://issues.apache.org/jira/browse/ZOOKEEPER-5?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nPatrick Hunt updated ZOOKEEPER-5:\n---------------------------------\n\n      Resolution: Fixed\n    Hadoop Flags: [Reviewed]\n          Status: Resolved  (was: Patch Available)\n\nCommitted revision 705421.\nCommitted revision 705424.\n\n\n> Upgrade Feature in Zookeeper server.\n> ------------------------------------\n>\n>                 Key: ZOOKEEPER-5\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-5\n>             Project: Zookeeper\n>          Issue Type: New Feature\n>          Components: server\n>            Reporter: Mahadev konar\n>            Assignee: Mahadev konar\n>             Fix For: 3.0.0\n>\n>         Attachments: log.100000001, log.100001bf0, snapshot.100000000, snapshot.100001bec, ZOOKEEPER-5.patch, ZOOKEEPER-5.patch, ZOOKEEPER-5.patch, ZOOKEEPER-5.patch, ZOOKEEPER-5.patch, ZOOKEEPER-5.patch\n>\n>\n> We need an upgrade feature in zookeeper where we can upgrade the old databases to a new one.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-10-16T17:24:44Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-5"
},{
  "_id": {
    "$oid": "5bc8588457a11257de561bca"
  },
  "message_id": "<JIRA.13021724.1479463521000.492.1488119986108@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8580c57a11257de5619fb"
    },
    {
      "$oid": "5bc8580c57a11257de5619fa"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8580c57a11257de5619fa"
  },
  "from_id": {
    "$oid": "58c9de5702ca40f8bf20c1ae"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-3579) Giant glyphs when rendering\n embedded fonts",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-3579?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15884769#comment-15884769 ] \n\nASF subversion and git services commented on PDFBOX-3579:\n---------------------------------------------------------\n\nCommit 1784450 from [~lehmi] in branch 'pdfbox/branches/2.0'\n[ https://svn.apache.org/r1784450 ]\n\nPDFBOX-3579: use fraction value instead of ignoring it\n\n> Giant glyphs when rendering embedded fonts\n> ------------------------------------------\n>\n>                 Key: PDFBOX-3579\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-3579\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: FontBox\n>    Affects Versions: 2.0.3\n>            Reporter: Holger Floerke\n>            Assignee: Andreas Lehmkühler\n>            Priority: Minor\n>             Fix For: 2.0.5, 2.1.0\n>\n>         Attachments: 978-3-86764-721-2_t21.jpg, 978-3-86764-721-2_t2.pdf, PDFBOX-3579_FontMatrix_patch.txt, PDFBOX-3579-reduced2.pdf, PDFBOX-3579-reduced.pdf\n>\n>\n> Hi,\n> I try to generate an image out of the attatched pdf. PDFViewer like \"Acrobat Reader\" or the Ubuntu \"Document Viewer\" are able to display the PDF in a correct way. PDFBox is a little bit confused about the font.\n> Checked with the latest relase version:\n> java -jar pdfbox-app-2.0.3.jar PDFToImage 978-3-86764-721-2_t2.pdf\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2017-02-26T14:39:46Z"
  },
  "patches": [],
  "external_id": "PDFBOX-3579"
},{
  "_id": {
    "$oid": "60fac2cbc290fbbd48278bb9"
  },
  "message_id": "<JIRA.12754424.1415699338000.1349.1430186107003@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac240c290fbbd4827667b"
  },
  "reference_ids": [
    {
      "$oid": "60fac2c9c290fbbd48278b21"
    },
    {
      "$oid": "60fac2c9c290fbbd48278b20"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2c9c290fbbd48278b20"
  },
  "from_id": {
    "$oid": "58bfcf2f02ca40f8bf147ff9"
  },
  "to_ids": [
    {
      "$oid": "58bfcbdbe4f89451f55cdffa"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (CURATOR-164) curator-x-discovery:\n unregisterService is not guaranteed to remove the service, due to\n reconnectListener concurrency issue",
  "body": "\n    [ https://issues.apache.org/jira/browse/CURATOR-164?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14516171#comment-14516171 ] \n\nCameron McKenzie commented on CURATOR-164:\n------------------------------------------\n\nNot sure why the build has suddenly started breaking. It seems like it's happening with tests where they close a TestingServer and then recreate it to simulate connection loss. I've changed these over to use the TestingServer.restart() method and they seem to work ok now. Just running a full test locally to make sure nothing else is broken.\n\n> curator-x-discovery: unregisterService is not guaranteed to remove the service, due to reconnectListener concurrency issue\n> --------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: CURATOR-164\n>                 URL: https://issues.apache.org/jira/browse/CURATOR-164\n>             Project: Apache Curator\n>          Issue Type: Bug\n>          Components: Framework\n>    Affects Versions: 2.7.0\n>            Reporter: Rasmus Berg Palm\n>            Assignee: Jordan Zimmerman\n>            Priority: Critical\n>             Fix For: 2.8.0\n>\n>\n> In ServiceDiscoveryImpl:\n> When unregistering a service, the reconnect listener might fire while deleting the path.\n> This can cause a condition where the delete finishes successfully, the service is removed from services, and then the reRegisterServices completes successfully and the service is added back in ZK and in services, end result being that the service was not removed, even though unregisterService did not throw any exceptions. \n> Essentially the use of the internal 'services' cache makes for a nightmare of concurrency issues. I put this as critical as the library it's really not usable IMO.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-04-28T01:55:07Z"
  },
  "patches": [],
  "external_id": "CURATOR-164"
},{
  "_id": {
    "$oid": "5c57f207149eba7f38217dba"
  },
  "message_id": "<JIRA.12687355.1388930575665.32628.1392235940789@arcas>",
  "mailing_list_id": {
    "$oid": "5c57eed8149eba7f3821534a"
  },
  "reference_ids": [
    {
      "$oid": "5c57f207149eba7f38217db4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c57f207149eba7f38217db4"
  },
  "from_id": {
    "$oid": "5c57f202621a9a77b3b49ef1"
  },
  "to_ids": [
    {
      "$oid": "59677b275005bf27642aa642"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Comment Edited] (CONNECTORS-850) Maximum interval in\n dynamic crawling",
  "body": "\n    [ https://issues.apache.org/jira/browse/CONNECTORS-850?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13899490#comment-13899490 ] \n\nFlorian Schmedding edited comment on CONNECTORS-850 at 2/12/14 8:11 PM:\n------------------------------------------------------------------------\n\nminimum interval: 2 min\nmaximum interval: 4 min\nThe job was run a few times before it was set to dynamic crawling. \n\n{noformat}\n02-12-2014 18:27:45.475 fetch\n02-12-2014 18:23:45.702 document ingest (solr localhost)\n02-12-2014 18:23:44.921 fetch\n02-12-2014 18:19:44.451 document ingest (solr localhost)\n02-12-2014 18:19:43.837 fetch\n02-12-2014 18:15:42.929 fetch\n02-12-2014 18:11:41.582 document ingest (solr localhost)\n02-12-2014 18:11:41.058 fetch\n*** document changed\n02-12-2014 18:07:40.744 document ingest (solr localhost)\n02-12-2014 18:07:40.249 fetch\n02-12-2014 18:03:37.546 fetch\n02-12-2014 17:59:36.426 fetch\n02-12-2014 17:55:34.297 fetch\n02-12-2014 17:51:33.431 document ingest (solr localhost)\n02-12-2014 17:51:32.973 fetch\n*** job changed from scheduled to dynamic crawling\n02-12-2014 17:24:24.560 document ingest (solr localhost)\n02-12-2014 17:24:24.413 fetch\n02-12-2014 17:21:17.042 document ingest (solr localhost)\n02-12-2014 17:21:16.919 fetch\n02-12-2014 17:18:15.892 document ingest (solr localhost)\n02-12-2014 17:18:15.643 fetch\n02-12-2014 17:17:18.500 fetch\n02-12-2014 17:12:18.564 fetch\n02-12-2014 17:11:38.665 fetch\n02-12-2014 17:10:55.889 fetch\n02-12-2014 17:10:13.386 fetch\n02-12-2014 17:09:31.423 document ingest (solr localhost)\n02-12-2014 17:09:28.560 fetch\n02-12-2014 17:05:10.197 document ingest (solr localhost)\n02-12-2014 17:05:07.356 fetch\n02-12-2014 17:00:37.508 document ingest (solr localhost)\n02-12-2014 17:00:33.256 fetch\n{noformat}\n\n\nwas (Author: florianschmedding):\nminimum interval: 2 min\nmaximum interval: 4 min\nThe job was run a few times before it was set to dynamic crawling. \n\n{noformat}\n02-12-2014 18:27:45.475 fetch\n02-12-2014 18:23:45.702 document ingest (solr localhost)\n02-12-2014 18:23:44.921 fetch\n02-12-2014 18:19:44.451 document ingest (solr localhost)\n02-12-2014 18:19:43.837 fetch\n02-12-2014 18:15:42.929 fetch\n02-12-2014 18:11:41.582 document ingest (solr localhost)\n02-12-2014 18:11:41.058 fetch\n*** document changed\n02-12-2014 18:07:40.744 document ingest (solr localhost)\n02-12-2014 18:07:40.249 fetch\n02-12-2014 18:03:37.546 fetch\n02-12-2014 17:59:36.426 fetch\n02-12-2014 17:55:34.297 fetch\n02-12-2014 17:51:33.431 document ingest (solr localhost)\n02-12-2014 17:51:32.973 fetch\n*** job changed from scheduled to dynamic crawling\n02-12-2014 17:24:24.560 document ingest (solr localhost)\n02-12-2014 17:24:24.413 fetch\n02-12-2014 17:21:17.042 document ingest (solr localhost)\n02-12-2014 17:21:16.919 fetch\n02-12-2014 17:18:15.892 document ingest (solr localhost)\n{noformat}\n\n> Maximum interval in dynamic crawling\n> ------------------------------------\n>\n>                 Key: CONNECTORS-850\n>                 URL: https://issues.apache.org/jira/browse/CONNECTORS-850\n>             Project: ManifoldCF\n>          Issue Type: New Feature\n>          Components: Framework crawler agent\n>    Affects Versions: ManifoldCF 1.4.1\n>            Reporter: Florian Schmedding\n>            Assignee: Karl Wright\n>            Priority: Minor\n>              Labels: features\n>             Fix For: ManifoldCF 1.5\n>\n>\n> Currently, the dynamic crawling method used for a continuous job extends the reseed and recrawl intervals when no changes are found in a checked document. However, it should be possible to restrict this extension to a maximum value in order to make sure that new documents are discovered within a certain interval.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-12T20:12:20Z"
  },
  "patches": [],
  "external_id": "CONNECTORS-850"
},{
  "_id": {
    "$oid": "5bacc4d356f6a00b0209430f"
  },
  "message_id": "<JIRA.12770232.1422318163000.132153.1426691978279@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc4c456f6a00b02093e1d"
    },
    {
      "$oid": "5bacc4c456f6a00b02093e1c"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc4c456f6a00b02093e1c"
  },
  "from_id": {
    "$oid": "5bacc47357674ee167dcb16f"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (MAHOUT-1638) H2O bindings fail at\n drmParallelizeWithRowLabels(...)",
  "body": "\n     [ https://issues.apache.org/jira/browse/MAHOUT-1638?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAndrew Palumbo updated MAHOUT-1638:\n-----------------------------------\n    Fix Version/s:     (was: 1.0)\n                   0.10.0\n\n> H2O bindings fail at drmParallelizeWithRowLabels(...)\n> -----------------------------------------------------\n>\n>                 Key: MAHOUT-1638\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-1638\n>             Project: Mahout\n>          Issue Type: Bug\n>    Affects Versions: 0.10.0\n>            Reporter: Andrew Palumbo\n>            Priority: Blocker\n>              Labels: DSL, h2o, scala\n>             Fix For: 0.10.0\n>\n>\n> The H2OHelper.drmFromMatrix(...) function fails when trying to write row label String keys to a water.fvec.Vec.:\n> {code:java}\n>  java.lang.IllegalArgumentException: Not a String\n>   at water.fvec.Chunk.set_impl(Chunk.java:507)\n>   at water.fvec.Chunk.set0(Chunk.java:469)\n>   at water.fvec.Chunk.set(Chunk.java:371)\n>   at water.fvec.Vec$Writer.set(Vec.java:803)\n>   at org.apache.mahout.h2obindings.H2OHelper.drmFromMatrix(H2OHelper.java:331)\n>   at org.apache.mahout.h2obindings.H2OEngine$.drmParallelizeWithRowLabels(H2OEngine.scala:83)                                                                   \n>   at org.apache.mahout.math.drm.package$.drmParallelizeWithRowLabels(package.scala:67)\n> {code} \n> This causes an exception when calling drm.drmParallelizeWithRowLabels(...)\n> To reproduce, apply [PR#72: Enable Naive Bayes Tests in h2o Module|https://github.com/apache/mahout/pull/72] and run:\n> {code} $ mvn test \n> {code}\n> from the h2o module:\n> {code:java}\n> - NB Aggregator *** FAILED ***\n>   java.lang.IllegalArgumentException: Not a String\n>   at water.fvec.Chunk.set_impl(Chunk.java:507)\n>   at water.fvec.Chunk.set0(Chunk.java:469)\n>   at water.fvec.Chunk.set(Chunk.java:371)\n>   at water.fvec.Vec$Writer.set(Vec.java:803)\n>   at org.apache.mahout.h2obindings.H2OHelper.drmFromMatrix(H2OHelper.java:331)\n>   at org.apache.mahout.h2obindings.H2OEngine$.drmParallelizeWithRowLabels(H2OEngine.scala:83)                                                                   \n>   at org.apache.mahout.math.drm.package$.drmParallelizeWithRowLabels(package.scala:67)                                                                          \n>   at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply$mcV$sp(NBTestBase.scala:91)                                                            \n>   at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply(NBTestBase.scala:70)                                                                   \n>   at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply(NBTestBase.scala:70)                                                                   \n>   ...\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-03-18T15:19:38Z"
  },
  "patches": [],
  "external_id": "MAHOUT-1638"
},{
  "_id": {
    "$oid": "5f27c258b7fc3b4361ce685d"
  },
  "message_id": "<3246714.39981273265868471.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5f27c121b7fc3b4361ce181a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27c258b7fc3b4361ce685c"
  },
  "from_id": {
    "$oid": "58c11ed902ca40f8bfb1f8c8"
  },
  "to_ids": [
    {
      "$oid": "59677e435005bf27642aa6e6"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (VYSPER-195) Remove dependency on nekopull",
  "body": "\n    [ https://issues.apache.org/jira/browse/VYSPER-195?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12865304#action_12865304 ] \n\nNiklas Gustavsson commented on VYSPER-195:\n------------------------------------------\n\ncommit -m \"Removing dependency on nekopull parser (VYSPER-195)\" /Users/niklas/Documents/svn/vysper-trunk/server/core/pom.xml /Users/niklas/Documents/svn/vysper-trunk/server/core/src/main/java/org/apache/vysper/xmpp/parser/AbstractNekopullStreamParser.java /Users/niklas/Documents/svn/vysper-trunk/server/core/src/main/java/org/apache/vysper/xmpp/parser/InputStreamParser.java /Users/niklas/Documents/svn/vysper-trunk/server/core/src/main/java/org/apache/vysper/xmpp/parser/StringStreamParser.java /Users/niklas/Documents/svn/vysper-trunk/server/core/src/test/java/org/apache/vysper/xmpp/parser/AbstractStreamParserTestCase.java /Users/niklas/Documents/svn/vysper-trunk/server/core/src/test/java/org/apache/vysper/xmpp/parser/StringStreamParserTestCase.java /Users/niklas/Documents/svn/vysper-trunk/server/core/src/test/java/org/apache/vysper/xmpp/server/response/ServerResponsesTestCase.java\n    Sending        /Users/niklas/Documents/svn/vysper-trunk/server/core/pom.xml\n    Deleting       /Users/niklas/Documents/svn/vysper-trunk/server/core/src/main/java/org/apache/vysper/xmpp/parser/AbstractNekopullStreamParser.java\n    Deleting       /Users/niklas/Documents/svn/vysper-trunk/server/core/src/main/java/org/apache/vysper/xmpp/parser/InputStreamParser.java\n    Deleting       /Users/niklas/Documents/svn/vysper-trunk/server/core/src/main/java/org/apache/vysper/xmpp/parser/StringStreamParser.java\n    Deleting       /Users/niklas/Documents/svn/vysper-trunk/server/core/src/test/java/org/apache/vysper/xmpp/parser/AbstractStreamParserTestCase.java\n    Deleting       /Users/niklas/Documents/svn/vysper-trunk/server/core/src/test/java/org/apache/vysper/xmpp/parser/StringStreamParserTestCase.java\n    Sending        /Users/niklas/Documents/svn/vysper-trunk/server/core/src/test/java/org/apache/vysper/xmpp/server/response/ServerResponsesTestCase.java\n    Transmitting file data ...\n    Committed revision 942216.\n\n> Remove dependency on nekopull\n> -----------------------------\n>\n>                 Key: VYSPER-195\n>                 URL: https://issues.apache.org/jira/browse/VYSPER-195\n>             Project: VYSPER\n>          Issue Type: Task\n>    Affects Versions: 0.5\n>            Reporter: Niklas Gustavsson\n>            Assignee: Niklas Gustavsson\n>             Fix For: 0.6\n>\n>\n> We currently depend on nekopull, but only uses it in some no longer needed unit tests. Clean this up and remove the dependency.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-05-07T16:57:48Z"
  },
  "patches": [],
  "external_id": "VYSPER-195"
},{
  "_id": {
    "$oid": "5f27bdc7a7dc6ca79d80a669"
  },
  "message_id": "<JIRA.13252532.1566509473000.42751.1566681240109@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27bd66a7dc6ca79d809788"
  },
  "reference_ids": [
    {
      "$oid": "5f27bdbba7dc6ca79d80a2eb"
    },
    {
      "$oid": "5f27bdbba7dc6ca79d80a2ec"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bdbba7dc6ca79d80a2eb"
  },
  "from_id": {
    "$oid": "5f27bb67af02e2d6de59a525"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c04"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (HTTPCLIENT-2013) Connect timeout throws\n HttpHostConnectException (ConnectException) instead of\n ConnectTimeoutException (IOException)",
  "body": "\n    [ https://issues.apache.org/jira/browse/HTTPCLIENT-2013?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16915095#comment-16915095 ] \n\nASF subversion and git services commented on HTTPCLIENT-2013:\n-------------------------------------------------------------\n\nCommit d5e1392840fcbd9ff9d9c5b19d8ed9da2883c224 in httpcomponents-client's branch refs/heads/development from Oleg Kalnichevski\n[ https://gitbox.apache.org/repos/asf?p=httpcomponents-client.git;h=d5e1392 ]\n\nHTTPCLIENT-2013: revised handling of connect exceptions; improved consistency in behavior of the classic and async clients; ConnectTimeoutException now extends SocketTimeoutException\n\n\n> Connect timeout throws HttpHostConnectException (ConnectException) instead of ConnectTimeoutException (IOException)\n> -------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: HTTPCLIENT-2013\n>                 URL: https://issues.apache.org/jira/browse/HTTPCLIENT-2013\n>             Project: HttpComponents HttpClient\n>          Issue Type: Improvement\n>          Components: HttpClient (classic)\n>    Affects Versions: 4.5.9\n>         Environment: Linux\n> openjdk version \"1.8.0_222\"\n> OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1-b10)\n> OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)\n>            Reporter: Chanseok Oh\n>            Priority: Minor\n>\n> It is obvious from the code in DefaultHttpClientConnectionOperator that it is supposed to throw ConnectTimeoutException in the case of connection timeout:\n> {code:java}\n> } catch (final ConnectException ex) {\n>     if (last) {\n>         final String msg = ex.getMessage();\n>         throw \"Connection timed out\".equals(msg)\n>                         ? new ConnectTimeoutException(ex, host, addresses)\n>                         : new HttpHostConnectException(ex, host, addresses);\n>     }\n> {code}\n> Recently, we've upgraded Apache HttpClient (indirectly through Google HTTP Client), and our production code handling ConnectionException got broken due to DefaultHttpClientConnectionOperator throwing HttpHostConnectException that extends ConnectionException. (OTOH, ConnectTimeoutException is an IOException and not a ConnectionException.)\n> Java version:\n> {code}\n> openjdk version \"1.8.0_222\"\n> OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1-b10)\n> OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)\n> {code}\n> Example code to reproduce:\n> {code:java}\n> public static void main(String[] args) throws IOException {\n>    // example.com is reserved by the DNS standard and will always trigger timeout\n>    try (CloseableHttpClient client = HttpClients.createDefault();\n>        CloseableHttpResponse response = client.execute(new HttpGet(\"https://example.com:81\"))) {}\n> }\n> {code}\n> It currently throws the other exception HttpHostConnectException rather than the supposed ConnectTimeoutException.\n> {code}\n> Exception in thread \"main\" org.apache.http.conn.HttpHostConnectException: Connect to example.com:81 [example.com/93.184.216.34, example.com/2606:2800:220:1:248:1893:25c8:1946] failed: Connection timed out (Connection timed out)\n> \tat org.apache.http.impl.conn.HttpClientConnectionOperator.connect(HttpClientConnectionOperator.java:138)\n> \tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:314)\n> \tat org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:357)\n> \tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:218)\n> \tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:194)\n> \tat org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:85)\n> \tat org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:108)\n> \tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:186)\n> \tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n> \tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:106)\n> \tat com.example.ApacheHttpClient2.main(ApacheHttpClient2.java:13)\n> Caused by: java.net.ConnectException: Connection timed out (Connection timed out)\n> \tat java.net.PlainSocketImpl.socketConnect(Native Method)\n> \tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n> \tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n> \tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n> \tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n> \tat java.net.Socket.connect(Socket.java:589)\n> \tat sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)\n> \tat org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:251)\n> \tat org.apache.http.impl.conn.HttpClientConnectionOperator.connect(HttpClientConnectionOperator.java:118)\n> \t... 10 more\n> {code}\n>  \n>  \n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.2#803003)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@hc.apache.org\nFor additional commands, e-mail: dev-help@hc.apache.org\n\n",
  "date": {
    "$date": "2019-08-24T21:14:00Z"
  },
  "patches": [],
  "external_id": "HTTPCLIENT-2013"
},{
  "_id": {
    "$oid": "58bfcfd315d83644fcc4bde8"
  },
  "message_id": "<JIRA.12862422.1441721280000.279277.1441723245988@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfcfc915d83644fcc4bbed"
    },
    {
      "$oid": "58bfcfc915d83644fcc4bbee"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfcfc915d83644fcc4bbed"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a6"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-2268) Zookeeper doc creation fails on\n windows",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-2268?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14734911#comment-14734911 ] \n\nHadoop QA commented on ZOOKEEPER-2268:\n--------------------------------------\n\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12754638/ZOOKEEPER-2268-01.patch\n  against trunk revision 1701505.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2862//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2862//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2862//console\n\nThis message is automatically generated.\n\n> Zookeeper doc creation fails on windows\n> ---------------------------------------\n>\n>                 Key: ZOOKEEPER-2268\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-2268\n>             Project: ZooKeeper\n>          Issue Type: Bug\n>          Components: build\n>    Affects Versions: 3.5.0\n>            Reporter: Arshad Mohammad\n>            Assignee: Arshad Mohammad\n>             Fix For: 3.5.2\n>\n>         Attachments: ZOOKEEPER-2268-01.patch\n>\n>\n> Zookeeper doc creation fails on windows with following error\n> {code}\n> D:\\gitHome\\zookeeper-trunk\\build.xml:484: Execute failed: java.io.IOException: Cannot run program \"C:\\non-install\\apache-forrest-0.9\\bin\\forrest\"\n> y \"D:\\gitHome\\zookeeper-trunk\\src\\docs\"): CreateProcess error=193, %1 is not a valid Win32 application\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-09-08T14:40:45Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-2268"
},{
  "_id": {
    "$oid": "5f27c4cfe2367af241b6d702"
  },
  "message_id": "<JIRA.12625123.1356207754794.53217.1364263995408@arcas>",
  "mailing_list_id": {
    "$oid": "5f27c38ce2367af241b67514"
  },
  "reference_ids": [
    {
      "$oid": "5f27c4bae2367af241b6d016"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27c4bae2367af241b6d016"
  },
  "from_id": {
    "$oid": "58bfd0c702ca40f8bf1482ca"
  },
  "to_ids": [
    {
      "$oid": "58bfd014e4f89451f55ce17b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (BIGTOP-816) Bigtop 0.6.0 release",
  "body": "\n    [ https://issues.apache.org/jira/browse/BIGTOP-816?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13613403#comment-13613403 ] \n\nPeter Linnell commented on BIGTOP-816:\n--------------------------------------\n\n+1 for Hue version bump\n                \n> Bigtop 0.6.0 release\n> --------------------\n>\n>                 Key: BIGTOP-816\n>                 URL: https://issues.apache.org/jira/browse/BIGTOP-816\n>             Project: Bigtop\n>          Issue Type: Task\n>            Reporter: Roman Shaposhnik\n>            Assignee: Roman Shaposhnik\n>         Attachments: BIGTOP-816.patch.txt\n>\n>\n> We have to start collecting feedback and defining BOM/supported platforms list for the next quarterly release of Bigtop. Given our quarterly release schedule 0.6.0 should be coming out sometime in March 2013.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-03-26T02:13:15Z"
  },
  "patches": [],
  "external_id": "BIGTOP-816"
},{
  "_id": {
    "$oid": "5f27d165ee5dd685a6706d3a"
  },
  "message_id": "<CABXgNG=wkqOPD2u+XTB2eonqbcb1_gjdD02=dUiPhVTTnWg+7w@mail.gmail.com>",
  "mailing_list_id": {
    "$oid": "5f27d01eee5dd685a670262f"
  },
  "reference_ids": [
    {
      "$oid": "5f27d164ee5dd685a6706cef"
    },
    {
      "$oid": "5f27d165ee5dd685a6706d39"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27d165ee5dd685a6706d39"
  },
  "from_id": {
    "$oid": "59bf936df2a4565fe9e9f38c"
  },
  "to_ids": [
    {
      "$oid": "59bf92eaf2a4565fe9e6a45b"
    }
  ],
  "cc_ids": [],
  "subject": "Re: Re (OAK-36) Implement a query parser - what about indexing?",
  "body": "On Mon, Mar 26, 2012 at 12:56 PM, Jukka Zitting <jukka.zitting@gmail.com> wrote:\n> Hi,\n>\n> There's a number of points in this thread that I wanted to address, so\n> instead of replying to them individually, let me try to summarize my\n> thinking.\n>\n> One of the bigger pain points in the Jackrabbit 2.x architecture has\n> been the query engine and the workspace-global query index that has\n> been pretty difficult to customize for special needs and to handle in\n> terms of backup/recovery and scaling to multiple cluster nodes. My\n> wish for Oak is that we come up with a much more flexible search and\n> indexing architecture that solves these issues and is easy to extend\n> for any future use cases we may encounter.\n>\n> I think the biggest issue, as brought up by Alex and then elaborated\n> by Ard, is the way we handle indexing. Instead of having a single,\n> more or less fixed index for a repository like in Jackrabbit 2.x, Oak\n> should provide generic extension points that various different kinds\n> of indexing components could hook into. We should have at least three\n> such extension points: pre- and post-commit hooks, and observation\n> based on the commit journal.\n>\n> For example a low-level UUID-to-path index should preferably use the\n> pre-commit hook for atomic index updates as a part of each commit. A\n> post-commit hook could be used to trigger full-text extraction of\n> nt:file binaries, a bit like we currently do in Jackrabbit 2.x. And an\n> observation client could use the commit journal to feed an external\n> Solr index for application-level index features. A given deployment\n> can choose which ones of these and any other indexing components are\n> needed based on relevant application needs and related\n> performance/scalability overhead. A single solution does not fit all\n> needs, so we need to make such customization as easy as possible.\n>\n> On the other hand there's a lot of value in having a single, unified\n> query abstraction instead of having client applications reach out\n> directly to Solr, Lucene, or custom indexes. Thus, in addition to the\n> extensions points for indexing, we need a way for the indexing\n> components to extend the Oak query engine with ways to evaluate given\n> queries against the various configured indexes. This way all\n> applications can use the same generic Oak query API (exposed through\n> QueryManager in JCR, DASL in WebDAV, and/or something else in JSOP)\n> while leveraging the custom indexes available in each deployment.\n\nThanks for this summary. I now really understand what the goals are\nand how to achieve it. Especially the unified generic Oak query API is\nsomething I really like. Currently, for Hippo, I am doing something\nsimilar for the query api, that can seamlessly delegate to Solr or\njackrabbit, both returning a jcr node iterator (although the solr\nindex through solrj can also return plain pojo's). I really like the\nfirst option (pre-commit example) and third (observation based), and\nstill see many bears on the road for the second (full-text on\npost-commit)\n\nI've one more question regarding the oak search/indexes : Will we be\nable to query that returns something else than jcr nodes/rows? I\nfrequently want to be able to get a query result from the repository\nthat cannot be returned as node iterators. For example query on stats,\nor a query for 'auto-completion' on some property (thus return some\npart of the TermEnum for example)\n\nRegards Ard\n\n>\n> BR,\n>\n> Jukka Zitting\n\n\n\n-- \nAmsterdam - Oosteinde 11, 1017 WT Amsterdam\nBoston - 1 Broadway, Cambridge, MA 02142\n\nUS +1 877 414 4776 (toll free)\nEurope +31(0)20 522 4466\nwww.onehippo.com\n",
  "date": {
    "$date": "2012-03-26T13:14:58Z"
  },
  "patches": [],
  "external_id": "OAK-36"
},{
  "_id": {
    "$oid": "5bc86f2f57a11257de56c5b8"
  },
  "message_id": "<JIRA.12625202.1356405473143.50753.1356405975846@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc86f2f57a11257de56c5b5"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc86f2f57a11257de56c5b5"
  },
  "from_id": {
    "$oid": "5bc86f2f57674ee167d71121"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-1479) Text location in ExtractText",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1479?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nlucas leong updated PDFBOX-1479:\n--------------------------------\n\n    Description: \nThe sequence of output in ExtractText with my sample(attachment) goes something wrong\nIdeal sequence = 1WAP - 0460247 O 0000001 - 02\nbut the output sequence = 1WAP - O - 020460247 0000001\n\nBut the sequence of PrintTextLocations is correct\n\n  was:The sequence in ExtractText with my sample has something wrong\n\n    \n> Text location in ExtractText\n> ----------------------------\n>\n>                 Key: PDFBOX-1479\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1479\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Text extraction\n>    Affects Versions: 1.7.1\n>         Environment: Ubuntu 10.04\n>            Reporter: lucas leong\n>            Priority: Minor\n>         Attachments: tr370n.pdf\n>\n>\n> The sequence of output in ExtractText with my sample(attachment) goes something wrong\n> Ideal sequence = 1WAP - 0460247 O 0000001 - 02\n> but the output sequence = 1WAP - O - 020460247 0000001\n> But the sequence of PrintTextLocations is correct\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-12-25T03:26:15Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1479"
},{
  "_id": {
    "$oid": "5bea9af59e73d744d41240b8"
  },
  "message_id": "<7088672.37111277375870190.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9af39e73d744d412402d"
  },
  "from_id": {
    "$oid": "5bea97ad35e3ea2b7b4d7d4f"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-4706) Remove stale and potentially unused\n code Request.writeEncryptedScalarStream",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-4706?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKristian Waagan updated DERBY-4706:\n-----------------------------------\n\n    Fix Version/s: 10.6.1.1\n\nBackported to 10.6 with revision 957498.\nFYI, the patch doesn't currently merge cleanly with the 10.5 branch.\n\n> Remove stale and potentially unused code Request.writeEncryptedScalarStream\n> ---------------------------------------------------------------------------\n>\n>                 Key: DERBY-4706\n>                 URL: https://issues.apache.org/jira/browse/DERBY-4706\n>             Project: Derby\n>          Issue Type: Task\n>          Components: Network Client\n>    Affects Versions: 10.7.0.0\n>            Reporter: Kristian Waagan\n>            Assignee: Kristian Waagan\n>            Priority: Trivial\n>             Fix For: 10.6.1.1, 10.7.0.0\n>\n>         Attachments: derby-4706-1a-remove_writeEncryptedScalarStream_and_friends.diff\n>\n>\n> The code in net.Request.writeEncryptedScalarStream() is in a bad state, for instance:\n>  - it materializes the stream\n>  - it doesn't support streams longer than Integer.MAX_VALUE\n>    (this is fine for BLOB since Derby limits the LOB size to 2G-1, but for CLOBs the data stream may be longer since the length is expressed in characters)\n>  - in some error situations it causes a disconnect\n> Since the security mechanisms required to run this code (SECMEC_EUSRIDDTA and  SECMEC_EUSRPWDDTA) are apparently supported in the client but not in the network server, deleting it seems like a good option considering its state.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-06-24T06:37:50Z"
  },
  "patches": [],
  "external_id": "DERBY-4706"
},{
  "_id": {
    "$oid": "5bacc62756f6a00b0209a5d5"
  },
  "message_id": "<33086653.79401292178421064.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacc62156f6a00b0209a3e7"
  },
  "from_id": {
    "$oid": "5bacc4d557674ee167dd96c2"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (MAHOUT-564) KMeansClusterer does not use\n distanceThreshold parameter in testConvergence(Iterable<Cluster> clusters,\n double distanceThreshold) method",
  "body": "\n     [ https://issues.apache.org/jira/browse/MAHOUT-564?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nFrank Scholten updated MAHOUT-564:\n----------------------------------\n\n    Status: Patch Available  (was: Open)\n\n> KMeansClusterer does not use distanceThreshold parameter in testConvergence(Iterable<Cluster> clusters, double distanceThreshold) method\n> ----------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: MAHOUT-564\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-564\n>             Project: Mahout\n>          Issue Type: Bug\n>          Components: Clustering\n>    Affects Versions: 0.4\n>            Reporter: Frank Scholten\n>             Fix For: 0.5\n>\n>         Attachments: MAHOUT-564.patch\n>\n>\n> While running KMeansClusterer#runKMeansIteration sequentially I noticed that the distanceThreshold parameter is not used. The convergenceDelta field is used instead. However, it's initialized at 0 and only set when creating a KMeansClusterer with a Configuration object, which is only used in a MapReduce setting.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-12-12T13:27:01Z"
  },
  "patches": [],
  "external_id": "MAHOUT-564"
},{
  "_id": {
    "$oid": "5f27bdf4a7dc6ca79d80b4af"
  },
  "message_id": "<JIRA.13142708.1520294235000.375898.1520610060603@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27bd66a7dc6ca79d809788"
  },
  "reference_ids": [
    {
      "$oid": "5f27bdf3a7dc6ca79d80b490"
    },
    {
      "$oid": "5f27bdf3a7dc6ca79d80b48f"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bdf3a7dc6ca79d80b48f"
  },
  "from_id": {
    "$oid": "58c11e1302ca40f8bfb1f6d6"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c04"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Comment Edited] (HTTPCORE-517) Allow SecurityManager to\n stop socket connections",
  "body": "\n    [ https://issues.apache.org/jira/browse/HTTPCORE-517?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16393025#comment-16393025 ] \n\nGary Gregory edited comment on HTTPCORE-517 at 3/9/18 3:40 PM:\n---------------------------------------------------------------\n\nResolved in git {{master}} for the upcoming 5.0 and in 4.4.x for the upcoming 4.4.10.\n\nPlease verify and close.\n\n\nwas (Author: garydgregory):\nResolved in git {{master}} for the upcoming 5.0 and in 4.4.x for the upcoming 4.4.10.\n\n> Allow SecurityManager to stop socket connections\n> ------------------------------------------------\n>\n>                 Key: HTTPCORE-517\n>                 URL: https://issues.apache.org/jira/browse/HTTPCORE-517\n>             Project: HttpComponents HttpCore\n>          Issue Type: Improvement\n>          Components: HttpCore NIO\n>            Reporter: Paul Thompson\n>            Assignee: Gary Gregory\n>            Priority: Major\n>             Fix For: 4.4.10, 5.0-beta3\n>\n>\n> Utilising a java security manager you're able to block certain socket connections from taking place. This can be useful to block outgoing connections for all components.\n> {code:java}\n> @Override\n> public void checkConnect(String host, int port) {\n>     if(port != -1) {\n>         for (String bannedPerm : bannedSocketPerms) {\n>             if (host.equalsIgnoreCase(bannedPerm)) {\n>                 throw new new SecurityException();\n>             }\n>         }\n>     }\n> }{code}\n> Unfortunately when doing this, the apache reactor shuts down. The call site is in the {{DefaultConnectingIOReactor}}. \n> {code:java}\n> final boolean connected = socketChannel.connect(request.getRemoteAddress());\n> {code}\n> this line is wrapped in a try/catch that catches an {{IOException}}. This means the {{SecurityException}} is propagated out, and is never offered to be caught (even with the {{ExceptionHandler}} that you can set. \n> It would be an improvement to be able to throw these types of exceptions and have the reactor continue on. It's very understandable for the {{SecurityException}} to be a transient error and as such shouldn't shut down the entire reactor by default.\n> Either that, or one should be able to define a {{handleRuntimeException}} (as seen in the {{BaseIOREactor}}) in such a way that it would get the option to handle the exceptions that are thrown as part of the {{processEvents}} call in the {{AbstractIOReactor}}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@hc.apache.org\nFor additional commands, e-mail: dev-help@hc.apache.org\n\n",
  "date": {
    "$date": "2018-03-09T15:41:00Z"
  },
  "patches": [],
  "external_id": "HTTPCORE-517"
},{
  "_id": {
    "$oid": "5bacb3b1faaadd76f8aa44b9"
  },
  "message_id": "<14412940.47161287498688196.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb3affaaadd76f8aa445b"
  },
  "from_id": {
    "$oid": "5baca91c57674ee167cde68e"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (PIG-1675) Suggest to allow PigServer can register\n pig script from InputStream",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-1675?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJeff Zhang updated PIG-1675:\n----------------------------\n\n    Status: Patch Available  (was: Open)\n\n> Suggest to allow PigServer can register pig script from InputStream\n> -------------------------------------------------------------------\n>\n>                 Key: PIG-1675\n>                 URL: https://issues.apache.org/jira/browse/PIG-1675\n>             Project: Pig\n>          Issue Type: Improvement\n>    Affects Versions: 0.8.0\n>            Reporter: Jeff Zhang\n>            Assignee: Jeff Zhang\n>             Fix For: 0.9.0\n>\n>         Attachments: pig-findbugs-report.html, PIG_1675.patch, PIG_1675_2.patch, PIG_1675_3.patch\n>\n>\n> Currently, Pig only allow users to register script from file. Although it satisfy most people's requirements, sometimes people hope to build pig script dynamically using code, then they need to create temp file for the script they build. So here I suggest to allow PigServer be able to register pig script from InputStream.\n> InputStream is a more general type than File, pig script can been from file (FileInputStream)\n> or from in-memory (ByteArrayInputStream) even it can been from remote machines (SocketInputStream)\n> Here's a blog which explains why using InputStream is better than using File in interface http://java.dzone.com/articles/using-files-your-interfaces-0\n> So I suggest to add the following 4 methods in PigServer:\n> {code}\n> public void registerScript(InputStream in) throws IOException\n> public void registerScript(InputStream in, Map<String,String> params) throws IOException\n> public void registerScript(InputStream in, List<String> paramsFiles) throws IOException\n> public void registerScript(InputStream in, Map<String,String> params,List<String> paramsFiles) throws IOException \n> {code}\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-10-19T10:31:28Z"
  },
  "patches": [],
  "external_id": "PIG-1675"
},{
  "_id": {
    "$oid": "5bf68a8bf36e975afa4e51da"
  },
  "message_id": "<JIRA.12625510.1356824170540.75536.1357240572758@arcas>",
  "mailing_list_id": {
    "$oid": "5bf68976f36e975afa4e36af"
  },
  "reference_ids": [
    {
      "$oid": "5bf68a8bf36e975afa4e51d8"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bf68a8bf36e975afa4e51d8"
  },
  "from_id": {
    "$oid": "5bf68a2635e3ea2b7b1c9139"
  },
  "to_ids": [
    {
      "$oid": "5bf6898e35e3ea2b7b1be76d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (GIRAPH-459) Group Vertex Mutations by Partition\n ID",
  "body": "\n    [ https://issues.apache.org/jira/browse/GIRAPH-459?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13543204#comment-13543204 ] \n\nAlessandro Presta commented on GIRAPH-459:\n------------------------------------------\n\nIt would be way easier to review this on ReviewBoard, although I think we're still having issues after the switch to git.\n\nA few comments:\n- Please restore the original indentation of createMessageStoreFactory().\n- Just curious: why did you change the ordering, meaning you now process mutation requests first, then messages?\n- When checking for vertices that have received messages, you're loading each partition no matter what. I would first call getPartitionDestinationVertices(partitionId) and then, if it's not empty, load the partition and iterate.\n- Nothing to do with your patch, but can you also change \"prepareSuperstep\" to \"resolveMutations\" in the log message?\n- Can you run a quick benchmark (e.g., PageRankBenchmark with edge input) to make sure there are no performance regressions?\n\nOverall looks good!\n                \n> Group Vertex Mutations by Partition ID\n> --------------------------------------\n>\n>                 Key: GIRAPH-459\n>                 URL: https://issues.apache.org/jira/browse/GIRAPH-459\n>             Project: Giraph\n>          Issue Type: Improvement\n>          Components: graph\n>            Reporter: Claudio Martella\n>         Attachments: GIRAPH-459.patch\n>\n>\n> Currently, vertex mutations, and implicit creations of vertices based on messages to non-existing vertices, are executed randomly partition-wise. The iterated vertices can belong to different partitions. This is bad when we work out-of-core, as we need to load and unload the whole partition for each vertex. We should group these operations per-partition, and batch-execute them.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-01-03T19:16:12Z"
  },
  "patches": [],
  "external_id": "GIRAPH-459"
},{
  "_id": {
    "$oid": "5bdbffaf16772b6055c7c8dd"
  },
  "message_id": "<JIRA.13151567.1523430093000.104161.1534517520219@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdbffaf16772b6055c7c8dc"
    },
    {
      "$oid": "5bdbffaf16772b6055c7c8db"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdbffaf16772b6055c7c8db"
  },
  "from_id": {
    "$oid": "5baca97857674ee167cebe8c"
  },
  "to_ids": [
    {
      "$oid": "5bdbff5a35e3ea2b7bb7ff7b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (KNOX-1245) Failed to edit workflow from Ambari\n workflow Manager while accessing Ambari UI over Knox",
  "body": "\n     [ https://issues.apache.org/jira/browse/KNOX-1245?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nSandeep More updated KNOX-1245:\n-------------------------------\n    Fix Version/s: 1.2.0\n\n> Failed to edit workflow from Ambari workflow Manager while accessing Ambari UI over Knox\n> ----------------------------------------------------------------------------------------\n>\n>                 Key: KNOX-1245\n>                 URL: https://issues.apache.org/jira/browse/KNOX-1245\n>             Project: Apache Knox\n>          Issue Type: Bug\n>    Affects Versions: 0.14.0\n>            Reporter: Raghavender Rao Guruvannagari\n>            Priority: Major\n>             Fix For: 1.2.0\n>\n>\n> Ambari workflow manager view is accessed over knox, editing the workflow from the view will fail as knox fails to identify/parse the xml format.\n> Exception in knox logs\n> {code:java}\n> ==> gateway.log <==\n> 2018-03-28 10:16:25,542 ERROR hadoop.gateway (AbstractGatewayFilter.java:doFilter(63)) - Failed to execute filter: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('<' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n>  at [Source: java.io.InputStreamReader@254359d8; line: 1, column: 2]\n> 2018-03-28 10:16:25,542 ERROR hadoop.gateway (AbstractGatewayFilter.java:doFilter(63)) - Failed to execute filter: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('<' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n>  at [Source: java.io.InputStreamReader@254359d8; line: 1, column: 2]\n> 2018-03-28 10:16:25,542 ERROR hadoop.gateway (AbstractGatewayFilter.java:doFilter(63)) - Failed to execute filter: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('<' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n>  at [Source: java.io.InputStreamReader@254359d8; line: 1, column: 2]\n> 2018-03-28 10:16:25,543 ERROR hadoop.gateway (GatewayFilter.java:doFilter(141)) - Gateway processing failed: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('<' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n>  at [Source: java.io.InputStreamReader@254359d8; line: 1, column: 2]\n> com.fasterxml.jackson.core.JsonParseException: Unexpected character ('<' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n>  at [Source: java.io.InputStreamReader@254359d8; line: 1, column: 2]\n>  at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1369)\n>  at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:599)\n>  at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:520)\n>  at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleUnexpectedValue(ReaderBasedJsonParser.java:1379)\n>  at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:669)\n>  at org.apache.hadoop.gateway.filter.rewrite.impl.json.JsonFilterReader.read(JsonFilterReader.java:89)\n>  at org.apache.hadoop.gateway.filter.rewrite.impl.json.JsonUrlRewriteFilterReader.read(JsonUrlRewriteFilterReader.java:32)\n>  at org.apache.commons.io.input.ReaderInputStream.fillBuffer(ReaderInputStream.java:198)\n>  at org.apache.commons.io.input.ReaderInputStream.read(ReaderInputStream.java:242)\n> {code}\n> {code:java}\n> ==> gateway-audit.log <==\n> 18/03/28 10:16:25 ||6e7f841c-3aa9-4d8f-a71f-bfbc6138dedf|audit|10.200.4.250|AMBARI|anonymous|||access|uri|/gateway/ui/ambari/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/wfm/resources/proxy/readWorkflow?workflowPath=/user/admin/workflow.xml&jobType=WORKFLOW|failure|\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-08-17T14:52:00Z"
  },
  "patches": [],
  "external_id": "KNOX-1245"
},{
  "_id": {
    "$oid": "5bacb33cfaaadd76f8aa2a96"
  },
  "message_id": "<582725787.41330.1324602930962.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb2e0faaadd76f8aa1626"
  },
  "from_id": {
    "$oid": "5bacb30c57674ee167d90db7"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PIG-2326) Pig minicluster tests can not be run\n from eclipse",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-2326?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJulien Le Dem updated PIG-2326:\n-------------------------------\n\n    Attachment:     (was: PIG-2326_a.patch)\n    \n> Pig minicluster tests can not be run from eclipse\n> -------------------------------------------------\n>\n>                 Key: PIG-2326\n>                 URL: https://issues.apache.org/jira/browse/PIG-2326\n>             Project: Pig\n>          Issue Type: Bug\n>            Reporter: Julien Le Dem\n>            Assignee: Julien Le Dem\n>            Priority: Minor\n>         Attachments: PIG-2326.patch, PIG-2326_a.patch\n>\n>\n> Some of the setup of the minicluster tests are in the ant config hence they don't run in eclipse.\n> In particular:\n> System.setProperty(\"hadoop.log.dir\", \"build/test/logs\");\n> hadoop-site.xml in the classpath\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-12-23T01:15:30Z"
  },
  "patches": [],
  "external_id": "PIG-2326"
},{
  "_id": {
    "$oid": "5bacb320faaadd76f8aa245b"
  },
  "message_id": "<718435365.2833.1328108579893.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5bacb31557674ee167d91f90"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PIG-2500) dear all,",
  "body": "dear all,\n---------\n\n                 Key: PIG-2500\n                 URL: https://issues.apache.org/jira/browse/PIG-2500\n             Project: Pig\n          Issue Type: Bug\n            Reporter: vaibhav\n\n\ninput file :\n\ncat input13\n(3,8,9) {(3,8,9)} [open#apache]\n(1,4,7) {(1,4,7)} [apache#hadoop]\n(2,5,8) {(2,5,8)} [open#apache]\n\n\nA = LOAD '/data/input13' AS (T1:tuple(f1:int, f2:int), B:bag{T2:tuple(t1:float,t2:float)}, M:map[] );\n\noutput : \n\ndump A ; \n\n(3,),,)\n((1,),,)\n((2,),,)\n\nbut it should be the same as input? \n2)\n\n\ncat input15\n(3,8,9) (mary,19)\n(1,4,7) (john,18)\n(2,5,8) (joe,18)\n\n\no/p\n((3,8,9),)\n((1,4,7),)\n((2,5,8),)\n---------------------------------------\n\nfirst logs\n\n--------------------------------------------------------------------------------\n\ngrunt> A = LOAD '/data/input13' AS (T1:tuple(f1:int, f2:int), B:bag{T2:tuple(t1:float,t2:float)}, M:map[] );\ngrunt> dump A ;\n2012-02-01 20:22:14,025 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for A\n2012-02-01 20:22:14,025 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for A\n2012-02-01 20:22:14,032 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:14,034 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: Store(hdfs://localhost:54310/tmp/temp537168513/tmp899939258:org.apache.pig.builtin.BinStorage) - 1-246 Operator Key: 1-246)\n2012-02-01 20:22:14,035 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n2012-02-01 20:22:14,035 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n2012-02-01 20:22:14,040 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:14,040 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:14,040 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n2012-02-01 20:22:15,334 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n2012-02-01 20:22:15,335 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,336 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n2012-02-01 20:22:15,336 [Thread-149] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n2012-02-01 20:22:15,378 [Thread-149] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,382 [Thread-149] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,388 [Thread-149] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2012-02-01 20:22:15,388 [Thread-149] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n2012-02-01 20:22:15,425 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,427 [Thread-157] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2012-02-01 20:22:15,427 [Thread-157] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n2012-02-01 20:22:15,437 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,438 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,440 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,444 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,463 [Thread-157] INFO  org.apache.hadoop.mapred.TaskRunner - Task:attempt_local_0011_m_000000_0 is done. And is in the process of commiting\n2012-02-01 20:22:15,464 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,465 [Thread-157] INFO  org.apache.hadoop.mapred.LocalJobRunner -\n2012-02-01 20:22:15,465 [Thread-157] INFO  org.apache.hadoop.mapred.TaskRunner - Task attempt_local_0011_m_000000_0 is allowed to commit now\n2012-02-01 20:22:15,466 [Thread-157] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:15,471 [Thread-157] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local_0011_m_000000_0' to hdfs://localhost:54310/tmp/temp537168513/tmp899939258\n2012-02-01 20:22:15,471 [Thread-157] INFO  org.apache.hadoop.mapred.LocalJobRunner -\n2012-02-01 20:22:15,471 [Thread-157] INFO  org.apache.hadoop.mapred.TaskRunner - Task 'attempt_local_0011_m_000000_0' done.\n2012-02-01 20:22:15,837 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local_0011\n2012-02-01 20:22:15,837 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Successfully stored result in: \"hdfs://localhost:54310/tmp/temp537168513/tmp899939258\"\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Records written : 0\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Bytes written : 0\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Spillable Memory Manager spill count : 0\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Proactive spill count : 0\n2012-02-01 20:22:20,843 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n2012-02-01 20:22:20,855 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:22:20,859 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2012-02-01 20:22:20,859 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n((3,),,)\n((1,),,)\n((2,),,)\n\n\n----------------------------------------------------------------------\n\nsecond logs\n\n\ngrunt> D = LOAD '/data/input15'  AS (F:tuple(f1:int,f2:int,f3:int),T:tuple(t1:chararray,t2:int));\ngrunt> dump D ;\n2012-02-01 20:28:32,287 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for D\n2012-02-01 20:28:32,287 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for D\n2012-02-01 20:28:32,330 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=\n2012-02-01 20:28:32,399 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: Store(hdfs://localhost:54310/tmp/temp2086651143/tmp-1826566586:org.apache.pig.builtin.BinStorage) - 1-14 Operator Key: 1-14)\n2012-02-01 20:28:32,428 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n2012-02-01 20:28:32,428 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n2012-02-01 20:28:32,443 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:32,448 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:32,448 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n2012-02-01 20:28:33,845 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n2012-02-01 20:28:33,869 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:33,870 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n2012-02-01 20:28:33,873 [Thread-8] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n2012-02-01 20:28:33,968 [Thread-8] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:33,977 [Thread-8] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:33,985 [Thread-8] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2012-02-01 20:28:33,985 [Thread-8] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n2012-02-01 20:28:34,102 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,104 [Thread-17] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2012-02-01 20:28:34,105 [Thread-17] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n2012-02-01 20:28:34,136 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,145 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,149 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,153 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,179 [Thread-17] INFO  org.apache.hadoop.mapred.TaskRunner - Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting\n2012-02-01 20:28:34,181 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,183 [Thread-17] INFO  org.apache.hadoop.mapred.LocalJobRunner -\n2012-02-01 20:28:34,184 [Thread-17] INFO  org.apache.hadoop.mapred.TaskRunner - Task attempt_local_0001_m_000000_0 is allowed to commit now\n2012-02-01 20:28:34,185 [Thread-17] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:34,192 [Thread-17] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local_0001_m_000000_0' to hdfs://localhost:54310/tmp/temp2086651143/tmp-1826566586\n2012-02-01 20:28:34,193 [Thread-17] INFO  org.apache.hadoop.mapred.LocalJobRunner -\n2012-02-01 20:28:34,193 [Thread-17] INFO  org.apache.hadoop.mapred.TaskRunner - Task 'attempt_local_0001_m_000000_0' done.\n2012-02-01 20:28:34,371 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local_0001\n2012-02-01 20:28:34,372 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n2012-02-01 20:28:39,379 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n2012-02-01 20:28:39,379 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Successfully stored result in: \"hdfs://localhost:54310/tmp/temp2086651143/tmp-1826566586\"\n2012-02-01 20:28:39,380 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Records written : 0\n2012-02-01 20:28:39,380 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Bytes written : 0\n2012-02-01 20:28:39,381 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Spillable Memory Manager spill count : 0\n2012-02-01 20:28:39,381 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Proactive spill count : 0\n2012-02-01 20:28:39,381 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n2012-02-01 20:28:39,394 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2012-02-01 20:28:39,400 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2012-02-01 20:28:39,400 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n((3,8,9),)\n((1,4,7),)\n((2,5,8),)\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-02-01T15:02:59Z"
  },
  "patches": [],
  "external_id": "PIG-2500"
},{
  "_id": {
    "$oid": "5f27cd63442ab9b9860ed9df"
  },
  "message_id": "<JIRA.13157857.1525775332000.96419.1526554020121@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27cd5f442ab9b9860ed8d2"
    },
    {
      "$oid": "5f27cd5f442ab9b9860ed8d3"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cd5f442ab9b9860ed8d2"
  },
  "from_id": {
    "$oid": "5f27cd0caf02e2d6de7a322a"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (OOZIE-3241) Flaky test\n TestRecoveryService#testCoordActionRecoveryServiceForWaitingRegisterPartition",
  "body": "\n     [ https://issues.apache.org/jira/browse/OOZIE-3241?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nPeter Bacsko resolved OOZIE-3241.\n---------------------------------\n       Resolution: Fixed\n    Fix Version/s: 5.1.0\n\n> Flaky test TestRecoveryService#testCoordActionRecoveryServiceForWaitingRegisterPartition\n> ----------------------------------------------------------------------------------------\n>\n>                 Key: OOZIE-3241\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-3241\n>             Project: Oozie\n>          Issue Type: Sub-task\n>            Reporter: Peter Bacsko\n>            Assignee: Peter Bacsko\n>            Priority: Major\n>             Fix For: 5.1.0\n>\n>         Attachments: OOZIE-3241.txt\n>\n>\n> The test TestRecoveryService#testCoordActionRecoveryServiceForWaitingRegisterPartition occasionally fails with the following error:\n> {noformat}\n> junit.framework.AssertionFailedError\n> \tat org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForWaitingRegisterPartition(TestRecoveryService.java:506)\n> {noformat}\n> Errors in the test:\n> {noformat}\n> [jndiProperties=java.naming.factory.initial#org.apache.activemq.jndi.ActiveMQInitialContextFactory;java.naming.provider.url#vm://localhost?broker.persistent=falseconnectionFactoryNames#ConnectionFactory]]\n> javax.jms.JMSException: Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost\n> \tat org.apache.activemq.util.JMSExceptionSupport.create(JMSExceptionSupport.java:36)\n> \tat org.apache.activemq.ActiveMQConnectionFactory.createTransport(ActiveMQConnectionFactory.java:332)\n> \tat org.apache.activemq.ActiveMQConnectionFactory.createActiveMQConnection(ActiveMQConnectionFactory.java:345)\n> \tat org.apache.activemq.ActiveMQConnectionFactory.createActiveMQConnection(ActiveMQConnectionFactory.java:303)\n> \tat org.apache.activemq.ActiveMQConnectionFactory.createConnection(ActiveMQConnectionFactory.java:243)\n> \tat org.apache.oozie.jms.DefaultConnectionContext.createConnection(DefaultConnectionContext.java:53)\n> \tat org.apache.oozie.service.JMSAccessorService.createConnectionContext(JMSAccessorService.java:264)\n> \tat org.apache.oozie.service.JMSAccessorService.registerForNotification(JMSAccessorService.java:110)\n> \tat org.apache.oozie.service.HCatAccessorService.registerForNotification(HCatAccessorService.java:236)\n> \tat org.apache.oozie.dependency.HCatURIHandler.registerForNotification(HCatURIHandler.java:119)\n> \tat org.apache.oozie.command.coord.CoordPushDependencyCheckXCommand.registerForNotification(CoordPushDependencyCheckXCommand.java:314)\n> \tat org.apache.oozie.command.coord.CoordPushDependencyCheckXCommand.execute(CoordPushDependencyCheckXCommand.java:174)\n> \tat org.apache.oozie.command.coord.CoordPushDependencyCheckXCommand.execute(CoordPushDependencyCheckXCommand.java:62)\n> \tat org.apache.oozie.command.XCommand.call(XCommand.java:290)\n> \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n> \tat org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:181)\n> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n> \tat java.lang.Thread.run(Thread.java:748)\n> Caused by: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost\n> \tat com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)\n> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)\n> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)\n> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)\n> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)\n> \tat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)\n> \tat org.apache.activemq.broker.jmx.ManagementContext.registerMBean(ManagementContext.java:408)\n> \tat org.apache.activemq.broker.jmx.AnnotatedMBean.registerMBean(AnnotatedMBean.java:72)\n> \tat org.apache.activemq.broker.BrokerService.startManagementContext(BrokerService.java:2581)\n> \tat org.apache.activemq.broker.BrokerService.start(BrokerService.java:606)\n> \tat org.apache.activemq.transport.vm.VMTransportFactory.doCompositeConnect(VMTransportFactory.java:127)\n> \tat org.apache.activemq.transport.vm.VMTransportFactory.doConnect(VMTransportFactory.java:56)\n> \tat org.apache.activemq.transport.TransportFactory.connect(TransportFactory.java:69)\n> \tat org.apache.activemq.ActiveMQConnectionFactory.createTransport(ActiveMQConnectionFactory.java:330)\n> \t... 17 more\n> {noformat}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-05-17T10:47:00Z"
  },
  "patches": [],
  "external_id": "OOZIE-3241"
},{
  "_id": {
    "$oid": "5bea9af29e73d744d4123ffd"
  },
  "message_id": "<20337309.81151276877486381.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9abd9e73d744d4123556"
  },
  "from_id": {
    "$oid": "5bea97f835e3ea2b7b4e2a04"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-4677) SYSCS_COMPRESS_TABLE disables unique\n constraints",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-4677?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMamta A. Satoor updated DERBY-4677:\n-----------------------------------\n\n    Fix Version/s: 10.4.2.1\n\n> SYSCS_COMPRESS_TABLE disables unique constraints\n> ------------------------------------------------\n>\n>                 Key: DERBY-4677\n>                 URL: https://issues.apache.org/jira/browse/DERBY-4677\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: SQL\n>    Affects Versions: 10.4.1.3, 10.4.2.0, 10.4.2.1, 10.5.1.1, 10.5.2.0, 10.5.3.0, 10.6.1.0\n>         Environment: Output of sysinfo:\n> ------------------ Java Information ------------------\n> Java Version:    1.6.0_20\n> Java Vendor:     Sun Microsystems Inc.\n> Java home:       C:\\Program Files (x86)\\Java\\jre6\n> Java classpath:  .;C:\\Program Files (x86)\\Java\\jre6\\lib\\ext\\QTJava.zip;C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\bin\\../lib/derby.jar;C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\bin\\../lib/derbynet.jar;C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\bin\\../lib/derbyclient.jar;C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\bin\\../lib/derbytools.jar\n> OS name:         Windows 7\n> OS architecture: x86\n> OS version:      6.1\n> Java user name:  bmason\n> Java user home:  C:\\Users\\BMASON\n> Java user dir:   C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\bin\n> java.specification.name: Java Platform API Specification\n> java.specification.version: 1.6\n> java.runtime.version: 1.6.0_20-b02\n> --------- Derby Information --------\n> JRE - JDBC: Java SE 6 - JDBC 4.0\n> [C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\lib\\derby.jar] 10.6.1.0 - (938214)\n> [C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\lib\\derbytools.jar] 10.6.1.0 - (938214)\n> [C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\lib\\derbynet.jar] 10.6.1.0 - (938214)\n> [C:\\Users\\BMASON\\Sandbox\\libs\\db-derby-10.6.1.0-bin\\lib\\derbyclient.jar] 10.6.1.0 - (938214)\n> ------------------------------------------------------\n> ----------------- Locale Information -----------------\n> Current Locale :  [English/New Zealand [en_NZ]]\n> Found support for locale: [cs]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [de_DE]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [es]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [fr]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [hu]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [it]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [ja_JP]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [ko_KR]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [pl]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [pt_BR]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [ru]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [zh_CN]\n> \t version: 10.6.1.0 - (938214)\n> Found support for locale: [zh_TW]\n> \t version: 10.6.1.0 - (938214)\n> ------------------------------------------------------\n>            Reporter: Brett Mason\n>            Assignee: Mamta A. Satoor\n>             Fix For: 10.4.2.1, 10.5.3.1, 10.6.1.1, 10.7.0.0\n>\n>         Attachments: DERBY4677_diff_patch1.txt, DERBY4677_stat_patch1.txt, releaseNote.html\n>\n>\n> It appears that running SYSCS_UTIL.SYSCS_COMPRESS_TABLE on a table with a null-able unique constraint will disable the unique constraint. The script\n> below should reproduce the problem. The expected behaviour is for the second insert to fail due to the unique constraint but instead it is allowed. The second insert will fail as expected if either the call to SYSCS_COMPRESS_TABLE is skipped or if the column is declared NOT NULL.\n> I have reproduced the problem using embedded Derby 10.5.1.1, 10.5.3.0 and 10.6.1.0 using ij.\n> CREATE TABLE TABLE1(NAME1 INT UNIQUE);\n> CALL SYSCS_UTIL.SYSCS_COMPRESS_TABLE('APP', 'TABLE1', 1);\n> INSERT INTO TABLE1(NAME1) VALUES(1);\n> INSERT INTO TABLE1(NAME1) VALUES(1);\n> SELECT * FROM TABLE1;\n> DROP TABLE TABLE1;\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-06-18T12:11:26Z"
  },
  "patches": [],
  "external_id": "DERBY-4677"
},{
  "_id": {
    "$oid": "5bbf0bb635bc96443481a499"
  },
  "message_id": "<JIRA.12955564.1459661202000.120419.1459673425495@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf090e35bc964434814f5a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0b9935bc96443481a072"
    },
    {
      "$oid": "5bbf0b9935bc96443481a073"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0b9935bc96443481a072"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdff0"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (STORM-1678) abstract batch processing to common\n api `BatchHelper`",
  "body": "\n    [ https://issues.apache.org/jira/browse/STORM-1678?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15223172#comment-15223172 ] \n\nASF GitHub Bot commented on STORM-1678:\n---------------------------------------\n\nGitHub user vesense opened a pull request:\n\n    https://github.com/apache/storm/pull/1304\n\n    STORM-1678 (1.x): abstract batch processing to common api `BatchHelper`\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/vesense/storm STORM-1678-1.x\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/1304.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #1304\n    \n----\ncommit 320950d9f2810dd8e97a0b05cc21ce51b177bb8b\nAuthor: Xin Wang <best.wangxin@163.com>\nDate:   2016-04-03T06:31:55Z\n\n    STORM-1678: abstract batch processing to common api\n\n----\n\n\n> abstract batch processing to common api `BatchHelper`\n> -----------------------------------------------------\n>\n>                 Key: STORM-1678\n>                 URL: https://issues.apache.org/jira/browse/STORM-1678\n>             Project: Apache Storm\n>          Issue Type: Improvement\n>          Components: storm-core, storm-hbase, storm-hive, storm-mongodb\n>            Reporter: Xin Wang\n>            Assignee: Xin Wang\n>\n> Some external projects(`storm-hbase`, `storm-hive`, `storm-mongodb`) have the same batch processing logic.\n> Abstracting for that could make code simpler and cleaner.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-04-03T08:50:25Z"
  },
  "patches": [],
  "external_id": "STORM-1678"
},{
  "_id": {
    "$oid": "5bc857096e373d4fe81c43cc"
  },
  "message_id": "<JIRA.12687984.1389231760976.10764.1389821839306@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [
    {
      "$oid": "5bc856fa6e373d4fe81c4389"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc856fa6e373d4fe81c4389"
  },
  "from_id": {
    "$oid": "58cb76b102ca40f8bf703fe7"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1da"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FLUME-2294) Add a sink for Kite Datasets",
  "body": "\n     [ https://issues.apache.org/jira/browse/FLUME-2294?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRyan Blue updated FLUME-2294:\n-----------------------------\n\n    Attachment: 0001-FLUME-2294-Add-sink-for-Kite-Datasets.patch\n\n> Add a sink for Kite Datasets\n> ----------------------------\n>\n>                 Key: FLUME-2294\n>                 URL: https://issues.apache.org/jira/browse/FLUME-2294\n>             Project: Flume\n>          Issue Type: New Feature\n>          Components: Sinks+Sources\n>    Affects Versions: v1.4.0\n>            Reporter: Ryan Blue\n>         Attachments: 0001-FLUME-2294-Add-sink-for-Kite-Datasets.patch\n>\n>\n> I'd like to add a flume sink for Kite (kitesdk.org) Datasets. This is an API for working with data in Hadoop, which can be backed by partitioned HDFS directories (with support that syncs with Hive) and HBase tables. This sink will depend on the Kite library and use it to write.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-01-15T21:37:19Z"
  },
  "patches": [],
  "external_id": "FLUME-2294"
},{
  "_id": {
    "$oid": "58bfc97aaea7c7604a49ebe3"
  },
  "message_id": "<1011481863.1221485624567.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "58bfc8a9aea7c7604a49c2e7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "58bfc974aea7c7604a49ea8f"
  },
  "from_id": {
    "$oid": "58bfc94602ca40f8bf147998"
  },
  "to_ids": [
    {
      "$oid": "58bfc8c002ca40f8bf147881"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (ZOOKEEPER-63) Race condition in client close()\n operation",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-63?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12631021#action_12631021 ] \n\nStu Hood commented on ZOOKEEPER-63:\n-----------------------------------\n\nGreat work Patrick! Thanks a bunch for looking into it.\n\n> Race condition in client close() operation\n> ------------------------------------------\n>\n>                 Key: ZOOKEEPER-63\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-63\n>             Project: Zookeeper\n>          Issue Type: Bug\n>          Components: java client\n>            Reporter: Patrick Hunt\n>            Assignee: Patrick Hunt\n>         Attachments: client-test-fail.diff, ZOOKEEPER-63.patch\n>\n>\n> There is a race condition in the java close operation on ZooKeeper.java.\n> Client is sending a disconnect request to the server. Server will close any open connections with the client when it receives this. If the client has not yet shutdown it's subthreads (event/send threads for example) these threads may consider the condition an error. We see this alot in the tests where the clients output error logs because they are unaware that a disconnection has been requested by the client.\n> Ben mentioned: perhaps we just have to change state to closed (on client) before sending disconnect request.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-09-15T06:33:44Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-63"
},{
  "_id": {
    "$oid": "5c580562e078b00ec4e74e4a"
  },
  "message_id": "<JIRA.13060753.1490983898000.185049.1491083681543@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c580562e078b00ec4e74e49"
    },
    {
      "$oid": "5c580562e078b00ec4e74e48"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c580562e078b00ec4e74e48"
  },
  "from_id": {
    "$oid": "5baca96d57674ee167cea40b"
  },
  "to_ids": [
    {
      "$oid": "5bbdf31357674ee1677dbf18"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (RANGER-1489) Solr plugin fails to get client\n address",
  "body": "\n    [ https://issues.apache.org/jira/browse/RANGER-1489?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15952436#comment-15952436 ] \n\nJeffrey E  Rodriguez commented on RANGER-1489:\n----------------------------------------------\n\ncontext.getRemoteAddr returns the Internet Protocol (IP) address of the client or last proxy that sent the request. If Solr is going through a balancer it may return the IP of the balancer instead of the client making the request.\nSo if there are no proxy's in between client and server they should be equivalent.\n\n> Solr plugin fails to get client address\n> ---------------------------------------\n>\n>                 Key: RANGER-1489\n>                 URL: https://issues.apache.org/jira/browse/RANGER-1489\n>             Project: Ranger\n>          Issue Type: Bug\n>          Components: plugins\n>    Affects Versions: 0.6.3, 1.0.0\n>            Reporter: Yan\n>            Assignee: Yan\n>            Priority: Minor\n>\n> A immediate consequence is that IP-range conditions all fail for Solr authorizations.\n> context.getHttpHeader(\"REMOTE_ADDR\") is used; instead context.getRemoteAddr() is more appropriate.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-04-01T21:54:41Z"
  },
  "patches": [],
  "external_id": "RANGER-1489"
},{
  "_id": {
    "$oid": "5bbf0e91b79d666cbb22ebf8"
  },
  "message_id": "<JIRA.12762628.1418923145000.89056.1419306913802@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0e37b79d666cbb22df0d"
    },
    {
      "$oid": "5bbf0e37b79d666cbb22df0e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0e37b79d666cbb22df0d"
  },
  "from_id": {
    "$oid": "5bbf0c4757674ee1673b7f10"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-1824) in ConsoleProducer - properties\n key.separator and parse.key no longer work",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-1824?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14256569#comment-14256569 ] \n\nJoe Stein commented on KAFKA-1824:\n----------------------------------\n\nShouldn't this go into 0.8.2 branch also since it is a fix for a regression bug?\n\n> in ConsoleProducer - properties key.separator and parse.key no longer work\n> --------------------------------------------------------------------------\n>\n>                 Key: KAFKA-1824\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-1824\n>             Project: Kafka\n>          Issue Type: Bug\n>            Reporter: Gwen Shapira\n>            Assignee: Gwen Shapira\n>             Fix For: 0.8.3\n>\n>         Attachments: KAFKA-1824.patch, KAFKA-1824.patch, KAFKA-1824_2014-12-22_16:17:42.patch\n>\n>\n> Looks like the change in kafka-1711 breaks them accidentally.\n> reader.init is called with readerProps which is initialized with commandline properties as defaults.\n> the problem is that reader.init checks:\n>     if(props.containsKey(\"parse.key\"))\n> and defaults don't return true in this case.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-12-23T03:55:13Z"
  },
  "patches": [],
  "external_id": "KAFKA-1824"
},{
  "_id": {
    "$oid": "5bbf28f830623e2888adc780"
  },
  "message_id": "<JIRA.13025514.1480919060000.623980.1483364518368@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf28f830623e2888adc77c"
    },
    {
      "$oid": "5bbf28f830623e2888adc77b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf28f830623e2888adc77b"
  },
  "from_id": {
    "$oid": "58bfc8c302ca40f8bf14789e"
  },
  "to_ids": [
    {
      "$oid": "5bbf261257674ee1674f913a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (LENS-1379) Session expiry is not working\n properly",
  "body": "\n    [ https://issues.apache.org/jira/browse/LENS-1379?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15792884#comment-15792884 ] \n\nHudson commented on LENS-1379:\n------------------------------\n\nUNSTABLE: Integrated in Jenkins build Lens-Commit #1378 (See [https://builds.apache.org/job/Lens-Commit/1378/])\nLENS-1379 : Fix session expiry for sessions in which operations were (puneet.gupta: rev 98990c39f4f4826beaf59afb0ef9961f566000c3)\n* (edit) lens-server/src/main/java/org/apache/lens/server/session/LensSessionImpl.java\n* (edit) lens-server/src/main/java/org/apache/lens/server/session/HiveSessionService.java\n* (edit) lens-server/src/test/java/org/apache/lens/server/query/TestQueryIndependenceFromSessionClose.java\n\n\n> Session expiry is not working properly\n> --------------------------------------\n>\n>                 Key: LENS-1379\n>                 URL: https://issues.apache.org/jira/browse/LENS-1379\n>             Project: Apache Lens\n>          Issue Type: Bug\n>          Components: server\n>    Affects Versions: 2.6\n>            Reporter: Amareshwari Sriramadasu\n>            Assignee: Amareshwari Sriramadasu\n>             Fix For: 2.7\n>\n>         Attachments: LENS-1379.2.patch\n>\n>\n> We are seeing session expiry is not working properly and found two reasons for the same :\n> * isActive method logic is not marking expired sessions as inactive.\n> * Upon restart lastAccessTime is getting reset to restore time.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2017-01-02T13:41:58Z"
  },
  "patches": [],
  "external_id": "LENS-1379"
},{
  "_id": {
    "$oid": "5bbe11c1b1ffc5570d042d73"
  },
  "message_id": "<20188531.4101270926762368.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bbe0e4eb1ffc5570d03b7b4"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbe11c1b1ffc5570d042d72"
  },
  "from_id": {
    "$oid": "59bf9347f2a4565fe9e90595"
  },
  "to_ids": [
    {
      "$oid": "59bfb4a8f2a4565fe9228011"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (TIKA-375) Improve code quality metrics",
  "body": "\n    [ https://issues.apache.org/jira/browse/TIKA-375?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12855632#action_12855632 ] \n\nJeroen Reijn commented on TIKA-375:\n-----------------------------------\n\nI would like to help out with some of these issues. I wonder if the tika project team of lucene team have a checkstyle or coding guide for writing code. Is your Sonar instance already taking this into account or is there a resource out there available where I can read this? \n\n> Improve code quality metrics\n> ----------------------------\n>\n>                 Key: TIKA-375\n>                 URL: https://issues.apache.org/jira/browse/TIKA-375\n>             Project: Tika\n>          Issue Type: Improvement\n>          Components: general\n>            Reporter: Jukka Zitting\n>\n> The Sonar report at http://sonar.zitting.name/project/index/3338 highlights a number of code quality issues that we could fix with fairly little effort and risk.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: https://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2010-04-10T15:12:42Z"
  },
  "patches": [],
  "external_id": "TIKA-375"
},{
  "_id": {
    "$oid": "5bbdacbfc764eb6c7a264e41"
  },
  "message_id": "<JIRA.12758907.1417531867000.58929.1417534093846@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdacbfc764eb6c7a264e3b"
    },
    {
      "$oid": "5bbdacbfc764eb6c7a264e3a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdacbfc764eb6c7a264e3a"
  },
  "from_id": {
    "$oid": "5bbdac1c57674ee167da556a"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FALCON-927) Refactoring of entity helpers in\n falcon-regression",
  "body": "\n     [ https://issues.apache.org/jira/browse/FALCON-927?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRuslan Ostafiychuk updated FALCON-927:\n--------------------------------------\n    Attachment: FALCON-927.patch\n\n> Refactoring of entity helpers in falcon-regression\n> --------------------------------------------------\n>\n>                 Key: FALCON-927\n>                 URL: https://issues.apache.org/jira/browse/FALCON-927\n>             Project: Falcon\n>          Issue Type: Sub-task\n>          Components: merlin\n>            Reporter: Ruslan Ostafiychuk\n>            Assignee: Ruslan Ostafiychuk\n>         Attachments: FALCON-927.patch\n>\n>\n> Since IEntityManagerHelper is not interface and other classes in _interfaces_ package too we can move them to helpers package and rename.\n> org/apache/falcon/regression/core/(*interfaces* → *helpers*)/FalconClientBuilder.java\n> org/apache/falcon/regression/core/(*interfaces/IEntityManagerHelper*.java → *helpers/entity/AbstractEntityHelper*.java)\n> org/apache/falcon/regression/core/helpers/(*ClusterEntityHelperImpl*.java → *entity/ClusterEntityHelper*.java)\n> org/apache/falcon/regression/core/helpers/(*DataEntityHelperImpl*.java → *entity/DataEntityHelper*.java)\n> org/apache/falcon/regression/core/(*interfaces* → *helpers/entity*)/EntityHelperFactory.java\n> org/apache/falcon/regression/core/helpers/(*ProcessEntityHelperImpl*.java → *entity/ProcessEntityHelper*.java)\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-12-02T15:28:13Z"
  },
  "patches": [],
  "external_id": "FALCON-927"
},{
  "_id": {
    "$oid": "5bbf2ea230623e2888ae06a8"
  },
  "message_id": "<JIRA.12829134.1431421645000.140029.1431926280234@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf2dcb30623e2888adfab9"
    },
    {
      "$oid": "5bbf2dcb30623e2888adfaba"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf2dcb30623e2888adfab9"
  },
  "from_id": {
    "$oid": "5bacb1be57674ee167d69603"
  },
  "to_ids": [
    {
      "$oid": "5bbf2ca057674ee16751d09e"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (LENS-551) Cleanup testcases of exception stack\n traces",
  "body": "\n     [ https://issues.apache.org/jira/browse/LENS-551?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nYash Sharma updated LENS-551:\n-----------------------------\n    Status: Patch Available  (was: Open)\n\n> Cleanup testcases of exception stack traces\n> -------------------------------------------\n>\n>                 Key: LENS-551\n>                 URL: https://issues.apache.org/jira/browse/LENS-551\n>             Project: Apache Lens\n>          Issue Type: Sub-task\n>            Reporter: Yash Sharma\n>            Assignee: Yash Sharma\n>            Priority: Minor\n>             Fix For: 2.2\n>\n>         Attachments: LENS-551.02.patch\n>\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-05-18T05:18:00Z"
  },
  "patches": [],
  "external_id": "LENS-551"
},{
  "_id": {
    "$oid": "60fac3bed907ab79037eae88"
  },
  "message_id": "<JIRA.29854.1107370359000.88793.1393139900062@arcas>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac3b9d907ab79037eace2"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac3b9d907ab79037eace2"
  },
  "from_id": {
    "$oid": "5f27c246af02e2d6de6f10ee"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (DIRSERVER-1246) Add shell scripts and batch\n files for clients",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSERVER-1246?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKiran Ayyagari resolved DIRSERVER-1246.\n---------------------------------------\n\n    Resolution: Won't Fix\n\n> Add shell scripts and batch files for clients\n> ---------------------------------------------\n>\n>                 Key: DIRSERVER-1246\n>                 URL: https://issues.apache.org/jira/browse/DIRSERVER-1246\n>             Project: Directory ApacheDS\n>          Issue Type: New Feature\n>    Affects Versions: 1.5.4\n>            Reporter: Emmanuel Lecharny\n>            Assignee: Alex Karasulu\n>            Priority: Minor\n>             Fix For: 2.0.0-RC1\n>\n>\n> Create some shell scripts and batch files/exe files to encapsulate ldap clients\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-23T07:18:20Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-1246"
},{
  "_id": {
    "$oid": "5bc84fd257a11257de55ec32"
  },
  "message_id": "<JIRA.13187810.1538029154000.183868.1538076960761@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc84fd157a11257de55ec20"
    },
    {
      "$oid": "5bc84fd157a11257de55ec1f"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc84fd157a11257de55ec1f"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-4325) Validation fails with\n ClassCastException",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-4325?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16630949#comment-16630949 ] \n\nTilman Hausherr commented on PDFBOX-4325:\n-----------------------------------------\n\nThis partly fixed the bug, or at least avoids the CCE for your case. However now you get a false negative:\n{noformat}\n7.3 : Error on MetaData, Cannot find a definition for the namespace http://www.aiim.org/pdfua/ns/id/{noformat}\nThis is not true. the file is PDF/A. The problem is that xmpbox can't work with new Schemas. I don't know how to fix that, and I don't know if this will be fixed soon. In the meantime please use VeraPDF https://github.com/verapdf .\n\n> Validation fails with ClassCastException\n> ----------------------------------------\n>\n>                 Key: PDFBOX-4325\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-4325\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Preflight\n>    Affects Versions: 2.0.11\n>            Reporter: Alonso Gonzalez\n>            Priority: Major\n>         Attachments: CreatePdf.java, pdfa1b-itext.pdf\n>\n>\n> We have to validate PDFs that were created using iText5. But the validation fails with a ClassCastException:\n> {noformat}\n> Exception in thread \"main\" java.lang.ClassCastException: class org.apache.xmpbox.type.PDFAPropertyType\n> \tat java.lang.Class.asSubclass(Class.java:3404)\n> \tat org.apache.xmpbox.type.TypeMapping.instanciateSimpleProperty(TypeMapping.java:180)\n> \tat org.apache.xmpbox.xml.DomXmpParser.parseLiElement(DomXmpParser.java:521)\n> \tat org.apache.xmpbox.xml.DomXmpParser.parseLiDescription(DomXmpParser.java:603)\n> \tat org.apache.xmpbox.xml.DomXmpParser.parseLiElement(DomXmpParser.java:514)\n> \tat org.apache.xmpbox.xml.DomXmpParser.manageArray(DomXmpParser.java:465)\n> \tat org.apache.xmpbox.xml.DomXmpParser.createProperty(DomXmpParser.java:344)\n> \tat org.apache.xmpbox.xml.DomXmpParser.parseChildrenAsProperties(DomXmpParser.java:311)\n> \tat org.apache.xmpbox.xml.DomXmpParser.parseDescriptionRoot(DomXmpParser.java:240)\n> \tat org.apache.xmpbox.xml.DomXmpParser.parse(DomXmpParser.java:192)\n> \tat org.apache.pdfbox.preflight.process.MetadataValidationProcess.validate(MetadataValidationProcess.java:69)\n> \tat org.apache.pdfbox.preflight.utils.ContextHelper.callValidation(ContextHelper.java:84)\n> \tat org.apache.pdfbox.preflight.utils.ContextHelper.validateElement(ContextHelper.java:122)\n> \tat org.apache.pdfbox.preflight.PreflightDocument.validate(PreflightDocument.java:166)\n> \tat Main.main(Main.java:40)\n> \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n> \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n> \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n> \tat java.lang.reflect.Method.invoke(Method.java:498)\n> \tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)\n> {noformat}\n> I've attached a minimal example PDF that triggers the error.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2018-09-27T19:36:00Z"
  },
  "patches": [],
  "external_id": "PDFBOX-4325"
},{
  "_id": {
    "$oid": "5f27d46746816ce7cf50a7a7"
  },
  "message_id": "<19881315.1087800377437.JavaMail.orion@beaver.codehaus.org>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d46746816ce7cf50a7a6"
  },
  "from_id": {
    "$oid": "58c123f102ca40f8bfb1fc2a"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (MPHIBERNATE-5) maven.hibernate.delimiter isn't documented",
  "body": "The following comment has been added to this issue:\n\n     Author: Dan Washusen\n    Created: Mon, 21 Jun 2004 2:44 AM\n       Body:\nMaybe a defult value of \";\" could be used?  The current default value of \"\" causes problems with PostgreSQL.\n---------------------------------------------------------------------\nView this comment:\n  http://jira.codehaus.org/browse/MPHIBERNATE-5?page=comments#action_20878\n\n---------------------------------------------------------------------\nView the issue:\n  http://jira.codehaus.org/browse/MPHIBERNATE-5\n\nHere is an overview of the issue:\n---------------------------------------------------------------------\n        Key: MPHIBERNATE-5\n    Summary: maven.hibernate.delimiter isn't documented\n       Type: Bug\n\n     Status: Unassigned\n   Priority: Minor\n\n Original Estimate: Unknown\n Time Spent: Unknown\n  Remaining: Unknown\n\n    Project: maven-hibernate-plugin\n\n   Assignee: \n   Reporter: Dan Washusen\n\n    Created: Mon, 21 Jun 2004 2:40 AM\n    Updated: Mon, 21 Jun 2004 2:44 AM\n\nDescription:\nmaven.hibernate.delimiter isn't documented on the http://maven.apache.org/reference/plugins/hibernate/properties.html page.\n\n\n---------------------------------------------------------------------\nJIRA INFORMATION:\nThis message is automatically generated by JIRA.\n\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n\nIf you want more information on JIRA, or have a bug to report see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2004-06-21T02:46:17Z"
  },
  "patches": [],
  "external_id": "MPHIBERNATE-5"
},{
  "_id": {
    "$oid": "5bc868cf57a11257de569468"
  },
  "message_id": "<JIRA.12631214.1360224207982.14044.1408178959215@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8681c57a11257de568d19"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8681c57a11257de568d19"
  },
  "from_id": {
    "$oid": "58c9de5702ca40f8bf20c1ae"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-1511) pdfMerger App produces Garbage",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-1511?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14099570#comment-14099570 ] \n\nASF subversion and git services commented on PDFBOX-1511:\n---------------------------------------------------------\n\nCommit 1618314 from [~tilman] in branch 'pdfbox/trunk'\n[ https://svn.apache.org/r1618314 ]\n\nPDFBOX-1511: Test for PDFMergerUtility, thanks Maruan Sahyoun for the PDF test files\n\n> pdfMerger App produces Garbage\n> ------------------------------\n>\n>                 Key: PDFBOX-1511\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1511\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Utilities\n>    Affects Versions: 1.7.1\n>         Environment: Win XP; Windows Server 2008 R2; java version \"1.6.0_21\", \n>            Reporter: Michael Huber\n>             Fix For: 1.8.7, 2.0.0\n>\n>         Attachments: 078117u1.pdf, 078117u2.pdf, 078118.pdf, 1.pdf, 2.pdf, PDFBox.GlobalResourceMergeTest.Doc01.decoded.pdf, PDFBox.GlobalResourceMergeTest.Doc02.decoded.pdf, PDFBox.GlobalResourceMergeTest.Merged-1.8.6.pdf, PDFBox.GlobalResourceMergeTest.Merged-1.8.7.pdf, PDFMergerUtility.java, PDFMergerUtility.java.diff, PdfRenderer.java, targetPdfMergeJava.pdf, targetPdfMergeUtilityApp.pdf\n>\n>\n> pdfbox Utility pdfMerger produces a merged document containing garbage. All merged pdf files are contained but Strings are destroyed.\n> The source pdf files are created with graphviz and are readable without error or disturbance both with Acrobat X and pdfbox pdfDebug Utility.\n> Another astounding thing is that a handcoded merger using pdfMergerUtility class works fine when run within Eclipse Juno and creates same garbage when run from cmd line (pls. see attached source PdfRenderer.java)\n> I checked everything that comes in mind to find the differences, e.g. Java version, encoding/codepage issues, memory settings, found nothing.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-08-16T08:49:19Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1511"
},{
  "_id": {
    "$oid": "5bacb3e7faaadd76f8aa518d"
  },
  "message_id": "<33181799.30831272952855925.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb3e7faaadd76f8aa5169"
  },
  "from_id": {
    "$oid": "5bacb35c57674ee167d9a803"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (PIG-1401) \"explain -script <script file>\" executes\n grunt commands like run/dump/copy etc - explain -script should not execute\n any grunt command and only explain the query plans.",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-1401?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nPradeep Kamath updated PIG-1401:\n--------------------------------\n\n    Status: Open  (was: Patch Available)\n\n> \"explain -script <script file>\" executes grunt commands like run/dump/copy etc - explain -script should not execute any grunt command and only explain the query plans.\n> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: PIG-1401\n>                 URL: https://issues.apache.org/jira/browse/PIG-1401\n>             Project: Pig\n>          Issue Type: Bug\n>    Affects Versions: 0.6.0, 0.7.0\n>            Reporter: Pradeep Kamath\n>            Assignee: Pradeep Kamath\n>             Fix For: 0.8.0\n>\n>         Attachments: PIG-1401-2.patch, PIG-1401.patch\n>\n>\n> \"explain -script <script file>\" executes grunt commands like run/dump/copy etc - explain -script should not execute any grunt command and only explain the query plans.\n> Note: \"explain <alias>\" statement in the script will still cause all grunt commands upto the explain to be executed. This issue only fixes the behavior of \"explain -script <script file>\" wherein any grunt commands like \"run\", \"dump\", \"copy\", \"fs ..\" present in the supplied <script file> will need to be ignored.\n> This should be documented in the release in which this jira will be resolved.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-05-04T02:00:55Z"
  },
  "patches": [],
  "external_id": "PIG-1401"
},{
  "_id": {
    "$oid": "5f27b9c9641061285052ceb7"
  },
  "message_id": "<JIRA.12745888.1412396006000.261763.1413280353711@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b9c5641061285052cd2f"
    },
    {
      "$oid": "5f27b9c5641061285052cd30"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b9c5641061285052cd2f"
  },
  "from_id": {
    "$oid": "5f27b97faf02e2d6de561762"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PHOENIX-1313) Investigate why\n LocalIndexIT.testLocalIndexScanAfterRegionSplit() is failing",
  "body": "\n    [ https://issues.apache.org/jira/browse/PHOENIX-1313?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14170731#comment-14170731 ] \n\nrajeshbabu commented on PHOENIX-1313:\n-------------------------------------\n\nList<List<Scan>> newNestedScans = this.getParallelScans(oldScan.getStartRow(), oldScan.getStopRow());\n\nHere for local indexes stop row should be value of EXPECTED_UPPER_REGION_KEY attribute in the scan object which is end key of old region.\n\n> Investigate why LocalIndexIT.testLocalIndexScanAfterRegionSplit() is failing\n> ----------------------------------------------------------------------------\n>\n>                 Key: PHOENIX-1313\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-1313\n>             Project: Phoenix\n>          Issue Type: Bug\n>    Affects Versions: 4.2\n>            Reporter: James Taylor\n>            Assignee: rajeshbabu\n>             Fix For: 5.0.0, 4.2\n>\n>         Attachments: PHOENIX-1313.patch, PHOENIX-1313_2.patch, PHOENIX-1313_4.patch\n>\n>\n> I checked in a fairly big change (https://git-wip-us.apache.org/repos/asf?p=phoenix.git;a=commit;h=d018cc1c6e01d9836de6e67af4f8b91de3269bfd) to improve encapsulation and fix a bunch of stats issues, and the following test started failing: LocalIndexIT.testLocalIndexScanAfterRegionSplit(). I tried to diagnose it, but that test is complicated. Would you mind taking a look [~rajesh23]?\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-10-14T09:52:33Z"
  },
  "patches": [],
  "external_id": "PHOENIX-1313"
},{
  "_id": {
    "$oid": "5bacc4fc56f6a00b02094ec3"
  },
  "message_id": "<JIRA.12718891.1402076091960.3865.1403130027366@arcas>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc4f856f6a00b02094da4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc4f856f6a00b02094da4"
  },
  "from_id": {
    "$oid": "5bacab8e57674ee167d2b54a"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (MAHOUT-1573) More explicit parallelism\n adjustments in math-scala DRM apis; elements of automatic parallelism\n management",
  "body": "\n     [ https://issues.apache.org/jira/browse/MAHOUT-1573?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDmitriy Lyubimov updated MAHOUT-1573:\n-------------------------------------\n\n    Resolution: Fixed\n        Status: Resolved  (was: Patch Available)\n\n> More explicit parallelism adjustments in math-scala DRM apis; elements of automatic parallelism management\n> ----------------------------------------------------------------------------------------------------------\n>\n>                 Key: MAHOUT-1573\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-1573\n>             Project: Mahout\n>          Issue Type: Task\n>    Affects Versions: 0.9\n>            Reporter: Dmitriy Lyubimov\n>            Assignee: Dmitriy Lyubimov\n>             Fix For: 1.0\n>\n>\n> (1) add minSplit parameter pass-thru to drmFromHDFS to be able to explicitly increase parallelism. \n> (2) add parrallelism readjustment parameter to a checkpoint() call. This implies shuffle-less coalesce() translation to the data set before it is requested to be cached (if specified).\n> Going forward, we probably should try and figure how we can automate it,  at least a little bit. For example, the simplest automatic adjustment might include re-adjust parallelims on load to simply fit cluster size (95% or 180% of cluster size, for example), with some rule-of-thumb safeguards here, e.g. we cannot exceed a factor of say 8 (or whatever we configure) in splitting each original hdfs split. We should be able to get a reasonable parallelism performance out of the box on simple heuristics like that.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-06-18T22:20:27Z"
  },
  "patches": [],
  "external_id": "MAHOUT-1573"
},{
  "_id": {
    "$oid": "5bc859f657a11257de562294"
  },
  "message_id": "<JIRA.13020507.1479132931000.294094.1479315118389@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc859e957a11257de56221c"
    },
    {
      "$oid": "5bc859e957a11257de56221d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc859e957a11257de56221c"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-3568) Characters widths and x-positions\n incorrect",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-3568?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTilman Hausherr updated PDFBOX-3568:\n------------------------------------\n    Affects Version/s: 1.8.12\n\n> Characters widths and x-positions incorrect\n> -------------------------------------------\n>\n>                 Key: PDFBOX-3568\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-3568\n>             Project: PDFBox\n>          Issue Type: Bug\n>    Affects Versions: 1.8.12\n>            Reporter: Roman\n>         Attachments: screenshot-1.png, test_width1.pdf, test_width11.png\n>\n>\n> Using PdfBox 1.8.12 we are extracting character coordinates from [^test_width1.pdf], as described in PDFBOX-3464\n> We got two issues here:\n> - wrong height, this issue was already addressed by PDFBOX-3464\n> - wrong width and X positions of characters, which we are getting from PdfBox via the code:\n> {code} \n>    position.getX();\n>    position.getWidth();\n> {code}\n> The coordinates, returned by PdfBox shown on [^screenshot-1.png] with blue color.\n> This code works fine on most PDFs. Something is wrong with this specific PDF. \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2016-11-16T16:51:58Z"
  },
  "patches": [],
  "external_id": "PDFBOX-3568"
},{
  "_id": {
    "$oid": "5bacb46ffaaadd76f8aa73ea"
  },
  "message_id": "<1918638504.1239201793830.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb3ebfaaadd76f8aa527e"
  },
  "from_id": {
    "$oid": "5bacb2a257674ee167d8402c"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (PIG-756) UDFs should have API for transparently\n opening and reading files from HDFS or from local file system with only\n relative path",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12697043#action_12697043 ] \n\nDavid Ciemiewicz commented on PIG-756:\n--------------------------------------\n\nBTW, there used to be a mechanism to do this in early versions of Pig that was last in the transition to the new execution system.\n\n> UDFs should have API for transparently opening and reading files from HDFS or from local file system with only relative path\n> ----------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: PIG-756\n>                 URL: https://issues.apache.org/jira/browse/PIG-756\n>             Project: Pig\n>          Issue Type: Bug\n>            Reporter: David Ciemiewicz\n>\n> I have a utility function util.INSETFROMFILE() that I pass a file name during initialization.\n> {code}\n> define inQuerySet util.INSETFROMFILE(analysis/queries);\n> A = load 'logs' using PigStorage() as ( date int, query chararray );\n> B = filter A by inQuerySet(query);\n> {code}\n> This provides a computationally inexpensive way to effect map-side joins for small sets plus functions of this style provide the ability to encapsulate more complex matching rules.\n> For rapid development and debugging purposes, I want this code to run without modification on both my local file system when I do pig -exectype local and on HDFS.\n> Pig needs to provide an API for UDFs which allow them to either:\n> 1) \"know\"  when they are in local or HDFS mode and let them open and read from files as appropriate\n> 2) just provide a file name and read statements and have pig transparently manage local or HDFS opens and reads for the UDF\n> UDFs need to read configuration information off the filesystem and it simplifies the process if one can just flip the switch of -exectype local.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-04-08T07:43:13Z"
  },
  "patches": [],
  "external_id": "PIG-756"
},{
  "_id": {
    "$oid": "5bbdf79be8113566f664ff79"
  },
  "message_id": "<11764529.1154718135612.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbdf79ae8113566f664ff72"
  },
  "from_id": {
    "$oid": "59bfafacf2a4565fe9134028"
  },
  "to_ids": [
    {
      "$oid": "5bbdf69457674ee16789fd6d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (NUTCH-340) Bug(s) in 0.8 tutorial",
  "body": "    [ http://issues.apache.org/jira/browse/NUTCH-340?page=comments#action_12425820 ] \n            \nSami Siren commented on NUTCH-340:\n----------------------------------\n\nthanks for the effort, I however cannot apply your patch.\n\nCan you please check out http://wiki.apache.org/nutch/HowToContribute\nfor instructions on how to construct a patch file for nutch and recreate that.\n\n\n\n> Bug(s) in 0.8 tutorial\n> ----------------------\n>\n>                 Key: NUTCH-340\n>                 URL: http://issues.apache.org/jira/browse/NUTCH-340\n>             Project: Nutch\n>          Issue Type: Bug\n>          Components: documentation\n>    Affects Versions: 0.8\n>            Reporter: Sami Siren\n>         Assigned To: Sami Siren\n>             Fix For: 0.9.0, 0.8.1\n>\n>         Attachments: patch.txt\n>\n>\n> There seems to be error(s) in whole web crawling section. This generates constantly (unneccessary) traffic to users list.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2006-08-04T12:02:15Z"
  },
  "patches": [],
  "external_id": "NUTCH-340"
},{
  "_id": {
    "$oid": "60fd7afa36c61279ee0bb38c"
  },
  "message_id": "<JIRA.13026561.1481173419000.469000.1481173438402@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fd79fc36c61279ee0b865d"
  },
  "reference_ids": [
    {
      "$oid": "60fd7afa36c61279ee0bb38a"
    },
    {
      "$oid": "60fd7afa36c61279ee0bb38b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fd7afa36c61279ee0bb38a"
  },
  "from_id": {
    "$oid": "60fd7ac5f73e2aa3900008cf"
  },
  "to_ids": [
    {
      "$oid": "5f27c3fcaf02e2d6de73a42d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (FINERACT-279) Incentive created for a senior\n citizen is getting added for a client with Term deposit created on back\n dated (client was not a senior citizen at that time)",
  "body": "Santosh Math created FINERACT-279:\n-------------------------------------\n\n             Summary: Incentive created for a senior citizen is getting added for a client with Term deposit created on back dated (client was not a senior citizen at that time)\n                 Key: FINERACT-279\n                 URL: https://issues.apache.org/jira/browse/FINERACT-279\n             Project: Apache Fineract\n          Issue Type: Bug\n          Components: Savings\n            Reporter: Santosh Math\n            Assignee: Markus Geiss\n            Priority: Minor\n\n\nReported by Subramanya at https://mifosforge.jira.com/browse/MIFOSX-1478\n\nOriginal Description:\n1. Create a client with back dated 05 August 2011 date of birth 01 August 1954, create a Recurring deposit product with with rate of interest 8% and incentive as 1% for senior citizen ( age >59)\n2. Submit a new Recurring deposit application for a client with backdated 05 August 2012 -> Approve and activate on same date.\n> Rate of interest is displaying as senior citizen though client was not a senior citizen at that time.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-12-08T05:03:58Z"
  },
  "patches": [],
  "external_id": "FINERACT-279"
},{
  "_id": {
    "$oid": "5beaa08b9e73d744d4136d95"
  },
  "message_id": "<4210F424.1030802@sbcglobal.net>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [
    {
      "$oid": "5beaa08a9e73d744d4136d8e"
    },
    {
      "$oid": "5beaa08a9e73d744d4136d8f"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5beaa08a9e73d744d4136d8f"
  },
  "from_id": {
    "$oid": "5bea98fc35e3ea2b7b505f80"
  },
  "to_ids": [
    {
      "$oid": "5bea980135e3ea2b7b4e3e72"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [jira] Commented: (DERBY-145) RAFContainer readPage method is\n not thread safe",
  "body": "I agree with dan (and from your reply below - it looks like you\nalso agree).  The synchronization looks right to me.\n\nWhen reading the raw store page code, remember that most of the\nsynchonization at the page level is by getting a latch on the\npage for the duration of the page operation - thus you won't see\na lot of sync blocks.  As dan points out the sync blocks you\nsee are going to be protecting one page from another page while\nreading from the file shared by both.\n\nCould you mark the jira entry not a bug (or whatever the jira equivalent\nis) - or let me know if you\ndon't have permission to do so.\n\n/mikem\n\nRPost wrote:\n> You are correct. Brain fart! Fix - new supply of Beano.\n> ----- Original Message -----\n> From: \"Daniel John Debrunner (JIRA)\" <derby-dev@db.apache.org>\n> To: <rp0428@pacbell.net>\n> Sent: Saturday, February 12, 2005 9:46 PM\n> Subject: [jira] Commented: (DERBY-145) RAFContainer readPage method is not\n> thread safe\n> \n> \n> \n>>     [\n> \n> http://issues.apache.org/jira/browse/DERBY-145?page=comments#action_59105 ]\n> \n>>Daniel John Debrunner commented on DERBY-145:\n>>---------------------------------------------\n>>\n>>readPage() is correctly thread safe. The calculation of pageOffset is a\n> \n> local variable and references a local variable pageNumber and a constant\n> pageSize (for the lifetime of the object), thus it does not need to be\n> synchronized. The synchronized block is there to prevent multiple callers of\n> readPage from modifying the file descriptor fileData's position used to read\n> the page from file. I.e if multiple callers were allowed into that small\n> synchronized section then the offset could change between setting it and\n> reading the data, leading to the wrong page being read in. Comments could\n> probably be added to the code.\n> \n>>>RAFContainer readPage method is not thread safe\n>>>-----------------------------------------------\n>>>\n>>>         Key: DERBY-145\n>>>         URL: http://issues.apache.org/jira/browse/DERBY-145\n>>>     Project: Derby\n>>>        Type: Bug\n>>>  Components: Store\n>>>    Versions: 10.0.2.1\n>>> Environment: N/A\n>>>    Reporter: Rick Post\n>>>    Priority: Minor\n>>\n>>>readPage method comment says 'thread safe and has a synchronized block.\n>>>But 'pageOffset' computation occurs outside the sync block as does\n> \n> decryption.\n> \n>>>This allows the (remote?) possibility of part of the operation(s) being\n> \n> performed using the wrong pageNumber or pageData.\n> \n>>>Fix - synchronize the method rather than the block\n>>>/**\n>>>Read a page into the supplied array.\n>>><BR> MT - thread safe\n>>>@exception IOException exception reading page\n>>>@exception StandardException Standard Cloudscape error policy\n>>>*/\n>>>protected void readPage(long pageNumber, byte[] pageData)\n>>>throws IOException, StandardException\n>>>{\n>>>if (SanityManager.DEBUG) {\n>>>SanityManager.ASSERT(!getCommittedDropState());\n>>>}\n>>>long pageOffset = pageNumber * pageSize;\n>>>synchronized (this) {\n>>>fileData.seek(pageOffset);\n>>>fileData.readFully(pageData, 0, pageSize);\n>>>}\n>>>if (dataFactory.databaseEncrypted() &&\n>>>pageNumber != FIRST_ALLOC_PAGE_NUMBER)\n>>>{\n>>>decryptPage(pageData, pageSize);\n>>>}\n>>>}\n>>\n>>--\n>>This message is automatically generated by JIRA.\n>>-\n>>If you think it was sent incorrectly contact one of the administrators:\n>>   http://issues.apache.org/jira/secure/Administrators.jspa\n>>-\n>>If you want more information on JIRA, or have a bug to report see:\n>>   http://www.atlassian.com/software/jira\n>>\n> \n> \n> \n> \n\n",
  "date": {
    "$date": "2005-02-14T10:55:32Z"
  },
  "patches": [],
  "external_id": "DERBY-145"
},{
  "_id": {
    "$oid": "5bea9f559e73d744d4132d6b"
  },
  "message_id": "<53996063.1145032381915.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9dc19e73d744d412ced0"
  },
  "from_id": {
    "$oid": "5bea9e8b35e3ea2b7b5e34db"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (DERBY-1025) [xa] client XAResource.start() does\n not commit an active local transaction when auto commit is true",
  "body": "    [ http://issues.apache.org/jira/browse/DERBY-1025?page=comments#action_12374540 ] \n\nKathey Marsden commented on DERBY-1025:\n---------------------------------------\n\nI checked the patch into the trunk:\nDate: Fri Apr 14 09:30:00 2006\nNew Revision: 394134\n\nURL: http://svn.apache.org/viewcvs?rev=394134&view=rev\n\nThis change looks good to me but when I ran jdbcapi/checkDataSource.java  and jdbcapi/checkDataSource30.java and  with client I got these (expected I think) diffs.  I updated the masters and checked in.\nPlease check that the output difference is expected.\n\nAttempt to shutdown framework: DerbyNetClient\n58 del\n< expected java.sql.SQLException: Invalid operation: result set closed\n58a58\n> expected java.sql.SQLException: 'ResultSet' already closed.\n460 del\n< autocommitxastart expected Invalid operation: result set closed\n460a460\n> autocommitxastart expected 'ResultSet' already closed.\nTest Failed.\n*** End:   checkDataSource jdk1.4.2_07 DerbyNetClient 2006-04-14 09:04:19 ***\n*** Start: checkDataSource30 jdk1.4.2_07 DerbyNetClient 2006-04-14 09:17:14 ***\nInitialize for framework: DerbyNetClient\njava -Dderby.system.home=D:\\testout\\DerbyNetClient\\checkDataSource30 -Djava.security.manager -Djava.security.policy=D:\\testout\\derby_tests.policy -DderbyTesting.codeclasses=file:/D:/p4/marsden_patch/c\nlasses/ -DderbyTesting.codedir=D:\\p4\\marsden_patch\\classes -DderbyTesting.serverhost=localhost -DderbyTesting.clienthost=localhost -DderbyTesting.codejar=file://unused/ org.apache.derby.drda.NetworkSe\nrverControl start\nAttempt to shutdown framework: DerbyNetClient\n70 del\n< expected java.sql.SQLException: Invalid operation: result set closed\n70a70\n> expected java.sql.SQLException: 'ResultSet' already closed.\n551a552,559\n> conn4 autcommit true\n> acxs 1\n> acxs 2\n> autocommitxastart expected 'ResultSet' already closed.\n> acxs 1\n> acxs 2\n> autocommitxastart expected  : XAException - XAER_DUPID : Error executing a XAResource.start(), Server returned XAER_DUPID\n> acxs 3\n578 del\n< Expected SQLException Invalid operation: result set closed\n578a586\n> Expected SQLException 'ResultSet' already closed.\n590 del\n< Expected SQLException Invalid operation: result set closed\n591 del\n< Expected SQLException Invalid operation: result set closed\n591a598,599\n> Expected SQLException 'ResultSet' already closed.\n> Expected SQLException 'ResultSet' already closed.\nTest Failed.\n*** End:   checkDataSource30 jdk1.4.2_07 DerbyNetClient 2006-04-14 09:18:29 ***\n\n\n> [xa] client XAResource.start() does not commit an active local transaction when auto commit is true\n> ---------------------------------------------------------------------------------------------------\n>\n>          Key: DERBY-1025\n>          URL: http://issues.apache.org/jira/browse/DERBY-1025\n>      Project: Derby\n>         Type: Bug\n\n>   Components: Network Client\n>     Reporter: Daniel John Debrunner\n>     Assignee: Deepa Remesh\n>  Attachments: derby-1025-draft1.diff, derby-1025-draft1.status, derby-1025-patch1-v1.diff, derby-1025-patch1-v1.status\n>\n> Embedded XAResource.start() implementation commits the active local transaction on the Connection associated with the XAResource if the connection is auto-commit mode.\n> Client incorrectly throws an XAException with the XAER_RMFAIL error code (see DERBY-1024)\n> XATest contains a work-around for client (calling commit) with a comment with this bug number.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-04-14T16:33:01Z"
  },
  "patches": [],
  "external_id": "DERBY-1025"
},{
  "_id": {
    "$oid": "5bbf0be9b79d666cbb228509"
  },
  "message_id": "<JIRA.12936925.1454634476000.136155.1456340178117@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0bceb79d666cbb228087"
    },
    {
      "$oid": "5bbf0bceb79d666cbb228086"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0bceb79d666cbb228086"
  },
  "from_id": {
    "$oid": "5bbf072157674ee167304944"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (KAFKA-3214) Add consumer system tests for\n compressed topics",
  "body": "\n     [ https://issues.apache.org/jira/browse/KAFKA-3214?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAnna Povzner updated KAFKA-3214:\n--------------------------------\n    Status: Patch Available  (was: In Progress)\n\n> Add consumer system tests for compressed topics\n> -----------------------------------------------\n>\n>                 Key: KAFKA-3214\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-3214\n>             Project: Kafka\n>          Issue Type: Test\n>          Components: consumer\n>            Reporter: Jason Gustafson\n>            Assignee: Anna Povzner\n>             Fix For: 0.10.0.0\n>\n>\n> As far as I can tell, we don't have any ducktape tests which verify correctness when compression is enabled. If we did, we might have caught KAFKA-3179 earlier.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-02-24T18:56:18Z"
  },
  "patches": [],
  "external_id": "KAFKA-3214"
},{
  "_id": {
    "$oid": "5bc8613c6e373d4fe81c7be9"
  },
  "message_id": "<556220664.3001.1331564927245.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc860f66e373d4fe81c7a00"
  },
  "from_id": {
    "$oid": "5bc8605157674ee167cffdff"
  },
  "to_ids": [
    {
      "$oid": "5bc85ac857674ee167cc9912"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FLUME-1018) Context can cause\n NullPointerException",
  "body": "\n     [ https://issues.apache.org/jira/browse/FLUME-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nBrock Noland updated FLUME-1018:\n--------------------------------\n\n    Attachment: FLUME-1018-0.patch\n\nPatch from RB attached.\n                \n> Context can cause NullPointerException\n> --------------------------------------\n>\n>                 Key: FLUME-1018\n>                 URL: https://issues.apache.org/jira/browse/FLUME-1018\n>             Project: Flume\n>          Issue Type: Bug\n>          Components: Configuration\n>    Affects Versions: v1.0.0\n>            Reporter: Hari Shreedharan\n>            Priority: Minor\n>             Fix For: v1.1.0\n>\n>         Attachments: FLUME-1018-0.patch\n>\n>\n> The new get functions in context class like getInteger, getBoolean etc can cause NullPointerException if the configuration key which is being searched is not in the context map. This is because the get<Type>(String key) version is basically a call to get<Type>(String key, null). Therefore it can return null. Java will automatically unbox the value returned by this function if the user code accepts it into a native datatype variable rather than a wrapper object. \n> For example int rollInterval = getInteger(\"RollInterval\") would cause a NullPointerException if rollInterval is absent in the context(since getInteger would return null). It is common to accept an Integer object into an int. We should at the least fix the documentation to suggest that this function will return null if the value is not found, and the return value should be an Integer/Boolean/Long object, not their native counter parts.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-03-12T15:08:47Z"
  },
  "patches": [],
  "external_id": "FLUME-1018"
},{
  "_id": {
    "$oid": "5bbdad9ec764eb6c7a266e5d"
  },
  "message_id": "<JIRA.12685409.1387384876923.60354.1387400467472@arcas>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdac83c764eb6c7a2647d8"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdac83c764eb6c7a2647d8"
  },
  "from_id": {
    "$oid": "5bbdaba757674ee167d56da1"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (FALCON-234) Replication feed not respecting\n partition filter instead copying all data to target",
  "body": "\n    [ https://issues.apache.org/jira/browse/FALCON-234?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13852158#comment-13852158 ] \n\nVenkatesh Seetharam commented on FALCON-234:\n--------------------------------------------\n\nHmmm, I thought this was handled as part of FALCON-95 and [~samarthg] tested this on Hadoop-1. Not sure if Hadoop-2 should have anything to do with this custom code.\n\n[~sriksun], can you please take a look at this since I'm not quite familiar with the regex in FilteredCopyListing class.\n\n> Replication feed not respecting partition filter instead copying all data to target\n> -----------------------------------------------------------------------------------\n>\n>                 Key: FALCON-234\n>                 URL: https://issues.apache.org/jira/browse/FALCON-234\n>             Project: Falcon\n>          Issue Type: Bug\n>          Components: replication\n>    Affects Versions: 0.4\n>            Reporter: Arpit Gupta\n>\n> A replication feed was setup where the source was\n> cluster-1:8020/data with partition set to ua1. The feed instead of copying over just cluster-1:8020/data/ua1 copied over cluster-1:8020/data.\n> This was seen on a cluster running Hadoop 2.2.0 and Oozie 4.0.0\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.4#6159)\n",
  "date": {
    "$date": "2013-12-18T21:01:07Z"
  },
  "patches": [],
  "external_id": "FALCON-234"
},{
  "_id": {
    "$oid": "5f27cdc4442ab9b9860ef5b4"
  },
  "message_id": "<JIRA.13080339.1497597476000.136774.1498758960095@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27cd1d442ab9b9860ec60e"
    },
    {
      "$oid": "5f27cd1d442ab9b9860ec60f"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cd1d442ab9b9860ec60e"
  },
  "from_id": {
    "$oid": "5bacb0d457674ee167d4db67"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (OOZIE-2949) Escape quotes whitespaces in Sqoop\n <command> field",
  "body": "\n    [ https://issues.apache.org/jira/browse/OOZIE-2949?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16068708#comment-16068708 ] \n\nArtem Ervits commented on OOZIE-2949:\n-------------------------------------\n\n[~gezapeti] can you provide a sqoop import command to reproduce the issue? Are you saying the following should handle double quotes?\n\n{code}\nimport  --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:8020/user/tucu/foo -m 1\n{code}\n\nwith a fix this should work too\n\n{code}\nimport  --connect \"jdbc:hsqldb:file:db.hsqldb\" --table \"TT\" --target-dir \"hdfs://localhost:8020/user/tucu/foo\" -m \"1\"\n{code}\n\ndo I understand the problem correctly?\n\n> Escape quotes whitespaces in Sqoop <command> field\n> --------------------------------------------------\n>\n>                 Key: OOZIE-2949\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-2949\n>             Project: Oozie\n>          Issue Type: Bug\n>            Reporter: Peter Cseh\n>            Assignee: Artem Ervits\n>\n> The current behavior of the Sqoop action is:\n> {noformat}\n> The Sqoop command can be specified either using the command element or multiple arg elements.\n> When using the command element, Oozie will split the command on every space into multiple arguments.\n> When using the arg elements, Oozie will pass each argument value as an argument to Sqoop.\n> {noformat}\n> This prevents the user to simply copy-paste the command worked in the shell into the workflow.xml.\n> We should split the <command> field by taking quotes into account, similar to what OOZIE-2391\n> did for the Spark action's <spark-opts> field.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-06-29T17:56:00Z"
  },
  "patches": [],
  "external_id": "OOZIE-2949"
},{
  "_id": {
    "$oid": "5f27cd63014d3531c6cc1a2c"
  },
  "message_id": "<293928189.64650.1342544314654.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "59bf92f2f2a4565fe9e6d721"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (JCR-3390) Reordering policy node fails with\n AccessDeniedException",
  "body": "angela created JCR-3390:\n---------------------------\n\n             Summary: Reordering policy node fails with AccessDeniedException\n                 Key: JCR-3390\n                 URL: https://issues.apache.org/jira/browse/JCR-3390\n             Project: Jackrabbit Content Repository\n          Issue Type: Bug\n          Components: jackrabbit-core\n    Affects Versions: 2.4\n            Reporter: angela\n            Assignee: angela\n\n\nPrivilegeRegistry#calculatePermissions ignores the MODIFY_CHILD_NODE_COLLECTION permission when evaluating permissions for access control content. consequently reordering an policy node fails with\naccess denied exception even if all required privileges have been granted.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-07-17T16:58:34Z"
  },
  "patches": [],
  "external_id": "JCR-3390"
},{
  "_id": {
    "$oid": "5bc85a9957a11257de5626f9"
  },
  "message_id": "<JIRA.13014997.1477379686000.87195.1477467178811@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc85a9657a11257de5626e4"
    },
    {
      "$oid": "5bc85a9657a11257de5626e3"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc85a9657a11257de5626e3"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-3542) Can PDFBOX use Streams to read\n PDSignatures from document?",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-3542?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15607728#comment-15607728 ] \n\nTilman Hausherr commented on PDFBOX-3542:\n-----------------------------------------\n\nLoading from a file is best. To save memory, try\n{code}\nPDDocument.load (new File(), MemoryUsageSetting.setupTempFileOnly());\n{code}\n\n\n> Can PDFBOX use Streams to read PDSignatures from document?\n> ----------------------------------------------------------\n>\n>                 Key: PDFBOX-3542\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-3542\n>             Project: PDFBox\n>          Issue Type: Wish\n>          Components: PDModel\n>    Affects Versions: 2.0.3\n>            Reporter: Andrea Paternesi\n>            Priority: Critical\n>\n> I did not find a way to avoid loading into memory the whole PDDocument to read the signatures dictionaries.\n> If you have very big PDF files (30MB or more), java gets an Out of Memory error.\n> Right now i did not find a correct way to load signatures usign stream.\n> Can you give any hont?\n> Thanks in advance.\n> Andrea.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2016-10-26T07:32:58Z"
  },
  "patches": [],
  "external_id": "PDFBOX-3542"
},{
  "_id": {
    "$oid": "5c5801b5e078b00ec4e72bd1"
  },
  "message_id": "<JIRA.12910526.1446677195000.161934.1540056300393@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c5801b5e078b00ec4e72bd0"
    },
    {
      "$oid": "5c5801b5e078b00ec4e72bcf"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c5801b5e078b00ec4e72bcf"
  },
  "from_id": {
    "$oid": "59bfa255f2a4565fe9fa426d"
  },
  "to_ids": [
    {
      "$oid": "5bbdf31357674ee1677dbf18"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (RANGER-719) Audit in RangerAdmin doesn't show\n latest audits in UI if server is on different timezone",
  "body": "\n     [ https://issues.apache.org/jira/browse/RANGER-719?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDon Bosco Durai closed RANGER-719.\n----------------------------------\n\n> Audit in RangerAdmin doesn't show latest audits in UI if server is on different timezone\n> ----------------------------------------------------------------------------------------\n>\n>                 Key: RANGER-719\n>                 URL: https://issues.apache.org/jira/browse/RANGER-719\n>             Project: Ranger\n>          Issue Type: Bug\n>    Affects Versions: 0.5.1\n>            Reporter: Don Bosco Durai\n>            Assignee: Pradeep Agrawal\n>            Priority: Major\n>             Fix For: 0.5.0, 0.6.0\n>\n>\n> If the Hadoop cluster is on different timezone than the user browsers, then the time filter doesn't work as expected.\n> Since Solr stores everything in GMT, we have to make sure all calls are converted to GMT and also the browser should handle GMT values. This is just a theory, but we need to look into how date is managed:\n> 1. From plugins to Solr\n> 2. Request from browser to ranger admin to Solr and back.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-10-20T17:25:00Z"
  },
  "patches": [],
  "external_id": "RANGER-719"
},{
  "_id": {
    "$oid": "5bc85ef46e373d4fe81c6eff"
  },
  "message_id": "<302462809.501.1339262502829.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5bacb2f057674ee167d8d7f3"
  },
  "to_ids": [
    {
      "$oid": "5bc85ac857674ee167cc9912"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (FLUME-1268) FileChannel ReplayLog fails to replay",
  "body": "Mubarak Seyed created FLUME-1268:\n------------------------------------\n\n             Summary: FileChannel ReplayLog fails to replay\n                 Key: FLUME-1268\n                 URL: https://issues.apache.org/jira/browse/FLUME-1268\n             Project: Flume\n          Issue Type: Bug\n          Components: Sinks+Sources\n    Affects Versions: v1.2.0\n            Reporter: Mubarak Seyed\n\n\n\n\n{code}\n2012-06-09 17:15:05,718 ERROR file.Log: Failed to initialize Log\njava.lang.IllegalStateException: Pending takes 1516 exist after the end of replay\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n\tat org.apache.flume.channel.file.ReplayHandler.replayLog(ReplayHandler.java:137)\n\tat org.apache.flume.channel.file.Log.replay(Log.java:205)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:180)\n\tat org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:228)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n2012-06-09 17:15:05,718 ERROR lifecycle.LifecycleSupervisor: Unable to start org.apache.flume.channel.file.FileChannel@556917ee - Exception follows.\njava.lang.IllegalStateException: Log is closed\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n\tat org.apache.flume.channel.file.Log.getFlumeEventQueue(Log.java:226)\n\tat org.apache.flume.channel.file.FileChannel.getDepth(FileChannel.java:253)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:187)\n\tat org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:228)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n2012-06-09 17:15:05,926 INFO nodemanager.DefaultLogicalNodeManager: Waiting for channel: file-channel-1 to start. Sleeping for 500 ms\n2012-06-09 17:15:06,428 INFO nodemanager.DefaultLogicalNodeManager: Waiting for channel: file-channel-1 to start. Sleeping for 500 ms\n\n2012-06-09 17:15:08,377 INFO file.FileChannel: Starting FileChannel with dataDir [/data3/flume/file-channel/data2]\n2012-06-09 17:15:08,378 INFO file.Log: Cannot lock /data3/flume/file-channel/checkpoint2. The directory is already locked.\n2012-06-09 17:15:08,379 ERROR lifecycle.LifecycleSupervisor: Unable to start org.apache.flume.channel.file.FileChannel@4bf54c5f - Exception follows.\njava.lang.RuntimeException: java.io.IOException: Cannot lock /data3/flume/file-channel/checkpoint2. The directory is already locked.\n\tat com.google.common.base.Throwables.propagate(Throwables.java:156)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:182)\n\tat org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:228)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: Cannot lock /data3/flume/file-channel/checkpoint2. The directory is already locked.\n\tat org.apache.flume.channel.file.Log.lock(Log.java:574)\n\tat org.apache.flume.channel.file.Log.<init>(Log.java:95)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:178)\n\t... 10 more\n2012-06-09 17:15:08,425 INFO file.FileChannel: Starting FileChannel with dataDir [/data2/flume/file-channel/data1]\n2012-06-09 17:15:08,425 INFO file.Log: Cannot lock /data2/flume/file-channel/checkpoint1. The directory is already locked.\n2012-06-09 17:15:08,425 ERROR lifecycle.LifecycleSupervisor: Unable to start org.apache.flume.channel.file.FileChannel@42f1916f - Exception follows.\njava.lang.RuntimeException: java.io.IOException: Cannot lock /data2/flume/file-channel/checkpoint1. The directory is already locked.\n\tat com.google.common.base.Throwables.propagate(Throwables.java:156)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:182)\n\tat org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:228)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: Cannot lock /data2/flume/file-channel/checkpoint1. The directory is already locked.\n\tat org.apache.flume.channel.file.Log.lock(Log.java:574)\n\tat org.apache.flume.channel.file.Log.<init>(Log.java:95)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:178)\n\t... 10 more\n\n2012-06-09 17:15:08,535 INFO file.FileChannel: Starting FileChannel with dataDir [/data5/flume/file-channel/data4]\n2012-06-09 17:15:08,535 INFO file.Log: Cannot lock /data5/flume/file-channel/checkpoint4. The directory is already locked.\n2012-06-09 17:15:08,535 ERROR lifecycle.LifecycleSupervisor: Unable to start org.apache.flume.channel.file.FileChannel@66a23610 - Exception follows.\njava.lang.RuntimeException: java.io.IOException: Cannot lock /data5/flume/file-channel/checkpoint4. The directory is already locked.\n\tat com.google.common.base.Throwables.propagate(Throwables.java:156)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:182)\n\tat org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:228)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: Cannot lock /data5/flume/file-channel/checkpoint4. The directory is already locked.\n\tat org.apache.flume.channel.file.Log.lock(Log.java:574)\n\tat org.apache.flume.channel.file.Log.<init>(Log.java:95)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:178)\n\t... 10 more\n2012-06-09 17:15:08,719 INFO file.FileChannel: Starting FileChannel with dataDir [/data4/flume/file-channel/data3]\n2012-06-09 17:15:08,719 INFO file.Log: Cannot lock /data4/flume/file-channel/checkpoint3. The directory is already locked.\n2012-06-09 17:15:08,719 ERROR lifecycle.LifecycleSupervisor: Unable to start org.apache.flume.channel.file.FileChannel@556917ee - Exception follows.\njava.lang.RuntimeException: java.io.IOException: Cannot lock /data4/flume/file-channel/checkpoint3. The directory is already locked.\n\tat com.google.common.base.Throwables.propagate(Throwables.java:156)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:182)\n\tat org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:228)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: Cannot lock /data4/flume/file-channel/checkpoint3. The directory is already locked.\n\tat org.apache.flume.channel.file.Log.lock(Log.java:574)\n\tat org.apache.flume.channel.file.Log.<init>(Log.java:95)\n\tat org.apache.flume.channel.file.FileChannel.start(FileChannel.java:178)\n\t... 10 more\n2012-06-09 17:15:08,938 INFO nodemanager.DefaultLogicalNodeManager: Waiting for channel: file-channel-1 to start. Sleeping for 500 ms\n\n{code}\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-06-09T17:21:42Z"
  },
  "patches": [],
  "external_id": "FLUME-1268"
},{
  "_id": {
    "$oid": "5f27cff6532b7277349c14df"
  },
  "message_id": "<392718247.41389.1323100360170.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27cff6532b7277349c14d8"
  },
  "from_id": {
    "$oid": "5f27cfe4af02e2d6de8fb2d3"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (AMQ-3618) when using openwire with\n cacheEnabled=false memory is wasted on unmarshallCache[] and\n marshallCache[]",
  "body": "\n    [ https://issues.apache.org/jira/browse/AMQ-3618?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13162841#comment-13162841 ] \n\nRob Davies commented on AMQ-3618:\n---------------------------------\n\nThank you for contributing Jacob!\n                \n> when using openwire with cacheEnabled=false memory is wasted on unmarshallCache[] and marshallCache[]\n> -----------------------------------------------------------------------------------------------------\n>\n>                 Key: AMQ-3618\n>                 URL: https://issues.apache.org/jira/browse/AMQ-3618\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>            Reporter: jacob hameiri\n>            Assignee: Rob Davies\n>            Priority: Minor\n>             Fix For: 5.6.0\n>\n>         Attachments: OpenWireFormat.patch\n>\n>\n> The OpenWireFormat has 2 array fields unmarshallCache[] and marshallCache[] which together consume 256K, in case of many TopicConsumers when using FilePendingMessageCursor this causes allot of memory to be used by the OpenWire caching arrays. even if you disable caching these arrays are still created.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-12-05T15:52:40Z"
  },
  "patches": [],
  "external_id": "AMQ-3618"
},{
  "_id": {
    "$oid": "5bbf0d1a35bc96443481de7d"
  },
  "message_id": "<JIRA.12838886.1434652565000.143536.1446592467869@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf090e35bc964434814f5a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0ccb35bc96443481d185"
    },
    {
      "$oid": "5bbf0ccb35bc96443481d186"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0ccb35bc96443481d185"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdff0"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (STORM-876) Dist Cache: Basic Functionality",
  "body": "\n    [ https://issues.apache.org/jira/browse/STORM-876?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14988417#comment-14988417 ] \n\nASF GitHub Bot commented on STORM-876:\n--------------------------------------\n\nGithub user kishorvpatil commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/845#discussion_r43821497\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/blobstore/HdfsBlobStore.java ---\n    @@ -0,0 +1,369 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package backtype.storm.blobstore;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.generated.*;\n    +import backtype.storm.nimbus.NimbusInfo;\n    +import backtype.storm.utils.NimbusClient;\n    +import backtype.storm.utils.Utils;\n    +import org.apache.hadoop.conf.Configuration;\n    +import org.apache.hadoop.fs.Path;\n    +import org.apache.hadoop.security.UserGroupInformation;\n    +import org.apache.thrift.TBase;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import java.io.ByteArrayOutputStream;\n    +import java.io.FileNotFoundException;\n    +import java.io.IOException;\n    +import java.io.InputStream;\n    +import java.security.AccessController;\n    +import java.security.PrivilegedAction;\n    +import java.util.Iterator;\n    +import java.util.Map;\n    +\n    +import static backtype.storm.blobstore.BlobStoreAclHandler.*;\n    --- End diff --\n    \n    explicit import\n\n\n> Dist Cache: Basic Functionality\n> -------------------------------\n>\n>                 Key: STORM-876\n>                 URL: https://issues.apache.org/jira/browse/STORM-876\n>             Project: Apache Storm\n>          Issue Type: Improvement\n>          Components: storm-core\n>            Reporter: Robert Joseph Evans\n>            Assignee: Robert Joseph Evans\n>         Attachments: DISTCACHE.md, DistributedCacheDesignDocument.pdf\n>\n>\n> Basic functionality for the Dist Cache feature.\n> As part of this a new API should be added to support uploading and downloading dist cache items.  storm-core.ser, storm-conf.ser and storm.jar should be written into the blob store instead of residing locally. We need a default implementation of the blob store that does essentially what nimbus currently does and does not need anything extra.  But having an HDFS backend too would be great for scalability and HA.\n> The supervisor should provide a way to download and manage these blobs and provide a working directory for the worker process with symlinks to the blobs.  It should also allow the blobs to be updated and switch the symlink atomically to point to the new blob once it is downloaded.\n> All of this is already done by code internal to Yahoo! we are in the process of getting it ready to push back to open source shortly.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-11-03T23:14:27Z"
  },
  "patches": [],
  "external_id": "STORM-876"
},{
  "_id": {
    "$oid": "5c5805fde078b00ec4e7566f"
  },
  "message_id": "<JIRA.13041290.1486537585000.37717.1486640381759@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c5805f7e078b00ec4e7561a"
    },
    {
      "$oid": "5c5805f7e078b00ec4e7561b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c5805f7e078b00ec4e7561a"
  },
  "from_id": {
    "$oid": "5bbe12a057674ee1679b1512"
  },
  "to_ids": [
    {
      "$oid": "5bbdf31357674ee1677dbf18"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (RANGER-1363) BadCredentialsException when login",
  "body": "\n     [ https://issues.apache.org/jira/browse/RANGER-1363?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nColm O hEigeartaigh resolved RANGER-1363.\n-----------------------------------------\n    Resolution: Fixed\n\n> BadCredentialsException when login\n> ----------------------------------\n>\n>                 Key: RANGER-1363\n>                 URL: https://issues.apache.org/jira/browse/RANGER-1363\n>             Project: Ranger\n>          Issue Type: Bug\n>          Components: admin\n>    Affects Versions: 0.7.0\n>            Reporter: Qiang Zhang\n>            Assignee: Qiang Zhang\n>              Labels: patch\n>             Fix For: 0.7.0\n>\n>         Attachments: 0001-RANGER-1363-BadCredentialsException-when-login.patch\n>\n>\n> It will always print BadCredentialsException in ranger_admin.log when log to the system. I checked the codes and found the reason:\n> First we set the encoder as 'SHA256'.\n> String encoder=\"SHA256\";\n> try {\n> \tauthentication = getJDBCAuthentication(authentication,encoder);\n> } catch (Exception e) {\n>         logger.info(\"JDBC Authentication failure: \", e);\n> } \n> Then we set the encoder as 'MD5'.\n> if (authentication !=null && authentication.isAuthenticated()) {\n> \treturn authentication;\n> }\n> if (authentication != null && !authentication.isAuthenticated()) {\n> \tencoder=\"MD5\";\n>         ......\n>         try {\n> \t        authentication = getJDBCAuthentication(authentication,encoder);\n>         } catch (Exception e) {\n>                 logger.info(\"JDBC Authentication failure: \", e);\n>         } \n> ......\n> }\n> Since our encoder is 'MD5', we can log to the system successfully but still can see the error log for 'SHA256'. I have do some changes for the codes and avoid the BadCredentialsException.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-09T11:39:41Z"
  },
  "patches": [],
  "external_id": "RANGER-1363"
},{
  "_id": {
    "$oid": "5bbdaabec764eb6c7a260425"
  },
  "message_id": "<JIRA.13139397.1519037291000.227483.1519039380128@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdaabec764eb6c7a260418"
    },
    {
      "$oid": "5bbdaabec764eb6c7a260417"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdaabec764eb6c7a260417"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (FALCON-2322) ClassCastException while\n submitAndSchedule feed",
  "body": "\n    [ https://issues.apache.org/jira/browse/FALCON-2322?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16369005#comment-16369005 ] \n\nASF GitHub Bot commented on FALCON-2322:\n----------------------------------------\n\nGitHub user bpgergo opened a pull request:\n\n    https://github.com/apache/falcon/pull/396\n\n    FALCON-2322 cast Action.getAny() to org.w3c.dom.Node instead of Eleme…\n\n    …ntNSImpl in order to eliminate ClassCastException\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/bpgergo/falcon master\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/falcon/pull/396.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #396\n    \n----\ncommit 82e64c78c5a6c85a2416702789574f7e98ba20df\nAuthor: Peter Gergo Barna <pbarna@...>\nDate:   2018-02-19T11:20:29Z\n\n    FALCON-2322 cast Action.getAny() to org.w3c.dom.Node instead of ElementNSImpl in order to eliminate ClassCastException\n\n----\n\n\n> ClassCastException while submitAndSchedule feed\n> -----------------------------------------------\n>\n>                 Key: FALCON-2322\n>                 URL: https://issues.apache.org/jira/browse/FALCON-2322\n>             Project: Falcon\n>          Issue Type: Bug\n>    Affects Versions: 0.9\n>            Reporter: Péter Gergő Barna\n>            Priority: Major\n>             Fix For: trunk\n>\n>\n> ClassCastException occurs while submitAndSchedule feed\n> uri: http://[hostname]:15000/api/entities/submit/cluster?user.name=hrt_qa\n> payload:\n>  \n> {noformat}\n> <?xml version=\"1.0\" encoding=\"UTF-8\"?><feed xmlns=\"uri:falcon:feed:0.1\" name=\"A323e3eea-71bf9cf8\" description=\"clicks log\"> <frequency>hours(1)</frequency> <timezone>UTC</timezone> <late-arrival cut-off=\"hours(6)\"/> <clusters> <cluster name=\"A323e3eea-81f1526b\" type=\"source\"> <validity start=\"2010-01-01T20:00Z\" end=\"2099-01-01T00:00Z\"/> <retention limit=\"months(9000)\" action=\"delete\"/> </cluster> <cluster name=\"A323e3eea-0729973a\" type=\"target\"> <validity start=\"2010-01-01T20:00Z\" end=\"2099-01-01T00:00Z\"/> <retention limit=\"months(9000)\" action=\"delete\"/> <table uri=\"catalog:default:hcatFeedOperationsTest#year=${YEAR}\"/> </cluster> </clusters> <table uri=\"catalog:default:hcatFeedOperationsTest#year=${YEAR}\"/> <ACL owner=\"hrt_qa\" group=\"users\" permission=\"0x755\"/> <schema location=\"hcat\" provider=\"hcat\"/> <properties> <property name=\"field1\" value=\"value1\"/> <property name=\"field2\" value=\"value2\"/> </properties> </feed>\n>  \n> {noformat}\n>  \n> Exception in the log:\n> {noformat}\n> 000007.hwx.site:9083 (OozieEntityBuilder:71) 2018-02-15 00:09:53,273 ERROR - [290968357@qtp-57241990-276 - 64cb2640-bfde-48b7-a97a-70d016d74b98:hrt_qa:POST//entities/submitAndSchedule/feed] ~ Entity schedule failed for feed: A323e3eea-71bf9cf8 (AbstractSchedulableEntityManager:104) java.lang.ClassCastException: com.sun.org.apache.xerces.internal.dom.ElementNSImpl cannot be cast to org.apache.xerces.dom.ElementNSImpl at org.apache.falcon.util.OozieUtils.unMarshalHiveAction(OozieUtils.java:89) at org.apache.falcon.oozie.feed.HCatReplicationWorkflowBuilder.getWorkflow(HCatReplicationWorkflowBuilder.java:84) at org.apache.falcon.oozie.feed.FeedReplicationWorkflowBuilder.build(FeedReplicationWorkflowBuilder.java:74) at org.apache.falcon.oozie.feed.FeedReplicationCoordinatorBuilder.doBuild(FeedReplicationCoordinatorBuilder.java:121) at org.apache.falcon.oozie.feed.FeedReplicationCoordinatorBuilder.buildCoords(FeedReplicationCoordinatorBuilder.java:93) at org.apache.falcon.oozie.feed.FeedBundleBuilder.buildCoords(FeedBundleBuilder.java:69) at org.apache.falcon.oozie.OozieBundleBuilder.build(OozieBundleBuilder.java:71) at org.apache.falcon.oozie.OozieEntityBuilder.build(OozieEntityBuilder.java:120) at org.apache.falcon.oozie.OozieEntityBuilder.build(OozieEntityBuilder.java:116) at org.apache.falcon.workflow.engine.OozieWorkflowEngine.schedule(OozieWorkflowEngine.java:174) at org.apache.falcon.resource.AbstractSchedulableEntityManager.scheduleInternal(AbstractSchedulableEntityManager.java:102) at org.apache.falcon.resource.AbstractSchedulableEntityManager.schedule(AbstractSchedulableEntityManager.java:80) at org.apache.falcon.resource.SchedulableEntityManager.schedule(SchedulableEntityManager.java:283) at sun.reflect.GeneratedMethodAccessor165.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:49) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$11.doExecute(SchedulableEntityManagerProxy.java:552) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:841) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.schedule_aroundBody16(SchedulableEntityManagerProxy.java:555) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure17.run(SchedulableEntityManagerProxy.java:1) at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) at org.apache.falcon.aspect.AbstractFalconAspect.logAroundMonitored(AbstractFalconAspect.java:51) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.schedule(SchedulableEntityManagerProxy.java:536) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.schedule_aroundBody20(SchedulableEntityManagerProxy.java:581) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure21.run(SchedulableEntityManagerProxy.java:1) at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) at org.apache.falcon.aspect.AbstractFalconAspect.logAroundMonitored(AbstractFalconAspect.java:51) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule_aroundBody22(SchedulableEntityManagerProxy.java:581) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure23.run(SchedulableEntityManagerProxy.java:1) at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) at org.apache.falcon.aspect.AbstractFalconAspect.logAroundMonitored(AbstractFalconAspect.java:51) at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule(SchedulableEntityManagerProxy.java:572) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185) at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400) at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349) at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339) at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416) at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537) at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) at org.apache.falcon.security.FalconAuthorizationFilter.doFilter(FalconAuthorizationFilter.java:108) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.falcon.security.FalconCSRFFilter.doFilter(FalconCSRFFilter.java:78) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.falcon.security.FalconAuthenticationFilter$2.doFilter(FalconAuthenticationFilter.java:188) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:617) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:576) at org.apache.falcon.security.FalconAuthenticationFilter.doFilter(FalconAuthenticationFilter.java:197) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.falcon.security.FalconAuditFilter.doFilter(FalconAuditFilter.java:64) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.falcon.security.FalconXFrameOptionsFilter.doFilter(FalconXFrameOptionsFilter.java:56) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.falcon.security.HostnameFilter.doFilter(HostnameFilter.java:82) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) 2018-02-15 00:09:53,274 INFO - [290968357@qtp-57241990-276 - 64cb2640-bfde-48b7-a97a-70d016d74b98:hrt_qa:POST//entities/submitAndSchedule/feed] ~ Successfully released lock on (feed) A323e3eea-71bf9cf8 by 290968357@qtp-57241990-276 - 64cb2640-bfde-48b7-a97a-70d016d74b98 (MemoryLocks:70){noformat}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-02-19T11:23:00Z"
  },
  "patches": [],
  "external_id": "FALCON-2322"
},{
  "_id": {
    "$oid": "5f27ce05014d3531c6cc486c"
  },
  "message_id": "<900500894.1249030878531.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27cdf6014d3531c6cc43fe"
  },
  "from_id": {
    "$oid": "59bf92eef2a4565fe9e6b936"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (JCR-2236) Malformed excerpt if content contains\n markup and no highlights found",
  "body": "\n     [ https://issues.apache.org/jira/browse/JCR-2236?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMarcel Reutegger updated JCR-2236:\n----------------------------------\n\n       Resolution: Fixed\n    Fix Version/s: 2.0.0\n           Status: Resolved  (was: Patch Available)\n\nPatch applied in revision: 799550\n\n> Malformed excerpt if content contains markup and no highlights found\n> --------------------------------------------------------------------\n>\n>                 Key: JCR-2236\n>                 URL: https://issues.apache.org/jira/browse/JCR-2236\n>             Project: Jackrabbit Content Repository\n>          Issue Type: Bug\n>          Components: jackrabbit-core\n>            Reporter: Marcel Reutegger\n>            Priority: Minor\n>             Fix For: 2.0.0\n>\n>         Attachments: JCR-2236.patch\n>\n>\n> Any markup in content that is used in an excerpt is encoded with corresponding entity references. However, this process is broken when there are no highlights in the excerpt. In this case, the content is provided as is in the excerpt, which may lead to malformed HTML/XML.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-07-31T02:01:18Z"
  },
  "patches": [],
  "external_id": "JCR-2236"
},{
  "_id": {
    "$oid": "5c58057de078b00ec4e74fae"
  },
  "message_id": "<JIRA.13062104.1491485683000.306453.1492526261823@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c58056ce078b00ec4e74eb0"
    },
    {
      "$oid": "5c58056ce078b00ec4e74eb1"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c58056ce078b00ec4e74eb0"
  },
  "from_id": {
    "$oid": "5c58013e621a9a77b3bd62c3"
  },
  "to_ids": [
    {
      "$oid": "5bbdf31357674ee1677dbf18"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (RANGER-1499) Upgrade Tomcat version",
  "body": "\n    [ https://issues.apache.org/jira/browse/RANGER-1499?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15972805#comment-15972805 ] \n\nVelmurugan Periasamy commented on RANGER-1499:\n----------------------------------------------\n\n[~coheigea] - My only concern is 7.0.77 is released only couple of weeks back. But if you see specific reasons to upgrade to that latest version and there is no impact on functionality, then I am fine with that. \n\n> Upgrade Tomcat version\n> ----------------------\n>\n>                 Key: RANGER-1499\n>                 URL: https://issues.apache.org/jira/browse/RANGER-1499\n>             Project: Ranger\n>          Issue Type: Bug\n>          Components: Ranger\n>            Reporter: Velmurugan Periasamy\n>             Fix For: 0.6.4, 1.0.0, 0.7.1\n>\n>\n> Tomcat version used by Ranger & Ranger KMS is 7.0.68.\n> Need to upgrade to 7.0.73\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-04-18T14:37:41Z"
  },
  "patches": [],
  "external_id": "RANGER-1499"
},{
  "_id": {
    "$oid": "5bbe0e76272f7b1f6830cc3c"
  },
  "message_id": "<JIRA.13068471.1493740389000.308345.1511429040561@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e12272f7b1f6830bd59"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0e75272f7b1f6830cc18"
    },
    {
      "$oid": "5bbe0e75272f7b1f6830cc19"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0e75272f7b1f6830cc18"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "5bbe0e2757674ee1679173c5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PARQUET-970) Add Add Lz4 and Zstd compression\n codecs",
  "body": "\n    [ https://issues.apache.org/jira/browse/PARQUET-970?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16264047#comment-16264047 ] \n\nASF GitHub Bot commented on PARQUET-970:\n----------------------------------------\n\nxhochy closed pull request #419: PARQUET-970: Add Lz4 and Zstd compression codecs\nURL: https://github.com/apache/parquet-cpp/pull/419\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/CMakeLists.txt b/CMakeLists.txt\nindex c524ceb5..0183852c 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -562,7 +562,9 @@ else()\n       NOT DEFINED ENV{BROTLI_STATIC_LIB_DEC} OR\n       NOT DEFINED ENV{BROTLI_STATIC_LIB_COMMON} OR\n       NOT DEFINED ENV{SNAPPY_STATIC_LIB} OR\n-      NOT DEFINED ENV{ZLIB_STATIC_LIB})\n+      NOT DEFINED ENV{ZLIB_STATIC_LIB} OR\n+      NOT DEFINED ENV{LZ4_STATIC_LIB} OR\n+      NOT DEFINED ENV{ZSTD_STATIC_LIB})\n     message(FATAL_ERROR \"Missing transitive dependencies for Arrow static linking\")\n   endif()\n \n@@ -571,6 +573,8 @@ else()\n   set(BROTLI_STATIC_LIB_COMMON \"$ENV{BROTLI_STATIC_LIB_COMMON}\")\n   set(SNAPPY_STATIC_LIB \"$ENV{SNAPPY_STATIC_LIB}\")\n   set(ZLIB_STATIC_LIB \"$ENV{ZLIB_STATIC_LIB}\")\n+  set(LZ4_STATIC_LIB \"$ENV{LZ4_STATIC_LIB}\")\n+  set(ZSTD_STATIC_LIB \"$ENV{ZSTD_STATIC_LIB}\")\n \n   add_library(brotli_enc STATIC IMPORTED)\n   set_target_properties(brotli_enc PROPERTIES IMPORTED_LOCATION ${BROTLI_STATIC_LIB_ENC})\n@@ -582,6 +586,10 @@ else()\n   set_target_properties(snappy PROPERTIES IMPORTED_LOCATION ${SNAPPY_STATIC_LIB})\n   add_library(zlib STATIC IMPORTED)\n   set_target_properties(zlib PROPERTIES IMPORTED_LOCATION ${ZLIB_STATIC_LIB})\n+  add_library(lz4 STATIC IMPORTED)\n+  set_target_properties(lz4 PROPERTIES IMPORTED_LOCATION ${LZ4_STATIC_LIB})\n+  add_library(zstd STATIC IMPORTED)\n+  set_target_properties(zstd PROPERTIES IMPORTED_LOCATION ${ZSTD_STATIC_LIB})\n \n   set(TRANSITIVE_LINK_LIBS\n     snappy\n@@ -589,6 +597,8 @@ else()\n     brotli_enc\n     brotli_dec\n     brotli_common\n+    lz4\n+    zstd\n   )\n \n   set(ARROW_LINK_LIBS\ndiff --git a/ci/msvc-build.bat b/ci/msvc-build.bat\nindex 67df5651..29d8b839 100644\n--- a/ci/msvc-build.bat\n+++ b/ci/msvc-build.bat\n@@ -28,7 +28,9 @@ if NOT \"%CONFIGURATION%\" == \"Debug\" (\n )\n \n if \"%CONFIGURATION%\" == \"Toolchain\" (\n-  conda install -y boost-cpp=1.63 brotli=0.6.0 zlib=1.2.11 snappy=1.1.6 thrift-cpp=0.10.0 -c conda-forge\n+  conda install -y boost-cpp=1.63 thrift-cpp=0.10.0 ^\n+      brotli=0.6.0 zlib=1.2.11 snappy=1.1.6 lz4-c=1.7.5 zstd=1.2.0 ^\n+      -c conda-forge\n \n   set ARROW_BUILD_TOOLCHAIN=%MINICONDA%/Library\n   set PARQUET_BUILD_TOOLCHAIN=%MINICONDA%/Library\ndiff --git a/ci/travis_script_static.sh b/ci/travis_script_static.sh\nindex 29331e97..6da7a334 100755\n--- a/ci/travis_script_static.sh\n+++ b/ci/travis_script_static.sh\n@@ -62,6 +62,8 @@ export BROTLI_STATIC_LIB_ENC=$BROTLI_EP/libbrotlienc.a\n export BROTLI_STATIC_LIB_DEC=$BROTLI_EP/libbrotlidec.a\n export BROTLI_STATIC_LIB_COMMON=$BROTLI_EP/libbrotlicommon.a\n export ZLIB_STATIC_LIB=$ARROW_EP/zlib_ep/src/zlib_ep-install/lib/libz.a\n+export LZ4_STATIC_LIB=$ARROW_EP/lz4_ep-prefix/src/lz4_ep/lib/liblz4.a\n+export ZSTD_STATIC_LIB=$ARROW_EP/zstd_ep-prefix/src/zstd_ep/lib/libzstd.a\n \n cmake -DPARQUET_CXXFLAGS=\"$PARQUET_CXXFLAGS\" \\\n       -DPARQUET_TEST_MEMCHECK=ON \\\ndiff --git a/cmake_modules/ThirdpartyToolchain.cmake b/cmake_modules/ThirdpartyToolchain.cmake\nindex fe1d4999..386a3e19 100644\n--- a/cmake_modules/ThirdpartyToolchain.cmake\n+++ b/cmake_modules/ThirdpartyToolchain.cmake\n@@ -359,8 +359,8 @@ if (NOT ARROW_FOUND)\n     -DCMAKE_INSTALL_LIBDIR=${ARROW_LIB_DIR}\n     -DARROW_JEMALLOC=OFF\n     -DARROW_IPC=OFF\n-    -DARROW_WITH_LZ4=OFF\n-    -DARROW_WITH_ZSTD=OFF\n+    -DARROW_WITH_LZ4=ON\n+    -DARROW_WITH_ZSTD=ON\n     -DARROW_BUILD_SHARED=${PARQUET_BUILD_SHARED}\n     -DARROW_BOOST_USE_SHARED=${PARQUET_BOOST_USE_SHARED}\n     -DARROW_BUILD_TESTS=OFF)\ndiff --git a/src/parquet/column-io-benchmark.cc b/src/parquet/column-io-benchmark.cc\nindex c20d6e2a..ec7b52ef 100644\n--- a/src/parquet/column-io-benchmark.cc\n+++ b/src/parquet/column-io-benchmark.cc\n@@ -56,14 +56,16 @@ void SetBytesProcessed(::benchmark::State& state, Repetition::type repetition) {\n   state.SetBytesProcessed(state.iterations() * state.range(0) * sizeof(int16_t));\n }\n \n-template <Repetition::type repetition>\n+template <Repetition::type repetition,\n+          Compression::type codec = Compression::UNCOMPRESSED>\n static void BM_WriteInt64Column(::benchmark::State& state) {\n   format::ColumnChunk thrift_metadata;\n   std::vector<int64_t> values(state.range(0), 128);\n   std::vector<int16_t> definition_levels(state.range(0), 1);\n   std::vector<int16_t> repetition_levels(state.range(0), 0);\n   std::shared_ptr<ColumnDescriptor> schema = Int64Schema(repetition);\n-  std::shared_ptr<WriterProperties> properties = default_writer_properties();\n+  WriterProperties::Builder builder;\n+  std::shared_ptr<WriterProperties> properties = builder.compression(codec)->build();\n   auto metadata = ColumnChunkMetaDataBuilder::Make(\n       properties, schema.get(), reinterpret_cast<uint8_t*>(&thrift_metadata));\n \n@@ -84,6 +86,27 @@ BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::OPTIONAL)->Range(1024, 65536\n \n BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REPEATED)->Range(1024, 65536);\n \n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REQUIRED, Compression::SNAPPY)\n+    ->Range(1024, 65536);\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::OPTIONAL, Compression::SNAPPY)\n+    ->Range(1024, 65536);\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REPEATED, Compression::SNAPPY)\n+    ->Range(1024, 65536);\n+\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REQUIRED, Compression::LZ4)\n+    ->Range(1024, 65536);\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::OPTIONAL, Compression::LZ4)\n+    ->Range(1024, 65536);\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REPEATED, Compression::LZ4)\n+    ->Range(1024, 65536);\n+\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REQUIRED, Compression::ZSTD)\n+    ->Range(1024, 65536);\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::OPTIONAL, Compression::ZSTD)\n+    ->Range(1024, 65536);\n+BENCHMARK_TEMPLATE(BM_WriteInt64Column, Repetition::REPEATED, Compression::ZSTD)\n+    ->Range(1024, 65536);\n+\n std::unique_ptr<Int64Reader> BuildReader(std::shared_ptr<Buffer>& buffer,\n                                          int64_t num_values, ColumnDescriptor* schema) {\n   std::unique_ptr<InMemoryInputStream> source(new InMemoryInputStream(buffer));\n@@ -92,14 +115,16 @@ std::unique_ptr<Int64Reader> BuildReader(std::shared_ptr<Buffer>& buffer,\n   return std::unique_ptr<Int64Reader>(new Int64Reader(schema, std::move(page_reader)));\n }\n \n-template <Repetition::type repetition>\n+template <Repetition::type repetition,\n+          Compression::type codec = Compression::UNCOMPRESSED>\n static void BM_ReadInt64Column(::benchmark::State& state) {\n   format::ColumnChunk thrift_metadata;\n   std::vector<int64_t> values(state.range(0), 128);\n   std::vector<int16_t> definition_levels(state.range(0), 1);\n   std::vector<int16_t> repetition_levels(state.range(0), 0);\n   std::shared_ptr<ColumnDescriptor> schema = Int64Schema(repetition);\n-  std::shared_ptr<WriterProperties> properties = default_writer_properties();\n+  WriterProperties::Builder builder;\n+  std::shared_ptr<WriterProperties> properties = builder.compression(codec)->build();\n   auto metadata = ColumnChunkMetaDataBuilder::Make(\n       properties, schema.get(), reinterpret_cast<uint8_t*>(&thrift_metadata));\n \n@@ -134,6 +159,27 @@ BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::OPTIONAL)\n BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REPEATED)\n     ->RangePair(1024, 65536, 1, 1024);\n \n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REQUIRED, Compression::SNAPPY)\n+    ->RangePair(1024, 65536, 1, 1024);\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::OPTIONAL, Compression::SNAPPY)\n+    ->RangePair(1024, 65536, 1, 1024);\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REPEATED, Compression::SNAPPY)\n+    ->RangePair(1024, 65536, 1, 1024);\n+\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REQUIRED, Compression::LZ4)\n+    ->RangePair(1024, 65536, 1, 1024);\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::OPTIONAL, Compression::LZ4)\n+    ->RangePair(1024, 65536, 1, 1024);\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REPEATED, Compression::LZ4)\n+    ->RangePair(1024, 65536, 1, 1024);\n+\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REQUIRED, Compression::ZSTD)\n+    ->RangePair(1024, 65536, 1, 1024);\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::OPTIONAL, Compression::ZSTD)\n+    ->RangePair(1024, 65536, 1, 1024);\n+BENCHMARK_TEMPLATE(BM_ReadInt64Column, Repetition::REPEATED, Compression::ZSTD)\n+    ->RangePair(1024, 65536, 1, 1024);\n+\n static void BM_RleEncoding(::benchmark::State& state) {\n   std::vector<int16_t> levels(state.range(0), 0);\n   int64_t n = 0;\ndiff --git a/src/parquet/column_writer-test.cc b/src/parquet/column_writer-test.cc\nindex 3e4c04f9..681f022b 100644\n--- a/src/parquet/column_writer-test.cc\n+++ b/src/parquet/column_writer-test.cc\n@@ -307,6 +307,16 @@ TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithGzipCompression) {\n                                  LARGE_SIZE);\n }\n \n+TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithLz4Compression) {\n+  this->TestRequiredWithSettings(Encoding::PLAIN, Compression::LZ4, false, false,\n+                                 LARGE_SIZE);\n+}\n+\n+TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithZstdCompression) {\n+  this->TestRequiredWithSettings(Encoding::PLAIN, Compression::ZSTD, false, false,\n+                                 LARGE_SIZE);\n+}\n+\n TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithStats) {\n   this->TestRequiredWithSettings(Encoding::PLAIN, Compression::UNCOMPRESSED, false, true,\n                                  LARGE_SIZE);\n@@ -327,6 +337,16 @@ TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithStatsAndGzipCompression) {\n                                  LARGE_SIZE);\n }\n \n+TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithStatsAndLz4Compression) {\n+  this->TestRequiredWithSettings(Encoding::PLAIN, Compression::LZ4, false, true,\n+                                 LARGE_SIZE);\n+}\n+\n+TYPED_TEST(TestPrimitiveWriter, RequiredPlainWithStatsAndZstdCompression) {\n+  this->TestRequiredWithSettings(Encoding::PLAIN, Compression::ZSTD, false, true,\n+                                 LARGE_SIZE);\n+}\n+\n TYPED_TEST(TestPrimitiveWriter, Optional) {\n   // Optional and non-repeated, with definition levels\n   // but no repetition levels\ndiff --git a/src/parquet/file/file-deserialize-test.cc b/src/parquet/file/file-deserialize-test.cc\nindex 39ea1d9b..0cab75f1 100644\n--- a/src/parquet/file/file-deserialize-test.cc\n+++ b/src/parquet/file/file-deserialize-test.cc\n@@ -178,8 +178,9 @@ TEST_F(TestPageSerde, TestFailLargePageHeaders) {\n }\n \n TEST_F(TestPageSerde, Compression) {\n-  Compression::type codec_types[3] = {Compression::GZIP, Compression::SNAPPY,\n-                                      Compression::BROTLI};\n+  Compression::type codec_types[5] = {Compression::GZIP, Compression::SNAPPY,\n+                                      Compression::BROTLI, Compression::LZ4,\n+                                      Compression::ZSTD};\n \n   const int32_t num_rows = 32;  // dummy value\n   data_page_header_.num_values = num_rows;\ndiff --git a/src/parquet/file/file-serialize-test.cc b/src/parquet/file/file-serialize-test.cc\nindex 75f3fbdf..f9f12bea 100644\n--- a/src/parquet/file/file-serialize-test.cc\n+++ b/src/parquet/file/file-serialize-test.cc\n@@ -209,7 +209,17 @@ TYPED_TEST(TestSerialize, SmallFileBrotli) {\n   this->FileSerializeTest(Compression::BROTLI);\n }\n \n-TYPED_TEST(TestSerialize, SmallFileGzip) { this->FileSerializeTest(Compression::GZIP); }\n+TYPED_TEST(TestSerialize, SmallFileGzip) {\n+  this->FileSerializeTest(Compression::GZIP);\n+}\n+\n+TYPED_TEST(TestSerialize, SmallFileLz4) {\n+  this->FileSerializeTest(Compression::LZ4);\n+}\n+\n+TYPED_TEST(TestSerialize, SmallFileZstd) {\n+  this->FileSerializeTest(Compression::ZSTD);\n+}\n \n }  // namespace test\n \ndiff --git a/src/parquet/parquet.thrift b/src/parquet/parquet.thrift\nindex f774d492..a72ef2ca 100644\n--- a/src/parquet/parquet.thrift\n+++ b/src/parquet/parquet.thrift\n@@ -363,6 +363,8 @@ enum CompressionCodec {\n   GZIP = 2;\n   LZO = 3;\n   BROTLI = 4;\n+  LZ4 = 5;\n+  ZSTD = 6;\n }\n \n enum PageType {\ndiff --git a/src/parquet/properties-test.cc b/src/parquet/properties-test.cc\nindex c48fc34a..4a063c1e 100644\n--- a/src/parquet/properties-test.cc\n+++ b/src/parquet/properties-test.cc\n@@ -46,12 +46,14 @@ TEST(TestWriterProperties, Basics) {\n TEST(TestWriterProperties, AdvancedHandling) {\n   WriterProperties::Builder builder;\n   builder.compression(\"gzip\", Compression::GZIP);\n+  builder.compression(\"zstd\", Compression::ZSTD);\n   builder.compression(Compression::SNAPPY);\n   builder.encoding(Encoding::DELTA_BINARY_PACKED);\n   builder.encoding(\"delta-length\", Encoding::DELTA_LENGTH_BYTE_ARRAY);\n   std::shared_ptr<WriterProperties> props = builder.build();\n \n   ASSERT_EQ(Compression::GZIP, props->compression(ColumnPath::FromDotString(\"gzip\")));\n+  ASSERT_EQ(Compression::ZSTD, props->compression(ColumnPath::FromDotString(\"zstd\")));\n   ASSERT_EQ(Compression::SNAPPY,\n             props->compression(ColumnPath::FromDotString(\"delta-length\")));\n   ASSERT_EQ(Encoding::DELTA_BINARY_PACKED,\ndiff --git a/src/parquet/types.cc b/src/parquet/types.cc\nindex 0652c6a8..8ec3f3b1 100644\n--- a/src/parquet/types.cc\n+++ b/src/parquet/types.cc\n@@ -108,6 +108,12 @@ std::string CompressionToString(Compression::type t) {\n     case Compression::LZO:\n       return \"LZO\";\n       break;\n+    case Compression::LZ4:\n+      return \"LZ4\";\n+      break;\n+    case Compression::ZSTD:\n+      return \"ZSTD\";\n+      break;\n     default:\n       return \"UNKNOWN\";\n       break;\ndiff --git a/src/parquet/types.h b/src/parquet/types.h\nindex 53b33d56..a8109449 100644\n--- a/src/parquet/types.h\n+++ b/src/parquet/types.h\n@@ -109,7 +109,7 @@ struct Encoding {\n \n // Compression, mirrors parquet::CompressionCodec\n struct Compression {\n-  enum type { UNCOMPRESSED, SNAPPY, GZIP, LZO, BROTLI };\n+  enum type { UNCOMPRESSED, SNAPPY, GZIP, LZO, BROTLI, LZ4, ZSTD };\n };\n \n // parquet::PageType\ndiff --git a/src/parquet/util/memory.h b/src/parquet/util/memory.h\nindex 94b86c1a..a28917bd 100644\n--- a/src/parquet/util/memory.h\n+++ b/src/parquet/util/memory.h\n@@ -57,6 +57,12 @@ static inline std::unique_ptr<::arrow::Codec> GetCodecFromArrow(Compression::typ\n     case Compression::BROTLI:\n       PARQUET_THROW_NOT_OK(::arrow::Codec::Create(::arrow::Compression::BROTLI, &result));\n       break;\n+    case Compression::LZ4:\n+      PARQUET_THROW_NOT_OK(::arrow::Codec::Create(::arrow::Compression::LZ4, &result));\n+      break;\n+    case Compression::ZSTD:\n+      PARQUET_THROW_NOT_OK(::arrow::Codec::Create(::arrow::Compression::ZSTD, &result));\n+      break;\n     default:\n       break;\n   }\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n\n\n> Add Add Lz4 and Zstd compression codecs\n> ---------------------------------------\n>\n>                 Key: PARQUET-970\n>                 URL: https://issues.apache.org/jira/browse/PARQUET-970\n>             Project: Parquet\n>          Issue Type: New Feature\n>          Components: parquet-cpp\n>            Reporter: Xianjin YE\n>            Assignee: Xianjin YE\n>             Fix For: cpp-1.4.0\n>\n>\n> https://github.com/facebook/zstd looks quite promising, I'd like to add a compressor in parquet-cpp.\n> Lz4 and Zstd codecs are added as parquet-format has already added these codecs.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-11-23T09:24:00Z"
  },
  "patches": [],
  "external_id": "PARQUET-970"
},{
  "_id": {
    "$oid": "5f27cecd442ab9b9860f43c4"
  },
  "message_id": "<JIRA.12729311.1406161370434.61489.1407823032553@arcas>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ce96442ab9b9860f33e9"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ce96442ab9b9860f33e9"
  },
  "from_id": {
    "$oid": "5bacb0ba57674ee167d4aa49"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (OOZIE-1950) Coordinator job info should support\n timestamp (nominal time)",
  "body": "\n    [ https://issues.apache.org/jira/browse/OOZIE-1950?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14093776#comment-14093776 ] \n\nRohini Palaniswamy commented on OOZIE-1950:\n-------------------------------------------\n\n These need to be fixed :\n-1 the patch contains 1 line(s) with trailing spaces\nhttps://builds.apache.org/job/oozie-trunk-precommit-build/1521/testReport/junit/org.apache.oozie/TestCoordinatorEngine/testEngine/ -    -   \n\n> Coordinator job info should support timestamp (nominal time)\n> ------------------------------------------------------------\n>\n>                 Key: OOZIE-1950\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-1950\n>             Project: Oozie\n>          Issue Type: Bug\n>            Reporter: Purshotam Shah\n>            Assignee: Shwetha G S\n>             Fix For: 4.1.0\n>\n>         Attachments: OOZIE-1950-v2.patch, OOZIE-1950.patch\n>\n>\n> It will be useful for user to list all actions (with length and offset) after/before nominal time.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-08-12T05:57:12Z"
  },
  "patches": [],
  "external_id": "OOZIE-1950"
},{
  "_id": {
    "$oid": "5f27d049532b7277349c2af1"
  },
  "message_id": "<18592411.48801291219993051.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5f27ceceaf02e2d6de86e7e7"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (AMQNET-294) durable subscription message loss\n when master broker fails to slave",
  "body": "\n    [ https://issues.apache.org/jira/browse/AMQNET-294?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12965731#action_12965731 ] \n\nTimothy Bish commented on AMQNET-294:\n-------------------------------------\n\nI haven't been able to look into this yet.  What I would do to start would be to run a similar test in java with the trace option set on the uri and compare it to what the .NET client is sending to see if we are not sending something in an OpenWire message that we should be. \n\n> durable subscription message loss when master broker fails to slave\n> -------------------------------------------------------------------\n>\n>                 Key: AMQNET-294\n>                 URL: https://issues.apache.org/jira/browse/AMQNET-294\n>             Project: ActiveMQ .Net\n>          Issue Type: Bug\n>          Components: NMS\n>    Affects Versions: 1.4.1\n>         Environment: Windows 7 (client), Windows Server 2008 64-bit (server brokers run on), Sql Server 2008 (database)\n>            Reporter: Mark Gellings\n>            Assignee: Jim Gomes\n>             Fix For: 1.5.0\n>\n>         Attachments: Apache.NMS.Test.zip\n>\n>\n> We are seeing message loss on a durable subscription when using NMS ActiveMQ v1.4.1 and ActiveMQ v5.4.1.\n> Please run the included NUnit test and watch the console output.  When it says \"Failover the broker now!\" do as it says.  About 75% of the time less than half of the expected 250 messages come through.\n> Using version 1.1 of NMS the majority of the time the test passes.  I have seen it fail only a few times with this earlier version, and when it does there are only a couple messages that don't come through.\n> In the zip file will be the unit test, and a config directory containing the master and slave activemq configurations.  We are using JDBC master/slave.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-12-01T11:13:13Z"
  },
  "patches": [],
  "external_id": "AMQNET-294"
},{
  "_id": {
    "$oid": "5f27ce55442ab9b9860f20ae"
  },
  "message_id": "<JIRA.12651478.1370553678000.6810.1461318072963@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ce24442ab9b9860f124a"
    },
    {
      "$oid": "5f27ce24442ab9b9860f124b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ce24442ab9b9860f124a"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a6"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (OOZIE-1402) Increase retry interval for\n non-progressing coordinator action with fix value",
  "body": "\n    [ https://issues.apache.org/jira/browse/OOZIE-1402?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15253628#comment-15253628 ] \n\nHadoop QA commented on OOZIE-1402:\n----------------------------------\n\nTesting JIRA OOZIE-1402\n\nCleaning local git workspace\n\n----------------------------\n\n{color:green}+1 PATCH_APPLIES{color}\n{color:green}+1 CLEAN{color}\n{color:red}-1 RAW_PATCH_ANALYSIS{color}\n.    {color:green}+1{color} the patch does not introduce any @author tags\n.    {color:green}+1{color} the patch does not introduce any tabs\n.    {color:green}+1{color} the patch does not introduce any trailing spaces\n.    {color:green}+1{color} the patch does not introduce any line longer than 132\n.    {color:red}-1{color} the patch does not add/modify any testcase\n{color:green}+1 RAT{color}\n.    {color:green}+1{color} the patch does not seem to introduce new RAT warnings\n{color:green}+1 JAVADOC{color}\n.    {color:green}+1{color} the patch does not seem to introduce new Javadoc warnings\n{color:green}+1 COMPILE{color}\n.    {color:green}+1{color} HEAD compiles\n.    {color:green}+1{color} patch compiles\n.    {color:green}+1{color} the patch does not seem to introduce new javac warnings\n{color:green}+1 BACKWARDS_COMPATIBILITY{color}\n.    {color:green}+1{color} the patch does not change any JPA Entity/Colum/Basic/Lob/Transient annotations\n.    {color:green}+1{color} the patch does not modify JPA files\n{color:red}-1 TESTS{color}\n.    Tests run: 1777\n.    Tests failed: 4\n.    Tests errors: 0\n\n.    The patch failed the following testcases:\n\n.      testMemoryUsageAndSpeed(org.apache.oozie.service.TestPartitionDependencyManagerEhcache)\n.      testNone(org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand)\n.      testMemoryUsageAndSpeed(org.apache.oozie.service.TestPartitionDependencyManagerService)\n.      testbulkWfKillSuccess(org.apache.oozie.command.wf.TestBulkWorkflowXCommand)\n\n{color:green}+1 DISTRO{color}\n.    {color:green}+1{color} distro tarball builds with the patch \n\n----------------------------\n{color:red}*-1 Overall result, please check the reported -1(s)*{color}\n\n\nThe full output of the test-patch run is available at\n\n.   https://builds.apache.org/job/oozie-trunk-precommit-build/2845/\n\n> Increase retry interval for non-progressing coordinator action with fix value\n> -----------------------------------------------------------------------------\n>\n>                 Key: OOZIE-1402\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-1402\n>             Project: Oozie\n>          Issue Type: Improvement\n>          Components: core\n>    Affects Versions: trunk\n>            Reporter: Mona Chitnis\n>            Assignee: Satish Subhashrao Saley\n>            Priority: Minor\n>             Fix For: trunk\n>\n>         Attachments: OOZIE-1402-1.patch, OOZIE-1402-2.patch, OOZIE-1402-3.patch\n>\n>\n> Currently every coordinator action is retried to check data directory in the\n> next minute. \n> We could make it better by waiting longer for coordinator action that is not\n> progressing (i.e. find no new directory) for repeated retries\n> The waiting time should start from 1 minute for X retries. Then the action\n> should wait for 2 minutes. After X retries it should wait for 3. The same way\n> it will go to some max-wait-time and stay there until timeout\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-04-22T09:41:12Z"
  },
  "patches": [],
  "external_id": "OOZIE-1402"
},{
  "_id": {
    "$oid": "5bbdaac8c764eb697f536f73"
  },
  "message_id": "<6621F7A1-E56E-4B2D-8C24-03CDD1037C29@objectstyle.org>",
  "mailing_list_id": {
    "$oid": "5bbdaa4fc764eb697f536362"
  },
  "reference_ids": [
    {
      "$oid": "5bbdaac8c764eb697f536f72"
    },
    {
      "$oid": "5bbdaac8c764eb697f536f70"
    },
    {
      "$oid": "5bbdaac8c764eb697f536f6f"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdaac8c764eb697f536f72"
  },
  "from_id": {
    "$oid": "58c8dba1e4f89451f55a4898"
  },
  "to_ids": [
    {
      "$oid": "5bbdaa7557674ee167cfd9b3"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [jira] [Created] (CAY-1576) PK Generation with meaninful",
  "body": "+1 \n\nSubtask is important. We haven't used them at Cayenne but my job I am using them a lot to organize complex requirements into development work items.\n\nAndrus\n\nOn Jun 1, 2011, at 6:21 PM, Aristedes Maniatis wrote:\n\n> On 1/06/11 9:11 PM, Andrus Adamchik wrote:\n>> I think we should disable these arbitrary issue types. Jira is really not a support forum.\n> \n> I'll deal with this now and just leave:\n> \n> * task\n> * improvement\n> * bug\n> * subtask  (I have to leave this one otherwise Jira will complain)\n> \n> I'll ditch all the others and merge improvement and new feature.\n> \n> Ari\n> \n> -- \n> -------------------------->\n> Aristedes Maniatis\n> GPG fingerprint CBFB 84B4 738D 4E87 5E5C  5EFA EF6A 7D2E 3E49 102A\n> \n\n",
  "date": {
    "$date": "2011-06-01T18:38:26Z"
  },
  "patches": [],
  "external_id": "CAY-1576"
},{
  "_id": {
    "$oid": "5f27cf2e532b7277349bdbb0"
  },
  "message_id": "<JIRA.12694481.1392121056631.23981.1392132439795@arcas>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [
    {
      "$oid": "5f27cf1b532b7277349bd641"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cf1b532b7277349bd641"
  },
  "from_id": {
    "$oid": "5f27ceb5af02e2d6de8602e0"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (AMQ-5041) JMSClientTest is hanging",
  "body": "\n     [ https://issues.apache.org/jira/browse/AMQ-5041?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTimothy Bish resolved AMQ-5041.\n-------------------------------\n\n    Resolution: Fixed\n\n> JMSClientTest is hanging\n> ------------------------\n>\n>                 Key: AMQ-5041\n>                 URL: https://issues.apache.org/jira/browse/AMQ-5041\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: Test Cases\n>            Reporter: Kevin Earls\n>\n> The JMSClientTest is currently hanging on multiple test cases.  I'll attach a stack trace, but it looks like the test is hanging during the broker.stop in tearDown().\n> I'm going to add @Ignore tags to most of the tests so this stops hanging CI builds.  \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-11T15:27:19Z"
  },
  "patches": [],
  "external_id": "AMQ-5041"
},{
  "_id": {
    "$oid": "5bc8617257a11257de56596d"
  },
  "message_id": "<JIRA.12758393.1417209812000.187319.1436902805545@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc860bb57a11257de565363"
    },
    {
      "$oid": "5bc860bb57a11257de565362"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc860bb57a11257de565362"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-2530) Improve PDFDebugger",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-2530?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14626928#comment-14626928 ] \n\nTilman Hausherr commented on PDFBOX-2530:\n-----------------------------------------\n\nHow about checking if there is a mask, and offer both? I.e. with the mask as default, and optionally image without mask.\n\nRe printStackTrace - I agree, as long as the exception appears at some time.\n\n> Improve PDFDebugger\n> -------------------\n>\n>                 Key: PDFBOX-2530\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-2530\n>             Project: PDFBox\n>          Issue Type: Improvement\n>          Components: Utilities\n>    Affects Versions: 1.8.8, 2.0.0\n>            Reporter: Tilman Hausherr\n>            Assignee: khyrul bashar\n>              Labels: gsoc2015\n>         Attachments: Avoiding_NPE_for_null_Field_Type.diff, BracketsColorChooser.png, Class_cast_exception_in_page_mode_avoided.diff, DeviceNCS.diff, FlagBitsPane-26-06-2015.diff, Flag_bits_showing_feature-redesigned.diff, Flag_bits_showing_feature.diff, K4SystemFontsNotEmbeded218.pdf, PDFDebugger_StatusBar.png, PDFDebugger_StatusBar_01.png, Parent_dictionary_type_checking_for__f__and__flags.diff, Sonarqube_warning_resolved.diff, Stream_Showing_Feature.diff, indexedcs.diff, openSelectedPath.diff, parent_node_redirect.diff, parent_node_redirect_expand_disabled.diff, removed_redundant_codes.patch, separationCS.diff, sonarqube_warning_resolve.diff, tree.diff, treestatus.diff, treestatuspane.diff\n>\n>\n> (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])\n> Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:\n>    - hex view\n>    - view of non printable characters\n>    - ✓ saving streams\n>    - binary copy & paste\n>    - ✓ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)\n>    - ✓ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)\n>    - ✓ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string\n>    - ✓ ability to search in streams (very useful for content streams and meta data)\n>    - ✓ show images that are streams\n>    - ✓ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves\n>    - ✓ show PDSeparation color\n>    - ✓ show PDDeviceN colors\n>    - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. \"appearance stream\" when hovering over /AP\n>    - show font encodings and characters\n>    - ✓ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once\n>    - edit attributes (should be possible to enter values as decimal, hex or binary)\n>    - edit streams, while keeping or changing the compression filter\n>    - save altered PDF \n>    - color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the \"bracketing\" of these operators, i.e. understand where a sequence starts and where it ends. (See \"operator summary\" in the PDF Spec) Other \"important\" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.\n> To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].\n> I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for \"PDFDebugger\".\n> Prerequisites:\n> - java programming, especially the GUI components\n> - the ability to understand existing source code\n> Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.\n> Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).\n> Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].\n> Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2015-07-14T19:40:05Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2530"
},{
  "_id": {
    "$oid": "5bea9a249e73d744d4121673"
  },
  "message_id": "<1983755471.5733.1310474580050.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9a1f9e73d744d41215a8"
  },
  "from_id": {
    "$oid": "5bea9a0135e3ea2b7b52b981"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (DERBY-5311) Convert tools/ij4.sql  to junit",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-5311?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nHoux Zhang closed DERBY-5311.\n-----------------------------\n\n\nThanks.\n\n> Convert tools/ij4.sql  to junit\n> -------------------------------\n>\n>                 Key: DERBY-5311\n>                 URL: https://issues.apache.org/jira/browse/DERBY-5311\n>             Project: Derby\n>          Issue Type: Sub-task\n>          Components: Test\n>            Reporter: Houx Zhang\n>            Assignee: Houx Zhang\n>              Labels: gsoc2011\n>             Fix For: 10.9.0.0\n>\n>         Attachments: 5311-1.patch, 5311-1.stat, 5311-2.patch, 5311-2.stat, 5311-followup.txt, noSecMgr.txt, resetWidth.txt, sysprop.diff\n>\n>\n\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-07-12T12:43:00Z"
  },
  "patches": [],
  "external_id": "DERBY-5311"
},{
  "_id": {
    "$oid": "5bea9b639e73d744d4125895"
  },
  "message_id": "<54135579.1253036037577.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9b639e73d744d4125884"
  },
  "from_id": {
    "$oid": "5bea97f835e3ea2b7b4e2a04"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Assigned: (DERBY-4375) WEME6.2 : Handful of test fixtures in\n InbetweenTest are failing with java.sql.SQLException: The syntax of the\n string representation of a datetime value is incorrect.",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-4375?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMamta A. Satoor reassigned DERBY-4375:\n--------------------------------------\n\n    Assignee: Mamta A. Satoor\n\n> WEME6.2 : Handful of test fixtures in InbetweenTest are failing with java.sql.SQLException: The syntax of the string representation of a datetime value is incorrect.\n> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-4375\n>                 URL: https://issues.apache.org/jira/browse/DERBY-4375\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Test\n>    Affects Versions: 10.6.0.0\n>         Environment: weme 6.2 \n>            Reporter: Mamta A. Satoor\n>            Assignee: Mamta A. Satoor\n>\n> On weme6.2, 8 test fixtures from InBetweenTest are failing with following error\n> java.sql.SQLException: The syntax of the string representation of a datetime value is incorrect.\n> The failing test fixtures are \n> 1)testReproductionBeetle5135\n> 2)testBeetle4316\n> 3)testNestedQueries\n> 4)testCheckQueries\n> 5)testBigInList\n> 6)testInBetween\n> 7)testInList\n> 8)testBetween\n> The stack trace for testBetween looks as follows\n> 1) testBetween(org.apache.derbyTesting.functionTests.tests.lang.InbetweenTest)java.sql.SQLException: The syntax of the string representation of a datetime value is incorrect.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)\n> \tat org.apache.derby.impl.jdbc.EmbedStatement.executeUpdate(Unknown Source)\n> \tat org.apache.derbyTesting.functionTests.tests.lang.InbetweenTest.createTestObjects(InbetweenTest.java:195)\n> \tat org.apache.derbyTesting.functionTests.tests.lang.InbetweenTest.testBetween(InbetweenTest.java:214)\n> \tat java.lang.reflect.AccessibleObject.invokeV(AccessibleObject.java:195)\n> \tat org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:109)\n> \tat junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)\n> \tat junit.extensions.TestSetup$1.protect(TestSetup.java:19)\n> \tat junit.extensions.TestSetup.run(TestSetup.java:23)\n> \tat org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)\n> \tat junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)\n> \tat junit.extensions.TestSetup$1.protect(TestSetup.java:19)\n> \tat junit.extensions.TestSetup.run(TestSetup.java:23)\n> \tat org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)\n> Caused by: ERROR 22007: The syntax of the string representation of a datetime value is incorrect.\n> \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n> \tat org.apache.derby.iapi.types.DateTimeParser.parseInt(Unknown Source)\n> \tat org.apache.derby.iapi.types.SQLTimestamp.parseDateOrTimestamp(Unknown Source)\n> \tat org.apache.derby.iapi.types.SQLTimestamp.parseTimestamp(Unknown Source)\n> \tat org.apache.derby.iapi.types.SQLTimestamp.<init>(Unknown Source)\n> \tat org.apache.derby.iapi.types.SQLChar.getTimestamp(Unknown Source)\n> \tat org.apache.derby.iapi.types.SQLChar.getTimestamp(Unknown Source)\n> \tat org.apache.derby.iapi.types.SQLTimestamp.setFrom(Unknown Source)\n> \tat org.apache.derby.iapi.types.DataType.setValue(Unknown Source)\n> \tat org.apache.derby.iapi.types.DataType.normalize(Unknown Source)\n> \tat org.apache.derby.iapi.types.DataTypeDescriptor.normalize(Unknown Source)\n> \tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeColumn(Unknown Source)\n> \tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(Unknown Source)\n> \tat org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(Unknown Source)\n> \tat org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(Unknown Source)\n> \tat org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)\n> \tat org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)\n> \tat org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)\n> \t... 39 more\n>  \n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-09-15T10:33:57Z"
  },
  "patches": [],
  "external_id": "DERBY-4375"
},{
  "_id": {
    "$oid": "5f27bea0a7dc6ca79d80eb4b"
  },
  "message_id": "<325684756.7920.1337695180862.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5f27bd66a7dc6ca79d809788"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27bea0a7dc6ca79d80eb45"
  },
  "from_id": {
    "$oid": "59677c39aff2204b3cbd1571"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c04"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (HTTPCORE-263) IndexOutOfBoundsException thrown\n in AbstractSessionInputBuffer.readLine()",
  "body": "\n    [ https://issues.apache.org/jira/browse/HTTPCORE-263?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13280966#comment-13280966 ] \n\nMichael Pujos commented on HTTPCORE-263:\n----------------------------------------\n\nI reread the http spec and CRLF is used t separate lines so forget my comment on it.\n\nThinking of a situation corresponding to the above schema is harder than I thought. Since this exception happens when parsing the request header line (parseHead()), it cannot happen \non the initial client request. So it happens when the AbstractSessionInputubbfer is used to handle a subsequent request from the same client using the same connection.\n\nApp handle UPnp specific requests with SUBSCRIBE, UNSUBSCRIBE, NOTIFY methods mixed with regular GET and POST methods\nSUBSCRIBE AND UNSUBSCIRBE do not have a body and the spec says: \"No body for request with method SUBSCRIBE, but note that the message MUST have a blank line following the last HTTP header\nfield\"\n\nI think this situation may happen with malformed successive requests although I cannot think of an example\n\n\n\n\n\n\n\n                \n> IndexOutOfBoundsException thrown in AbstractSessionInputBuffer.readLine()\n> -------------------------------------------------------------------------\n>\n>                 Key: HTTPCORE-263\n>                 URL: https://issues.apache.org/jira/browse/HTTPCORE-263\n>             Project: HttpComponents HttpCore\n>          Issue Type: Bug\n>          Components: HttpCore\n>    Affects Versions: 4.1.1\n>         Environment: reported on Android  2.3.3 using repackaged httpcore 4.1.1, optimized and obfuscated with Proguard\n>            Reporter: Michael Pujos\n>            Priority: Minor\n>             Fix For: 4.2.1\n>\n>\n> I've got the exception below reported in my Android app using (repackaged) httpcore 4.1.1:\n> java.lang.IndexOutOfBoundsException: off: 1088 len: -1 b.length: 8192\n> \tat org.apache.http.util.CharArrayBuffer.append(SourceFile:185)\n> \tat org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(SourceFile:251)\n> \tat org.apache.http.impl.io.HttpRequestParser.parseHead(SourceFile:90)\n> \tat org.apache.http.impl.io.AbstractMessageParser.parseHead(SourceFile:252)\n>                                                                  parse\n> \tat org.apache.http.impl.AbstractHttpServerConnection.receiveRequestHeader(SourceFile:242)\n> \tat org.apache.http.protocol.HttpService.handleRequest(SourceFile:238)\n> \tat org.teleal.cling.transport.impl.apache.HttpServerConnectionUpnpStream.run(SourceFile:116)\n> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1088)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:581)\n> \tat java.lang.Thread.run(Thread.java:1019)\n> It seems to be very rare. Stack trace line number (185) in AbstractSessionInputBuffer doesn't exaclty match the exact line number of the offending append() call (probably due to Proguard).\n> However,  there are 2 append() calls in readLine(), and it looks like one of them is called with len = -1, triggering the IndexOutOfBoundsException in append()\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@hc.apache.org\nFor additional commands, e-mail: dev-help@hc.apache.org\n\n",
  "date": {
    "$date": "2012-05-22T13:59:40Z"
  },
  "patches": [],
  "external_id": "HTTPCORE-263"
},{
  "_id": {
    "$oid": "5bbdacd1c764eb6c7a2650b8"
  },
  "message_id": "<JIRA.12747233.1412930839000.397125.1415009914212@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdaac7c764eb6c7a26059e"
    },
    {
      "$oid": "5bbdacd1c764eb6c7a2650b7"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdaac7c764eb6c7a26059e"
  },
  "from_id": {
    "$oid": "5bacb12f57674ee167d58a1d"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (FALCON-795) Maven Enforcer plugin is throwing\n error while building Falcon.",
  "body": "\n    [ https://issues.apache.org/jira/browse/FALCON-795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14194405#comment-14194405 ] \n\nSrikanth Sundarrajan commented on FALCON-795:\n---------------------------------------------\n\n+1\n\n> Maven Enforcer plugin is throwing error while building Falcon.\n> --------------------------------------------------------------\n>\n>                 Key: FALCON-795\n>                 URL: https://issues.apache.org/jira/browse/FALCON-795\n>             Project: Falcon\n>          Issue Type: Sub-task\n>            Reporter: Peeyush Bishnoi\n>            Assignee: Peeyush Bishnoi\n>            Priority: Minor\n>             Fix For: 0.6\n>\n>         Attachments: FALCON-795.patch\n>\n>\n> When building Falcon, maven enforcer plugin is throwing error when we mention Oozie versions like 4.1.3 or 4.0.1-863 . \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-11-03T10:18:34Z"
  },
  "patches": [],
  "external_id": "FALCON-795"
},{
  "_id": {
    "$oid": "60fac499d907ab79037eedd9"
  },
  "message_id": "<1045157984.1240260647415.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac47ad907ab79037ee4f3"
  },
  "from_id": {
    "$oid": "5f27c1d6af02e2d6de6e05b2"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Resolved: (DIRSTUDIO-483) DN Editor escapes all non-ascii\n characters",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSTUDIO-483?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nStefan Seelmann resolved DIRSTUDIO-483.\n---------------------------------------\n\n    Resolution: Fixed\n\nFixed here:\n  http://svn.apache.org/viewvc?rev=766865&view=rev\n\n\n> DN Editor escapes all non-ascii characters\n> ------------------------------------------\n>\n>                 Key: DIRSTUDIO-483\n>                 URL: https://issues.apache.org/jira/browse/DIRSTUDIO-483\n>             Project: Directory Studio\n>          Issue Type: Bug\n>          Components: studio-ldapbrowser\n>    Affects Versions: 1.4.0\n>            Reporter: Stefan Seelmann\n>            Assignee: Stefan Seelmann\n>             Fix For: 1.5.0\n>\n>\n> When I edit the member attribute of a groupOfNames entry and I select an DN that contains non-ascii characters these characters are escaped. \n> Example: uid=rédacteur1,dc=example,dc=com is transformed to uid=r\\C3\\A9dacteur1,dc=example,dc=com\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-04-20T13:50:47Z"
  },
  "patches": [],
  "external_id": "DIRSTUDIO-483"
},{
  "_id": {
    "$oid": "5f27cee7442ab9b9860f4aec"
  },
  "message_id": "<JIRA.12722534.1403197298799.9709.1403197345225@arcas>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ce8a442ab9b9860f2fc3"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ce8a442ab9b9860f2fc3"
  },
  "from_id": {
    "$oid": "5f27c476af02e2d6de74ce96"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (OOZIE-1890) Make oozie-site empty and reconcile\n defaults between oozie-default and the code",
  "body": "Robert Kanter created OOZIE-1890:\n------------------------------------\n\n             Summary: Make oozie-site empty and reconcile defaults between oozie-default and the code\n                 Key: OOZIE-1890\n                 URL: https://issues.apache.org/jira/browse/OOZIE-1890\n             Project: Oozie\n          Issue Type: Bug\n          Components: core\n            Reporter: Robert Kanter\n\n\nAs shwetha and puru suggested in OOZIE-1888, oozie-site vs oozie-default is a point of confusion for users.  We've also had issues in the past where they've had different values from each other and/or from the code's default (i.e. {{conf.get(PROP_NAME, DEFAULT_VALUE)}}).\n\nWe should make oozie-default the only source of truth by:\n# Putting all configuration properties in oozie-default.\n# Making oozie-site empty; if the user wants to change a property, they can copy it out of the for-reference oozie-default.\n# Getting rid of the code defaults.  It's easy for these to be out of sync with oozie-default, leading to confusion.  They aren't used anyway because oozie-default should always be there (and will now have every property)\n\nThis will require looking through all classes to make sure we're not missing anything from oozie-default and also checking that we put the proper default value (from all 3 sources) into oozie-default.  It may be nice to also reorder the properties in oozie-default alphabetically (and also do this going forward with new properties).\n\nAlso, {{oozie.service.WorkflowAppService.system.libpath}} should be set to \"/user/$\\{user.name\\}/share/lib\" (which is what OOZIE-1888 wanted to do).\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-06-19T17:02:25Z"
  },
  "patches": [],
  "external_id": "OOZIE-1890"
},{
  "_id": {
    "$oid": "5bbdf369afe7211158858464"
  },
  "message_id": "<JIRA.12969501.1463134161000.184199.1463134212970@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdf24fafe721115885580d"
  },
  "reference_ids": [
    {
      "$oid": "5bbdf369afe7211158858462"
    },
    {
      "$oid": "5bbdf369afe7211158858463"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdf369afe7211158858462"
  },
  "from_id": {
    "$oid": "5bbdf28157674ee1677870fe"
  },
  "to_ids": [
    {
      "$oid": "5bbd8d0857674ee167ce759f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (KYLIN-1686) Get \"Access is denied\" error on\n clicking \"Admin\" tab",
  "body": "Shaofeng SHI created KYLIN-1686:\n-----------------------------------\n\n             Summary: Get \"Access is denied\" error on clicking \"Admin\" tab\n                 Key: KYLIN-1686\n                 URL: https://issues.apache.org/jira/browse/KYLIN-1686\n             Project: Kylin\n          Issue Type: Bug\n          Components: Web \n            Reporter: Shaofeng SHI\n            Assignee: Zhong,Jason\n\n\nEnable LDAP authentication, login with an analyst account, when clicking the \"Admin\" tab, the page shows an exception:\n{code}\norg.springframework.security.access.AccessDeniedException: Access is denied\n\tat org.springframework.security.access.vote.AffirmativeBased.decide(AffirmativeBased.java:83)\n\tat org.springframework.security.access.intercept.AbstractSecurityInterceptor.beforeInvocation(AbstractSecurityInterceptor.java:206)\n\tat org.springframework.security.access.intercept.aopalliance.MethodSecurityInterceptor.invoke(MethodSecurityInterceptor.java:60)\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)\n\tat org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:622)\n\tat org.apache.kylin.rest.service.DiagnosisService$$EnhancerByCGLIB$$6cb9c74a.getProjectBadQueryHistory(<generated>)\n\tat io.kyligence.kap.rest.controller.BadQueryController.getBadQuerySql(BadQueryController.java:60)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:213)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:126)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:96)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:617)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:578)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:923)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:852)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:882)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:778)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:620)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\n{code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-05-13T10:10:12Z"
  },
  "patches": [],
  "external_id": "KYLIN-1686"
},{
  "_id": {
    "$oid": "5cb585f4520bcf17358be7cc"
  },
  "message_id": "<JIRA.12630479.1359763030968.233665.1359769991890@arcas>",
  "mailing_list_id": {
    "$oid": "5cb584eb520bcf17358bc781"
  },
  "reference_ids": [
    {
      "$oid": "5cb585e1520bcf17358be3b5"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5cb585e1520bcf17358be3b5"
  },
  "from_id": {
    "$oid": "58c9e0c402ca40f8bf20c547"
  },
  "to_ids": [
    {
      "$oid": "5bbdf53d57674ee16787bc0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (GORA-203) Bug in setting column field attribute\n \"qualifier\" in CassandraMapping",
  "body": "\n    [ https://issues.apache.org/jira/browse/GORA-203?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13569368#comment-13569368 ] \n\nLewis John McGibbney commented on GORA-203:\n-------------------------------------------\n\nOK Kaz, thanks.\nWhat is your opinion about the status quo, which is that we are making qualifier as attributes mandatory in Cassandra column mappings?\nI do not think they are mandatory attributes within the Cassandra data model therefore personally I do not think it is appropriate for us to enforce them within Gora.\n                \n> Bug in setting column field attribute \"qualifier\" in CassandraMapping \n> ----------------------------------------------------------------------\n>\n>                 Key: GORA-203\n>                 URL: https://issues.apache.org/jira/browse/GORA-203\n>             Project: Apache Gora\n>          Issue Type: Bug\n>          Components: storage-cassandra\n>    Affects Versions: 0.2.1\n>            Reporter: Lewis John McGibbney\n>             Fix For: 0.3\n>\n>\n> Currently, we are absolutely required to set a value for a column field attribute \"qualifier\", however there are no checks to determine whether this is actually present or not, therefore this is a bug.\n> Renato pointed this out and hopefully he can upload some stack traces relating to the issue to display the kind of issues one faces when qualifier attributes and their values are not present when mapping columns to Cassandra.\n> As far as we know, column field attributes are supported in the most recent Cassandra data model (and this is not due to change) therefore we should also support them in Gora, however it is my opinion (please comment here) on whether they should be optional or not.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-02-02T01:53:11Z"
  },
  "patches": [],
  "external_id": "GORA-203"
},{
  "_id": {
    "$oid": "5bbdf81fe8113566f6651389"
  },
  "message_id": "<4260CD1F.1050606@cs.put.poznan.pl>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [
    {
      "$oid": "5bbdf81de8113566f6651339"
    },
    {
      "$oid": "5bbdf81fe8113566f6651381"
    },
    {
      "$oid": "5bbdf81ce8113566f665131c"
    },
    {
      "$oid": "5bbdf81fe8113566f6651383"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdf81fe8113566f6651383"
  },
  "from_id": {
    "$oid": "58fb5b25e4f89451f5fc14b5"
  },
  "to_ids": [
    {
      "$oid": "5bbdf69457674ee16789fd6d"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [jira] Commented: (NUTCH-39) pagination in search result",
  "body": "\nHi Doug,\n\nAndrzej suggested that I add clustered results to the XML RSS feed; in \norder to do so I'd have to modify the servlet slightly -- my question \nis: what's the best way do to it? Should I alter the servlet, or rather \nwrite (subclass?) it and put the code that uses the clustering plugin \nand outputs the results in there?\n\nDawid\n\nDoug Cutting wrote:\n> Jack Tang wrote:\n> \n>> If no more comments this thread more. I plan to implement it next two \n>> weeks.\n> \n> \n> Please note that I implemented an OpenSearch servlet a few days ago. \n> (Folks who are interested in closely tracking Nutch development should \n> subscribe to the nutch-commits list too.)\n> \n> http://svn.apache.org/viewcvs.cgi/incubator/nutch/trunk/src/java/org/apache/nutch/searcher/OpenSearchServlet.java?view=markup \n> \n> \n> I have not yet written an OpenSearch description document page, but that \n> should be easy to implement using JSP \n> (http://opensearch.a9.com/spec/opensearchdescription/1.0/).\n> \n> Nor have I yet done anything to generate HTML from the XML.  It would be \n> great if you could help develop the architecture to generate HTML result \n> pages from the servlet's XML, and to generate the opensearch description \n> document.\n> \n> I still intend to add a few things to the servlet.  In particular, I \n> intend to dump all stored fields for each document into the item xml, in \n> the nutch namespace.  I also intend to add navigation links the the \n> channel.\n> \n> Cheers,\n> \n> Doug\n",
  "date": {
    "$date": "2005-04-16T10:30:23Z"
  },
  "patches": [],
  "external_id": "NUTCH-39"
},{
  "_id": {
    "$oid": "5f27d0db532b7277349c5245"
  },
  "message_id": "<1556541317.1234297919467.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d05c532b7277349c3027"
  },
  "from_id": {
    "$oid": "5f27d05caf02e2d6de91cc46"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (AMQ-2103) Memory leak when marshaling\n ActiveMQTextMessage to persistence store",
  "body": "\n     [ https://issues.apache.org/activemq/browse/AMQ-2103?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTrevor Pounds updated AMQ-2103:\n-------------------------------\n\n    Attachment: jhat_ByteSequence_0xe837a5c0.htm\n\nattaching jhat heap analysis showing ActiveMQTextMessage->ByteSequence object holding the byte array data.\n\n> Memory leak when marshaling ActiveMQTextMessage to persistence store\n> --------------------------------------------------------------------\n>\n>                 Key: AMQ-2103\n>                 URL: https://issues.apache.org/activemq/browse/AMQ-2103\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: Broker\n>    Affects Versions: 5.0.0\n>         Environment: ActiveMQ 5.0.0.20-fuse\n>            Reporter: Trevor Pounds\n>         Attachments: Duplicate Message Data (Internal Marshalling).png, jhat_ActiveMQTextMessage_0xe837a478.htm, jhat_ByteSequence_0xe837a5c0.htm\n>\n>\n> When an org.apache.activemq.command.ActiveMQTextMessage is marshaled into the persistence store some portion of the messages are stored in memory (i.e. pending cursor/consumer dispatch queue).  The messages stored in memory have the potential to cause the broker to run out of memory because org.apache.activemq.command.ActiveMQTextMessage objects can store the data twice, once in the 'text' field and once in the 'content' field.  Normally this isn't a problem since the 'content' field is cleared when the message is being used in a client application (i.e. by calling getText() clears content).  The problem occurs when a consumer is slow and a large number of messages are sitting around on the broker in pending/dispatch memory space.  The message is marshaled for the store and then persisted to disk and copied to pending memory when space is available.\n> This bug affects any ActiveMQ*Message object that does not clear its temporary data (i.e. 'text' field) once it has been marshaled.  When a message is marshaled we should null the derived objects memory space once the data has been written to the parent object's 'content' field.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-02-10T12:31:59Z"
  },
  "patches": [],
  "external_id": "AMQ-2103"
},{
  "_id": {
    "$oid": "5bc860176e373d4fe81c7479"
  },
  "message_id": "<1074911004.2582.1337586042588.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc860166e373d4fe81c7473"
  },
  "from_id": {
    "$oid": "5bacb30257674ee167d8fa53"
  },
  "to_ids": [
    {
      "$oid": "5bc85ac857674ee167cc9912"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (FLUME-1219) Race conditions in BucketWriter /\n HDFSEventSink",
  "body": "\n    [ https://issues.apache.org/jira/browse/FLUME-1219?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13279996#comment-13279996 ] \n\nMike Percy commented on FLUME-1219:\n-----------------------------------\n\nThe reviewboard doesn't seem to be posting to JIRA so here are my comments regarding the patch on RB:\n\nBucketWriter refactoring: append() does all the work of open/close/roll. open() is a private method that takes no arguments. No abort() call. Only one constructor. Far fewer entry points and code paths. I believe I've closed all or many of the race conditions and clarified the API responsibilities/semantics. Also, renaming of the .tmp files works on shutdown.\n                \n> Race conditions in BucketWriter / HDFSEventSink\n> -----------------------------------------------\n>\n>                 Key: FLUME-1219\n>                 URL: https://issues.apache.org/jira/browse/FLUME-1219\n>             Project: Flume\n>          Issue Type: Bug\n>            Reporter: Mike Percy\n>            Assignee: Mike Percy\n>         Attachments: FLUME-1219-1.patch\n>\n>\n> BucketWriter has several race conditions that came up during my performance testing over the weekend. One issue that caused data loss was the lack of atomic close() and open() semantics related to the \"retry\" mechanism after the abort() call in HDFSEventSink.process().\n> Another issue is the lack of clearly delineated responsibilities for calling open(), flush(), close(), etc. For example, HDFSEventSink.start() calls open(), HDFSEventSink.process() calls and abort() which calls open(), and BucketWriter.append() also calls close() and open().\n> There is another race condition related to the JVM shutdown hooks, which cause .tmp files not to be renamed.\n> These APIs need to be refactored and their responsibilities need to be clarified.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-05-21T07:40:42Z"
  },
  "patches": [],
  "external_id": "FLUME-1219"
},{
  "_id": {
    "$oid": "5bc8618557a11257de5659ef"
  },
  "message_id": "<JIRA.12845230.1436956725000.199084.1436994065004@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc85d4657a11257de5639ae"
    },
    {
      "$oid": "5bc85d4657a11257de5639af"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc85d4657a11257de5639ae"
  },
  "from_id": {
    "$oid": "5bbe0f9657674ee1679862f7"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-2883) Unify memory handling",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-2883?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14628718#comment-14628718 ] \n\nJohn Hewson commented on PDFBOX-2883:\n-------------------------------------\n\n{quote}\nThis implementation could be used as the general 'backend' for all buffered streams and even the file input stream copy.\n{quote}\n\nThat doesn't work for my use case, where I never want to hit disk. I'm happy for a file to OOM and to skip processing rather than using disk - which doesn't scale in a multi-threaded environment - the disk becomes a bottleneck.\n\n{quote}\nIn order to use this the PDDocument methods should be changed to not have a 'useScratchFile' parameter but to take a MemoryHandling object which details the Buffering strategy (using ScratchFile; what amount of main memory can be used, ...).\n{quote}\n\nThis is starting to look a lot like virtual memory, why not increase Java's maximum heap to some huge value with -Xmx and rely on the OS using swap?\n\n> Unify memory handling\n> ---------------------\n>\n>                 Key: PDFBOX-2883\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-2883\n>             Project: PDFBox\n>          Issue Type: Improvement\n>          Components: Parsing\n>    Affects Versions: 2.0.0\n>            Reporter: Timo Boehme\n>            Assignee: Timo Boehme\n>\n> PDFBOX now has at least 2 different mechanisms to use main memory vs. keeping large data in temporary file: in case of provided input stream the stream is copied to temporary file and all read PDF streams are handled by RandomAccessBuffer/ScratchFile.\n> In PDFBOX-2882 I've done a re-implementation for ScratchFile which is quite fast and allows to set a maximum amount of memory to be used for its pages before it starts using the scratch file. This implementation could be used as the general 'backend' for all buffered streams and even the file input stream copy. As long as the PDF fits into the allowed maximum memory it should equally fast as RandomAccessBuffer while it allows for good control of memory usage by going to scratch file if needed. This prevents OOM in case of large files.\n> In order to use this the PDDocument methods should be changed to not have a 'useScratchFile' parameter but to take a MemoryHandling object which details the Buffering strategy (using ScratchFile; what amount of main memory can be used, ...).\n> I've opened this issue for discussing. Since we need API changes in PDDocument it should be done before 2.0 release.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2015-07-15T21:01:05Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2883"
},{
  "_id": {
    "$oid": "5f27c47de2367af241b6bd6d"
  },
  "message_id": "<JIRA.12697226.1393365964206.98783.1402359542013@arcas>",
  "mailing_list_id": {
    "$oid": "5f27c38ce2367af241b67514"
  },
  "reference_ids": [
    {
      "$oid": "5f27c46ee2367af241b6b8e9"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27c46ee2367af241b6b8e9"
  },
  "from_id": {
    "$oid": "5f27c3f3af02e2d6de738c36"
  },
  "to_ids": [
    {
      "$oid": "58bfd014e4f89451f55ce17b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (BIGTOP-1222) Simplify and gradleize a subset of\n the bigtop smokes",
  "body": "\n    [ https://issues.apache.org/jira/browse/BIGTOP-1222?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14025963#comment-14025963 ] \n\nNate DAmico commented on BIGTOP-1222:\n-------------------------------------\n\nAs discussed in the meetup today at Redhat, created new item: https://issues.apache.org/jira/browse/BIGTOP-1333\n\nThis is for work to have the build.gradle file read params/inputs from external parametrized file\n\n> Simplify and gradleize a subset of the bigtop smokes\n> ----------------------------------------------------\n>\n>                 Key: BIGTOP-1222\n>                 URL: https://issues.apache.org/jira/browse/BIGTOP-1222\n>             Project: Bigtop\n>          Issue Type: Improvement\n>          Components: Build, Tests\n>    Affects Versions: 0.7.0\n>            Reporter: jay vyas\n>            Assignee: jay vyas\n>             Fix For: backlog\n>\n>         Attachments: BIGTOP-1222-2.patch, BIGTOP-1222.patch, BIGTOP-1222.patch\n>\n>\n> (Rewritten the description for clarity)\n> We need an easier way to run bigtop smoke tests, and gradle provides this:\n> 1) Easy to script/modify\n> 2) Human readable\n> 3) equally oriented towards both groovy and plain old java\n> The advantage of this method to running smokes : \n> 1) No need to compile a jar : this is a costly step and not much value added, also creates indirection which can make debugging a broken test very hard.\n> 2) Simple: A smoke test doesnt need to make low level API calls or be compiled against the right APIs - rather, it should test the end user interface (\"hive -q  ....\", \"pig -x ....\", \"hadoop jar ....\", and so on).  \n> 3) Customizable:  The smoke tests shouldnt require users to have to write XML and debug environmental variables / grep around for System properties etc.  Rather, a high level controller should do all that checking for you.  \n> The initial idea was to write a python/bash implementation wrapper of scripts, but that was replaced by the idea of using gradle.  The advantage of gradle is that we don't need to manually set the classpath and run groovy commands: Gradle wraps groovy scripts in their native java context quite nicely - but it doesnt add any other unnecessary overhead (xml, jar files, no need for complex xml tag wrappers for simple tasks - just plain groovy code).\n> So, here the goal is just to create a nice, clean, extensible non-jar, non-API dependent gradle runner for the smoke tests which exersizes the hadoop cluster the same way a typical end-user would.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-06-10T00:19:02Z"
  },
  "patches": [],
  "external_id": "BIGTOP-1222"
},{
  "_id": {
    "$oid": "5f27c47ce2367af241b6bd0f"
  },
  "message_id": "<JIRA.12683710.1386630633868.90294.1406843140231@arcas>",
  "mailing_list_id": {
    "$oid": "5f27c38ce2367af241b67514"
  },
  "reference_ids": [
    {
      "$oid": "5f27c47be2367af241b6bd00"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27c47be2367af241b6bd00"
  },
  "from_id": {
    "$oid": "58bfd09f02ca40f8bf148280"
  },
  "to_ids": [
    {
      "$oid": "58bfd014e4f89451f55ce17b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (BIGTOP-1159) /usr/lib/flume-ng/plugins.d/ needs\n to be created and owned by flume",
  "body": "\n    [ https://issues.apache.org/jira/browse/BIGTOP-1159?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14081532#comment-14081532 ] \n\nSean Mackrory commented on BIGTOP-1159:\n---------------------------------------\n\n{quote}But why shouldn't plugins be managed through packages as well? {quote}\n\nPlugins that were delivered as packages would sidestep my concern entirely, so if we foresee that being the main use for this directory, then you have a more enthusiastic +1 from me. The /var/lib/bigtop thing was initially motivated by plugins that were added manually by the user and were not packaged in most distros, and not something we would consider packaging in Bigtop, so there's a fundamental difference there.\n\n> /usr/lib/flume-ng/plugins.d/ needs to be created and owned by flume\n> -------------------------------------------------------------------\n>\n>                 Key: BIGTOP-1159\n>                 URL: https://issues.apache.org/jira/browse/BIGTOP-1159\n>             Project: Bigtop\n>          Issue Type: Bug\n>            Reporter: Bruno Mahé\n>            Assignee: Bruno Mahé\n>         Attachments: 0001-BIGTOP-1159.-usr-lib-flume-ng-plugins.d-needs-to-be-.patch\n>\n>\n> Apache Flume expects plugins to be dropped in /usr/lib/flume-ng/plugins.d/.\n> But our package do not provide such directory since flume does not ship any plugin.\n> Apache Flume package should create an empty directory /usr/lib/flume-ng/plugins.d/ and owned it.\n> So plugins can be dropped automatically as packages into their own subdirectories.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-07-31T21:45:40Z"
  },
  "patches": [],
  "external_id": "BIGTOP-1159"
},{
  "_id": {
    "$oid": "5f27cd31014d3531c6cc0b52"
  },
  "message_id": "<JIRA.12676176.1382982317771.18967.1395947294807@arcas>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [
    {
      "$oid": "5f27cd20014d3531c6cc05e2"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cd20014d3531c6cc05e2"
  },
  "from_id": {
    "$oid": "59bf92ecf2a4565fe9e6add0"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (JCRVLT-21) \"Item is protected\" error during vlt\n rcp copy operation",
  "body": "\n     [ https://issues.apache.org/jira/browse/JCRVLT-21?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTobias Bocanegra updated JCRVLT-21:\n-----------------------------------\n\n    Fix Version/s:     (was: 3.1)\n                   3.2\n\n> \"Item is protected\" error during vlt rcp copy operation\n> -------------------------------------------------------\n>\n>                 Key: JCRVLT-21\n>                 URL: https://issues.apache.org/jira/browse/JCRVLT-21\n>             Project: Jackrabbit FileVault\n>          Issue Type: Bug\n>    Affects Versions: 3.0\n>            Reporter: Tobias Bocanegra\n>             Fix For: 3.2\n>\n>\n> When you do a vlt rcp copy of a node that has a protected property removed in the source then the \"Item is protected\" error is thrown because vault rcp attempts to do a node update instead of a replace operation.  If you install a package that has a protected property removed then it does a replace and throws no errors.\n> {code}\n> 000006 U /content/bhg/decorating/do-it-yourself/accents/diy-carboard-projects/jcr:content\n> [ERROR] Error during copy: javax.jcr.nodetype.ConstraintViolationException: Item is protected\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-03-27T19:08:14Z"
  },
  "patches": [],
  "external_id": "JCRVLT-21"
},{
  "_id": {
    "$oid": "5bbf0c20b79d666cbb228da3"
  },
  "message_id": "<JIRA.12935306.1454114407000.255558.1454120019827@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0a69b79d666cbb2248e6"
    },
    {
      "$oid": "5bbf08d3b79d666cbb2203c9"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf08d3b79d666cbb2203c9"
  },
  "from_id": {
    "$oid": "5bbf07b657674ee167314fc5"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (KAFKA-3174) Re-evaluate the CRC32 class\n performance.",
  "body": "\n     [ https://issues.apache.org/jira/browse/KAFKA-3174?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJiangjie Qin updated KAFKA-3174:\n--------------------------------\n    Description: \nWe used org.apache.kafka.common.utils.CRC32 in clients because it has better performance than java.util.zip.CRC32 in Java 1.6.\n\nIn a recent test I ran it looks in Java 1.8 the CRC32 class is 2x as fast as the Crc32 class we are using now. We may want to re-evaluate the performance of Crc32 class and see it makes sense to simply use java CRC32 instead.\n\n  was:\nWe org.apache.kafka.common.utils.CRC32 because it has better performance than java.util.zip.CRC32 in Java 1.6.\n\nIn a recent test I ran it looks in Java 1.8 the CRC32 class is 2x as fast as the Crc32 class we are using now. We may want to re-evaluate the performance of Crc32 class and see it makes sense to simply use java CRC32 instead.\n\n\n> Re-evaluate the CRC32 class performance.\n> ----------------------------------------\n>\n>                 Key: KAFKA-3174\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-3174\n>             Project: Kafka\n>          Issue Type: Improvement\n>    Affects Versions: 0.9.0.0\n>            Reporter: Jiangjie Qin\n>             Fix For: 0.9.0.1\n>\n>\n> We used org.apache.kafka.common.utils.CRC32 in clients because it has better performance than java.util.zip.CRC32 in Java 1.6.\n> In a recent test I ran it looks in Java 1.8 the CRC32 class is 2x as fast as the Crc32 class we are using now. We may want to re-evaluate the performance of Crc32 class and see it makes sense to simply use java CRC32 instead.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-01-30T02:13:39Z"
  },
  "patches": [],
  "external_id": "KAFKA-3174"
},{
  "_id": {
    "$oid": "5f27c487e2367af241b6c08c"
  },
  "message_id": "<JIRA.12711208.1398789031677.13582.1400607280146@arcas>",
  "mailing_list_id": {
    "$oid": "5f27c38ce2367af241b67514"
  },
  "reference_ids": [
    {
      "$oid": "5f27c47ce2367af241b6bd1a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27c47ce2367af241b6bd1a"
  },
  "from_id": {
    "$oid": "5f27c414af02e2d6de73de84"
  },
  "to_ids": [
    {
      "$oid": "58bfd014e4f89451f55ce17b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (BIGTOP-1289) Update hadoop version (including\n hadoop components) to 2.3",
  "body": "\n    [ https://issues.apache.org/jira/browse/BIGTOP-1289?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14003699#comment-14003699 ] \n\nMartin Bukatovic commented on BIGTOP-1289:\n------------------------------------------\n\nAdditional updates:\n * groovy bumped to 2.2.1 (following bigtop.mk file, see BIGTOP-1097)\n * spark bumped to 0.9.1 (see BIGTOP-1284)\n\nNo update needed for:\n * hive is not mentioned in the main pom file (bumped to 0.13 in BIGTOP-1110)\n\nIssues with:\n * HBase, maven wasn't able to get version 0.98.2 from repo.maven.apache.org\n * bumping groovy makes hadoopsmokes compilation fail, maven wasn't able to get\n   groovy-eclipse-batch:jar:2.2.1-01 from repo.maven.apache.org\n\nsee:\n\n{noformat}\n[ERROR] Failed to execute goal on project hbase-smoke: Could not resolve dependencies for project org.apache.bigtop.itest:hbase-smoke:jar:0.8.0-SNAPSHOT: The following artifac\nts could not be resolved: org.apache.hbase:hbase-common:jar:0.98.2, org.apache.hbase:hbase-common:jar:tests:0.98.2, org.apache.hbase:hbase-server:jar:0.98.2, org.apache.hbase:\nhbase-server:jar:tests:0.98.2: Could not find artifact org.apache.hbase:hbase-common:jar:0.98.2 in central (http://repo.maven.apache.org/maven2) -> [Help 1]\n{noformat}\n\n{noformat}\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project hadoop-smoke: Execution default-compile of goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile failed: Plugin org.apache.maven.plugins:maven-compiler-plugin:2.3.2 or one of its dependencies could not be resolved: Failure to find org.codehaus.groovy:groovy-eclipse-batch:jar:2.2.1-01 in http://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]\n{noformat}\n\nWhat is the correct way to resolve those issues? The specified versions follows\nthe plan for 0.8.0 release.\n\n> Update hadoop version (including hadoop components) to 2.3\n> ----------------------------------------------------------\n>\n>                 Key: BIGTOP-1289\n>                 URL: https://issues.apache.org/jira/browse/BIGTOP-1289\n>             Project: Bigtop\n>          Issue Type: Improvement\n>          Components: General, Tests\n>    Affects Versions: 0.7.0\n>            Reporter: Martin Bukatovic\n>            Assignee: Martin Bukatovic\n>             Fix For: 0.8.0\n>\n>         Attachments: 0001-BIGTOP-1289-update-component-versions-in-root-pom-fi.patch\n>\n>\n> Because we are switching to protobuf 2.5 (BIGTOP-1241) as used by hadoop 2.1 and later, we need to upgrade top level pom file to specify compatible hadoop version, including all hadoop components.\n> So in top level `bigtop/pom.xml` file, change:\n>  * /project/properties/hadoop.version = 2.3.0\n>  * update /project/properties/{hbase,pig, ... } accordingly\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-05-20T17:34:40Z"
  },
  "patches": [],
  "external_id": "BIGTOP-1289"
},{
  "_id": {
    "$oid": "5bc863e257a11257de5666d4"
  },
  "message_id": "<JIRA.12825620.1430245637000.10496.1430245686218@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc862ac57a11257de5660e4"
    },
    {
      "$oid": "5bc862ac57a11257de5660e5"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc862ac57a11257de5660e4"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PDFBOX-2777) Create convenience method to create\n an XImage object",
  "body": "Tilman Hausherr created PDFBOX-2777:\n---------------------------------------\n\n             Summary: Create convenience method to create an XImage object\n                 Key: PDFBOX-2777\n                 URL: https://issues.apache.org/jira/browse/PDFBOX-2777\n             Project: PDFBox\n          Issue Type: Bug\n          Components: PDModel\n    Affects Versions: 2.0.0\n            Reporter: Tilman Hausherr\n            Assignee: Tilman Hausherr\n             Fix For: 2.0.0\n\n\nAs suggested by [~jahewson] on the user mailing list. The convenience method PDImageXObject.createFromFile() will replace the long code that is in the ImageToPDF and AddImageToPDF examples.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2015-04-28T18:28:06Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2777"
},{
  "_id": {
    "$oid": "5bc8726757a11257de56e620"
  },
  "message_id": "<1113715188.1228124927532.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc8726157a11257de56e5c7"
  },
  "from_id": {
    "$oid": "5bc8726657674ee167d91f07"
  },
  "to_ids": [
    {
      "$oid": "5bc85c2e57674ee167cd4149"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (PDFBOX-390) org.pdfbox.filter.ASCIIHexFilter does\n not skip Whitespace",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-390?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMathias Bosch updated PDFBOX-390:\n---------------------------------\n\n    Attachment: 000161.pdf\n\nhi andreas,\n\ni did attach a sample document containing multiple hex encoded inline \nimages containing whitespaces and an EOD as the firstByte.\n\nIt does not provide an example for the \"special\" case where the \nsecondByte is the EOD,\nthis is coded according to the spec.\n\n\n\n\n\n> org.pdfbox.filter.ASCIIHexFilter does not skip Whitespace\n> ---------------------------------------------------------\n>\n>                 Key: PDFBOX-390\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-390\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Parsing\n>    Affects Versions: 0.8.0-incubator\n>            Reporter: Mathias Bosch\n>             Fix For: 0.8.0-incubator\n>\n>         Attachments: 000161.pdf, ASCIIHexFilter_390-Patch.diff\n>\n>\n> org.pdfbox.filter.ASCIIHexFilter does not skip Whitespace\n> According to the Specification (pdf_reference_1-7.pdf) all Whitespace\n> Characters between the ASCII-Hex values have to be skipped (see 3.3.1\n> ASCIIHexDecode Filter).\n> The 0.8.0-incubator source decodes (or attempts to decode) those Whitespace\n> Characters and as a result the byte values are wrong (all characters that\n> are not [0-9a-f] result in -1, but processing does continue).\n> This causes an invalid byte Stream.\n> The ASCIIHexDecode Filter Section also defines the EOD end Character of the\n> Byte Steam as '>' which might ease the parsing of inline Images.\n> (The EI Operator should follow the EOD in case of an inline Image).\n> Example for ASCII-Hex encoded value, copied from the Spec:\n> FF CE A3 7C 5B 3F 28 16 0A 02 00 02 0A 16 28 3F 5B 7C A3 CE FF >\n> I did fix the problem to be able to continue with my work.\n> I paste the changed code here as a hint that might help to fix the bug.\n> public class ASCIIHexFilter\n>   implements Filter\n> {\n>  /**\n>   * Whitespace\n>   *   0  0x00  Null (NUL)\n>   *   9  0x09  Tab (HT)\n>   *  10  0x0A  Line feed (LF)\n>   *  12  0x0C  Form feed (FF)\n>   *  13  0x0D  Carriage return (CR)\n>   *  32  0x20  Space (SP)  \n>   */\n>   protected boolean isWhitespace(int c) {\n>     return c == 0 || c == 9 || c == 10 || c == 12 || c == 13 || c == 32;\n>   }\n>   \n>   protected boolean isEOD(int c) {\n>     return (c == 62); // '>' - EOD\n>   }\n>   /**\n>    * {@inheritDoc}\n>    */\n>   public void decode(InputStream compressedData, OutputStream result, COSDictionary options, int filterIndex) throws IOException {\n>     int value = 0;\n>     int firstByte = 0;\n>     int secondByte = 0;\n>     while ((firstByte = compressedData.read()) != -1) {\n>       \n>       // always after first char\n>       while(isWhitespace(firstByte))\n>         firstByte = compressedData.read();\n>       if(isEOD(firstByte))\n>         break;\n>       \n>       if(REVERSE_HEX[firstByte] == -1)\n>         System.out.println(\"Invalid Hex Code; int: \" + firstByte + \" char: \" + (char) firstByte);\n>       value = REVERSE_HEX[firstByte] * 16;\n>       secondByte = compressedData.read();\n>       \n>       if(isEOD(secondByte)) {\n>         // second value behaves like 0 in case of EOD\n>         result.write(value);\n>         break;\n>       }\n>       if(secondByte >= 0) {\n>         if(REVERSE_HEX[secondByte] == -1)\n>           System.out.println(\"Invalid Hex Code; int: \" + secondByte + \" char: \" + (char) secondByte);\n>         value += REVERSE_HEX[secondByte];\n>       }\n>       result.write(value);\n>     }\n>     \n>     result.flush();\n>   }\n> // .....................................................\n> // other code remains unchanged\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-12-01T01:48:47Z"
  },
  "patches": [],
  "external_id": "PDFBOX-390"
},{
  "_id": {
    "$oid": "5bbf2eab30623e2888ae0772"
  },
  "message_id": "<JIRA.12829092.1431412027000.181353.1432216979772@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf2dd530623e2888adfb72"
    },
    {
      "$oid": "5bbf2dd530623e2888adfb71"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf2dd530623e2888adfb71"
  },
  "from_id": {
    "$oid": "5bbf274757674ee1674fb6e4"
  },
  "to_ids": [
    {
      "$oid": "5bbf2ca057674ee16751d09e"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (LENS-550) Remove enable multi table select",
  "body": "\n     [ https://issues.apache.org/jira/browse/LENS-550?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nSushil Mohanty updated LENS-550:\n--------------------------------\n    Attachment: LENS-550-2.patch\n\n> Remove enable multi table select \n> ---------------------------------\n>\n>                 Key: LENS-550\n>                 URL: https://issues.apache.org/jira/browse/LENS-550\n>             Project: Apache Lens\n>          Issue Type: Sub-task\n>          Components: cube\n>    Affects Versions: 2.0\n>            Reporter: Amareshwari Sriramadasu\n>            Assignee: Sushil Mohanty\n>             Fix For: 2.2\n>\n>         Attachments: LENS-550-2.patch\n>\n>\n> The feature for enable multi table select in lens-cube is partially broken and the its counterpart of accepting multiple tables in single query is not accepted in hive - HIVE-4956. \n> Since we have union query support, we can remove the feature from lens and always write union query for multiple tables.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-05-21T14:02:59Z"
  },
  "patches": [],
  "external_id": "LENS-550"
},{
  "_id": {
    "$oid": "5f27c365b7fc3b4361ceb26e"
  },
  "message_id": "<1746230031.1144012365384.JavaMail.jira@ajax>",
  "mailing_list_id": {
    "$oid": "5f27c121b7fc3b4361ce181a"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "5f27c197af02e2d6de6d6df8"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "body": "CPU exhaustion on heavy bind/unbind\n-----------------------------------\n\n         Key: DIRMINA-197\n         URL: http://issues.apache.org/jira/browse/DIRMINA-197\n     Project: Directory MINA\n        Type: Bug\n\n    Reporter: Emmanuel Lecharny\n    Priority: Blocker\n Attachments: stack.txt\n\nStill doing smoke tests...\n\nSo now, I have a program that bind and unbind one thousand times to ADS. Everything works just fine, but now, ADS is trying to transform my CPU to a popcorn machine. It uses 75% of my CPU just doing nothing seious. I have created a thread dump with jstack, attached.\n\nAny help welcomed !\n\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-04-02T22:12:45Z"
  },
  "from_id": {
    "$oid": "59bfaa3af2a4565fe90974cd"
  },
  "subject": "[jira] Created: (DIRMINA-197) CPU exhaustion on heavy bind/unbind",
  "external_id": "DIRMINA-197"
},{
  "_id": {
    "$oid": "58c11f296d2aba458ddf7a1b"
  },
  "message_id": "<12077198.1155799214821.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "58c117176d2aba458dde60bb"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "58c11f296d2aba458ddf7a18"
  },
  "from_id": {
    "$oid": "58c11e1f02ca40f8bfb1f6ef"
  },
  "to_ids": [
    {
      "$oid": "58c11420e4f89451f51d76f1"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Closed: (VFS-79) Cannot list children of HTTP folders",
  "body": "     [ http://issues.apache.org/jira/browse/VFS-79?page=all ]\n\nMario Ivankovits closed VFS-79.\n-------------------------------\n\n    Resolution: Won't Fix\n\nYep, unhappily the http filesystem do not support the getChildren method.\n\nSystem.err.println(\"can list children=\" + f.getFileSystem().hasCapability(Capability.LIST_CHILDREN));\n\nSome people already asked about this feature and maybe also started to implement it, but not patch has been submitted yet. The problem is, that its not necessarily easy to parse the html output.\n\n> Cannot list children of HTTP folders\n> ------------------------------------\n>\n>                 Key: VFS-79\n>                 URL: http://issues.apache.org/jira/browse/VFS-79\n>             Project: Commons VFS\n>          Issue Type: Bug\n>    Affects Versions: Nightly Builds\n>         Environment: Java 1.5\n>            Reporter: Ben Ashpole\n>\n> The following code produces the below exception at f.getChildren(), even though the specified location is a folder with children.  Similar problems exist for other protocol supported by VFS.\n>         FileSystemManager fsManager = VFS.getManager();\n>         final FileObject f = fsManager.resolveFile(\n>             \"http://people.apache.org/builds/jakarta-commons/nightly/commons-vfs/\");\n>         FileObject[] children = f.getChildren();\n> Exception in thread \"main\" org.apache.commons.vfs.FileSystemException: Could not list the contents of \"http://people.apache.org/builds/jakarta-commons/nightly/commons-vfs\" because it is not a folder.\n> \tat org.apache.commons.vfs.provider.AbstractFileObject.getChildren(AbstractFileObject.java:525)\n> \tat com.bashpole.reflectorGadget.reflectionGroup.Sync.test(Sync.java:118)\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: commons-dev-unsubscribe@jakarta.apache.org\nFor additional commands, e-mail: commons-dev-help@jakarta.apache.org\n\n",
  "date": {
    "$date": "2006-08-17T00:20:14Z"
  },
  "patches": [],
  "external_id": "VFS-79"
},{
  "_id": {
    "$oid": "5bbdf697e8113566f664dbce"
  },
  "message_id": "<1808963550.43439.1309174968106.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "59bfa272f2a4565fe9fa883a"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd1e4f89451f55cdfd2"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (NUTCH-295) More description for\n fetcher.threads.fetch property",
  "body": "\n     [ https://issues.apache.org/jira/browse/NUTCH-295?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMarkus Jelsma resolved NUTCH-295.\n---------------------------------\n\n    Resolution: Fixed\n\nCommitted for 1.4 in rev. 1140116 and for trunk in rev. 1140117.\n\n> More description for fetcher.threads.fetch property\n> ---------------------------------------------------\n>\n>                 Key: NUTCH-295\n>                 URL: https://issues.apache.org/jira/browse/NUTCH-295\n>             Project: Nutch\n>          Issue Type: Improvement\n>          Components: fetcher\n>    Affects Versions: 0.8\n>            Reporter: Dennis Kubes\n>            Assignee: Markus Jelsma\n>            Priority: Minor\n>             Fix For: 1.4, 2.0\n>\n>         Attachments: fetcher_threads_desc.patch\n>\n>\n> Added some description to the fetcher.threads.fetch property to explain the number of threads running in a cluster. Patch is attached.\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-06-27T11:42:48Z"
  },
  "patches": [],
  "external_id": "NUTCH-295"
},{
  "_id": {
    "$oid": "5bacb102faaadd76f8a9a3a7"
  },
  "message_id": "<JIRA.13045129.1487751793000.46277.1487874704328@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [
    {
      "$oid": "5bacb0e0faaadd76f8a99ab8"
    },
    {
      "$oid": "5bacb0e0faaadd76f8a99ab9"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacb0e0faaadd76f8a99ab8"
  },
  "from_id": {
    "$oid": "5bacb0bb57674ee167d4ac26"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Work started] (PIG-5134) fix  TestAvroStorage unit test\n failures after PIG-5132",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-5134?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nWork on PIG-5134 started by Nandor Kollar.\n------------------------------------------\n> fix  TestAvroStorage unit test failures after PIG-5132\n> ------------------------------------------------------\n>\n>                 Key: PIG-5134\n>                 URL: https://issues.apache.org/jira/browse/PIG-5134\n>             Project: Pig\n>          Issue Type: Sub-task\n>          Components: spark\n>            Reporter: liyunzhang_intel\n>            Assignee: Nandor Kollar\n>             Fix For: spark-branch\n>\n>         Attachments: PIG-5134.patch\n>\n>\n> It seems that test fails, because Avro GenericData#Record doesn't implement Serializable interface:\n> {code}\n> 2017-02-23 09:14:41,887 ERROR [main] spark.JobGraphBuilder (JobGraphBuilder.java:sparkOperToRDD(183)) - throw exception in sparkOperToRDD: \n> org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 9.0 (TID 9) had a not serializable result: org.apache.avro.generic.GenericData$Record\n> Serialization stack:\n> \t- object not serializable (class: org.apache.avro.generic.GenericData$Record, value: {\"key\": \"stuff in closet\", \"value1\": {\"thing\": \"hat\", \"count\": 7}, \"value2\": {\"thing\": \"coat\", \"count\": 2}})\n> \t- field (class: org.apache.pig.impl.util.avro.AvroTupleWrapper, name: avroObject, type: interface org.apache.avro.generic.IndexedRecord)\n> \t- object (class org.apache.pig.impl.util.avro.AvroTupleWrapper, org.apache.pig.impl.util.avro.AvroTupleWrapper@3d3a58c1)\n> \tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n> \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n> {code}\n> The failing tests is a new test introduced with merging trunk to spark branch, that's why we didn't see this error before.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-23T18:31:44Z"
  },
  "patches": [],
  "external_id": "PIG-5134"
},{
  "_id": {
    "$oid": "5bbef6a4748b2d53b761d596"
  },
  "message_id": "<JIRA.12922907.1450403884000.66243.1450435066700@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbef648748b2d53b761c752"
  },
  "reference_ids": [
    {
      "$oid": "5bbef6a4748b2d53b761d591"
    },
    {
      "$oid": "5bbef6a4748b2d53b761d590"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbef6a4748b2d53b761d590"
  },
  "from_id": {
    "$oid": "58c9e3ba02ca40f8bf20c9dd"
  },
  "to_ids": [
    {
      "$oid": "5bbef67e57674ee1676a2fde"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DELTASPIKE-1043) Loop redirect in error-page\n (404) with CLIENTWINDOW",
  "body": "\n    [ https://issues.apache.org/jira/browse/DELTASPIKE-1043?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15063803#comment-15063803 ] \n\nThomas Andraschko commented on DELTASPIKE-1043:\n-----------------------------------------------\n\nSorry but i can't replicate it.\nI added the page-not-found, added the web.xml entry, started via \"mvn clean package -P run-wildfly-8.0.0\" an call e.g. http://localhost:8080/ds/asdasd.xhtml\n\nThe windowhandler is only streamed one time and the error page will be displayed.\n\n> Loop redirect in error-page (404) with CLIENTWINDOW\n> ---------------------------------------------------\n>\n>                 Key: DELTASPIKE-1043\n>                 URL: https://issues.apache.org/jira/browse/DELTASPIKE-1043\n>             Project: DeltaSpike\n>          Issue Type: Bug\n>          Components: JSF22-Module\n>    Affects Versions: 1.5.2\n>         Environment: Error page in web.xml and clientWindowMode and WildFly\n>            Reporter: Janario\n>            Assignee: Thomas Andraschko\n>         Attachments: loop-redirect.patch, workaround.patch\n>\n>\n> I have defined an error-page in web.xml to 404\n> <error-page>\n>         <error-code>404</error-code>\n>         <location>/faces/page-not-found.xhtml</location>\n> </error-page>\n> But when I use CLIENTWINDOW it starts to redirects in loop from windowhandler.html\n> As I've seen this happens because the dsrid param is lost after the container forward it to error page. So ClientSideWindowStrategy.getRequestTokenParameter always return null and send to windowhandler.html\n> Attached a patch to simulate it in jsf-playground\n> Patch: loop-redirect.patch\n> After apply it. I ran with: mvn wildfly:run -P run-wildfly-8.0.0\n> And opened http://localhost:8080/ds/not-found\n> I've also attached an workaround that put dsrid param as attribute and lookup for it in ClientSideWindowStrategy\n> I know it isn't the best way but I couldn't find another way to do that\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-12-18T10:37:46Z"
  },
  "patches": [],
  "external_id": "DELTASPIKE-1043"
},{
  "_id": {
    "$oid": "5bea9c419e73d744d41287a9"
  },
  "message_id": "<1449880927.1217007811788.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9c419e73d744d41287a8"
  },
  "from_id": {
    "$oid": "5bea97b835e3ea2b7b4d9785"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-1916) convert\n jdbcapi/BLOBDataModelSetup.java to junit",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-1916?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKathey Marsden updated DERBY-1916:\n----------------------------------\n\n    Component/s: Test\n\n> convert jdbcapi/BLOBDataModelSetup.java to junit\n> ------------------------------------------------\n>\n>                 Key: DERBY-1916\n>                 URL: https://issues.apache.org/jira/browse/DERBY-1916\n>             Project: Derby\n>          Issue Type: Sub-task\n>          Components: Test\n>            Reporter: Anurag Shekhar\n>            Assignee: Anurag Shekhar\n>            Priority: Minor\n>\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-07-25T10:43:31Z"
  },
  "patches": [],
  "external_id": "DERBY-1916"
},{
  "_id": {
    "$oid": "5bbdf7c6e8113566f66506cb"
  },
  "message_id": "<1188942448.1140713798373.JavaMail.jira@ajax.apache.org>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbdf7c2e8113566f6650606"
  },
  "from_id": {
    "$oid": "5bbdf77457674ee1678b04aa"
  },
  "to_ids": [
    {
      "$oid": "5bbdf69457674ee16789fd6d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (NUTCH-204) multiple field values in HitDetails",
  "body": "    [ http://issues.apache.org/jira/browse/NUTCH-204?page=comments#action_12367530 ] \n\nJerome Charron commented on NUTCH-204:\n--------------------------------------\n\n> HitDetails is a writable and in case of multiple searchservers distributed in a network it makes\n> sense to minimize the network io since getting details should be as fast as possible.\nSure Stefan.\nI will take this into account of course. Using a map like structure in HitDetails will reduce the bytes used by not duplicating keys.\nI will commit something in the next few days.\n\n> multiple field values in HitDetails\n> -----------------------------------\n>\n>          Key: NUTCH-204\n>          URL: http://issues.apache.org/jira/browse/NUTCH-204\n>      Project: Nutch\n>         Type: Improvement\n>   Components: searcher\n>     Versions: 0.8-dev\n>     Reporter: Stefan Groschupf\n>      Fix For: 0.8-dev\n>  Attachments: DetailGetValues070206.patch\n>\n> Improvement as Howie Wang suggested.\n> http://mail-archives.apache.org/mod_mbox/lucene-nutch-dev/200601.mbox/%3c43D7D45A.2070609@nutch.org%3e\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-02-23T17:56:38Z"
  },
  "patches": [],
  "external_id": "NUTCH-204"
},{
  "_id": {
    "$oid": "5bacb337faaadd76f8aa298c"
  },
  "message_id": "<323436341.4232.1323746372756.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb326faaadd76f8aa2599"
  },
  "from_id": {
    "$oid": "5bacb32c57674ee167d94b1f"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PIG-2417) Streaming UDFs -  allow users to easily\n write UDFs in scripting languages with no JVM implementation.",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-2417?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJeremy Karn updated PIG-2417:\n-----------------------------\n\n    Attachment: python_streaming_string.patch\n    \n> Streaming UDFs -  allow users to easily write UDFs in scripting languages with no JVM implementation.\n> -----------------------------------------------------------------------------------------------------\n>\n>                 Key: PIG-2417\n>                 URL: https://issues.apache.org/jira/browse/PIG-2417\n>             Project: Pig\n>          Issue Type: Improvement\n>    Affects Versions: 0.11\n>            Reporter: Jeremy Karn\n>         Attachments: python_streaming_string.patch\n>\n>\n> The goal of Streaming UDFs is to allow users to easily write UDFs in scripting languages with no JVM implementation or a limited JVM implementation.  The initial proposal is outlined here: https://cwiki.apache.org/confluence/display/PIG/StreamingUDFs.\n> In order to implement this we need new syntax to distinguish a streaming UDF from an embedded JVM UDF.  I'd propose something like the following (although I'm not sure 'language' is the best term to be using):\n> {code}define my_streaming_udfs language('python') ship('my_streaming_udfs.py'){code}\n> We'll also need a language-specific controller script that gets shipped to the cluster which is responsible for reading the input stream, deserializing the input data, passing it to the user written script, serializing that script output, and writing that to the output stream.\n> Finally, we'll need to add a StreamingUDF class that extends evalFunc.  This class will likely share some of the existing code in POStream and ExecutableManager (where it make sense to pull out shared code) to stream data to/from the controller script.\n> One alternative approach to creating the StreamingUDF EvalFunc is to use the POStream operator directly.  This would involve inserting the POStream operator instead of the POUserFunc operator whenever we encountered a streaming UDF while building the physical plan.  This approach seemed problematic because there would need to be a lot of changes in order to support POStream in all of the places we want to be able use UDFs (For example - to operate on a single field inside of a for each statement).\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-12-13T03:19:32Z"
  },
  "patches": [],
  "external_id": "PIG-2417"
},{
  "_id": {
    "$oid": "60fac592d907ab79037f3201"
  },
  "message_id": "<18407105.1152445710164.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac4a5d907ab79037ef107"
  },
  "from_id": {
    "$oid": "59bfaa3af2a4565fe90974cd"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Assigned: (DIRSERVER-651) query incorrectly parsed if first\n part contains wild-cards (asterisk) - most prominently for gpg/gnupg",
  "body": "     [ http://issues.apache.org/jira/browse/DIRSERVER-651?page=all ]\n\nEmmanuel Lecharny reassigned DIRSERVER-651:\n-------------------------------------------\n\n    Assign To: Emmanuel Lecharny\n\n> query incorrectly parsed if first part contains wild-cards (asterisk) - most prominently for gpg/gnupg\n> ------------------------------------------------------------------------------------------------------\n>\n>          Key: DIRSERVER-651\n>          URL: http://issues.apache.org/jira/browse/DIRSERVER-651\n>      Project: Directory ApacheDS\n>         Type: Bug\n\n>  Environment: all\n>     Reporter: Ralf Hauser\n>     Assignee: Emmanuel Lecharny\n>  Attachments: ldapAsterisk.txt, ldapNoAsterisk.txt\n>\n> As reported by Valdimir (http://mail-archives.apache.org/mod_mbox/directory-dev/200606.mbox/ajax/%3c4492B645.9020205@netcetera.com.mk%3e)  this query is not handled correctly.\n> In short: \n>   ldapsearch -x -H ldap://localhost:11500 -D \"dn=bugs\" -w bunny -b \"dc=pgpkeys\" \"(&(pgpuserid=test*)(pgpdisabled=0))\"\n> only brings up a SimpleNode instead of a BranchNode.\n> Some further insights:\n> -----------------\n> 1) a unit test on the query with the parser in shared-ldap-0.9.5.jar appears to work:\n>             FilterParserImpl parser = new FilterParserImpl();\n>             ExprNode node = parser\n>                     .parse(\"(&(pgpuserid=*@test*)(pgpdisabled=0))\");\n>   ==> a BranchNode is returned here, but not when using apacheDS\n> 2) when switching the order of the sub-queries, I do see the BranchNode even when using apacheDS with both parts:\n>      ldapsearch -x -H ldap://localhost:2389 -d5 -D \"dn=bugs\" -w bunny -b \"dc=pgpkeys\" \"(&(pgpdisabled=0)(pgpuserid=@test*))\"\n> 3) increasing the debug level to \"ldapsearch -d10\" hints that the full query is sent to apacheDS and not only the \"pgpdisabled=0\" part\n> 4) when setting a break-point in org.apache.directory.shared.ldap.filter.FilterParserImpl, it appears that when doing my tests, the parse() is never called??\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-07-09T11:48:30Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-651"
},{
  "_id": {
    "$oid": "59bf821b69c36c4f7adfef40"
  },
  "message_id": "<JIRA.13099266.1504283656000.11972.1504292040168@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "59bf821b69c36c4f7adfef38"
    },
    {
      "$oid": "59bf821b69c36c4f7adfef39"
    }
  ],
  "in_reply_to_id": {
    "$oid": "59bf821b69c36c4f7adfef38"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-2887) define dependency versions in\n build.xml to be easily overridden in build.properties",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-2887?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16151019#comment-16151019 ] \n\nASF GitHub Bot commented on ZOOKEEPER-2887:\n-------------------------------------------\n\nGitHub user tamaashu opened a pull request:\n\n    https://github.com/apache/zookeeper/pull/357\n\n    ZOOKEEPER-2887: define dependency versions in build.xml to be easily …\n\n    …overridden in build.properties\n    \n    If the dependency versions are defined in build.xml they can be easily\n    overridden by re-defining them in build.properties\n    This process can be useful to avoid classpath clashes among different\n    Hadoop components\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/tamaashu/zookeeper ZOOKEEPER-2887\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/zookeeper/pull/357.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #357\n    \n----\ncommit 6cf315fdd6e7df2d091a5cd3a9753cb40d4357fe\nAuthor: Tamas Penzes <tamaas@cloudera.com>\nDate:   2017-09-01T18:50:23Z\n\n    ZOOKEEPER-2887: define dependency versions in build.xml to be easily overridden in build.properties\n    \n    If the dependency versions are defined in build.xml they can be easily\n    overridden by re-defining them in build.properties\n    This process can be useful to avoid classpath clashes among different\n    Hadoop components\n\n----\n\n\n> define dependency versions in build.xml to be easily overridden in build.properties\n> -----------------------------------------------------------------------------------\n>\n>                 Key: ZOOKEEPER-2887\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-2887\n>             Project: ZooKeeper\n>          Issue Type: Improvement\n>          Components: build\n>            Reporter: Tamas Penzes\n>            Assignee: Tamas Penzes\n>\n> Dependency versions are defined in ivy.xml, which is suboptimal since it is hard to override them from a script.\n> If we defined the versions in the main build.xml (just as we do with audience-annotations.version) and use variables in ivy.xml then we could easily override the versions with creating a build.properties file, which mechanism is already built in.\n> This way the dependency versions could be replaced by sed or any simple command line tool.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-09-01T18:54:00Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-2887"
},{
  "_id": {
    "$oid": "5c5808f5e078b00ec4e788c3"
  },
  "message_id": "<JIRA.12751837.1414716279000.378446.1414716454472@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c5808f4e078b00ec4e788c0"
    },
    {
      "$oid": "5c5808f4e078b00ec4e788c1"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c5808f4e078b00ec4e788c0"
  },
  "from_id": {
    "$oid": "5bdc01e735e3ea2b7bba62eb"
  },
  "to_ids": [
    {
      "$oid": "5c5808dd621a9a77b3c44af1"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (ARGUS-144) Leverage Directory Computed\n  Attribute for User Group Discovery",
  "body": "\n     [ https://issues.apache.org/jira/browse/ARGUS-144?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDilli Arumugam resolved ARGUS-144.\n----------------------------------\n    Resolution: Invalid\n\nfiled on a wrong project, meant for Knox\n\n> Leverage Directory Computed  Attribute for User Group Discovery\n> ---------------------------------------------------------------\n>\n>                 Key: ARGUS-144\n>                 URL: https://issues.apache.org/jira/browse/ARGUS-144\n>             Project: Argus\n>          Issue Type: Improvement\n>            Reporter: Dilli Arumugam\n>            Assignee: Dilli Arumugam\n>\n> Leverage Directory Computed  Attribute for User Group Discovery\n> We should use computed attribute memberof supported by Active Driectory to discover groups of the authenticated user. This would significantly boost performance as compared we computing groups using group search.\n> OpenLDAP also could be configured to return computed groups.\n> However, OpenLDAP would return this attribute as memberof.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-10-31T00:47:34Z"
  },
  "patches": [],
  "external_id": "ARGUS-144"
},{
  "_id": {
    "$oid": "60fac55cd907ab79037f2333"
  },
  "message_id": "<5588123.1175083172771.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "60fac450f73e2aa390ce1e38"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Closed: (DIR-154) Authentication mechanisms",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIR-154?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEmmanuel Lecharny closed DIR-154.\n---------------------------------\n\n    Resolution: Fixed\n\nThere is not much explanation about this issue... What is this supposed to be ? I guess that the recent implementation of SASL by Enrique cover it, so I close it.\n\n> Authentication mechanisms\n> -------------------------\n>\n>                 Key: DIR-154\n>                 URL: https://issues.apache.org/jira/browse/DIR-154\n>             Project: Directory\n>          Issue Type: New Feature\n>          Components: sandbox\n>            Reporter: Vincent Tence\n>         Assigned To: Vincent Tence\n>\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-03-28T04:59:32Z"
  },
  "patches": [],
  "external_id": "DIR-154"
},{
  "_id": {
    "$oid": "5bbdab77c764eb6c7a26249a"
  },
  "message_id": "<JIRA.12920454.1449672796000.313513.1449672850981@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdab77c764eb6c7a262498"
    },
    {
      "$oid": "5bbdab77c764eb6c7a262499"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdab77c764eb6c7a262498"
  },
  "from_id": {
    "$oid": "5bbdaac357674ee167d04309"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (FALCON-1655) Database creation fails with\n FileNotFound exception / Permission denied",
  "body": "Pragya Mittal created FALCON-1655:\n-------------------------------------\n\n             Summary: Database creation fails with FileNotFound exception / Permission denied\n                 Key: FALCON-1655\n                 URL: https://issues.apache.org/jira/browse/FALCON-1655\n             Project: Falcon\n          Issue Type: Bug\n          Components: scheduler\n    Affects Versions: 0.9\n         Environment: QA\n            Reporter: Pragya Mittal\n\n\nWhile creating database in falcon when state store is enabled in native schedular it fails with following error saying Permission denied :\n{noformat}\ndataqa@lda01:/mnt/falcon/server$ ./bin/falcon-db.sh create -sqlfile falcon.sql -run\nHadoop is installed, adding hadoop classpath to falcon classpath\nValidating DB Connection\nlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /.application.log (Permission denied)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseCategory(DOMConfigurator.java:436)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1004)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:66)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:270)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\n\tat org.apache.falcon.util.ApplicationProperties.<clinit>(ApplicationProperties.java:43)\n\tat org.apache.falcon.tools.FalconStateStoreDBCLI.getJdbcConf(FalconStateStoreDBCLI.java:208)\n\tat org.apache.falcon.tools.FalconStateStoreDBCLI.createConnection(FalconStateStoreDBCLI.java:344)\n\tat org.apache.falcon.tools.FalconStateStoreDBCLI.validateConnection(FalconStateStoreDBCLI.java:352)\n\tat org.apache.falcon.tools.FalconStateStoreDBCLI.createDB(FalconStateStoreDBCLI.java:249)\n\tat org.apache.falcon.tools.FalconStateStoreDBCLI.run(FalconStateStoreDBCLI.java:103)\n\tat org.apache.falcon.tools.FalconStateStoreDBCLI.main(FalconStateStoreDBCLI.java:57)\nlog4j:ERROR Either File or DatePattern options are not set for appender [FILE].\n{noformat}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-12-09T14:54:10Z"
  },
  "patches": [],
  "external_id": "FALCON-1655"
},{
  "_id": {
    "$oid": "5f27d16e532b7277349c784e"
  },
  "message_id": "<2726090.1170129003916.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d169532b7277349c76c9"
  },
  "from_id": {
    "$oid": "5f27d169af02e2d6de9964ce"
  },
  "to_ids": [
    {
      "$oid": "5f27d012af02e2d6de9082c9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (AMQ-1142) TemporaryQueue communication not\n working",
  "body": "\n    [ https://issues.apache.org/activemq/browse/AMQ-1142?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_38421 ] \n\nSystem Administrator commented on AMQ-1142:\n-------------------------------------------\n\nYour message\n\n  To:      jos.v.roosmalen@nl.fortis.com\n  Subject: [jira] Commented: (AMQ-1142) TemporaryQueue communication not\nworking\n  Sent:    Tue, 30 Jan 2007 04:48:04 +0100\n\ndid not reach the following recipient(s):\n\nc=NL;a=400Net;p=Fortis;o=DBIB0000;dda:SMTP=JOS.V.ROOSMALEN@NL.FORTIS.COM; on\nTue, 30 Jan 2007 04:48:08 +0100\n    The recipient name is not recognized\n\tThe MTS-ID of the original message is:\nc=nl;a=400net;p=fortis;l=DBIBAA060701300348CXXNB96S\n    MSEXCH:IMS:Fortis:DBIB0000:DBIBAA06 0 (000C05A6) Unknown Recipient\n\n\n\n\n****************************DISCLAIMER***********************************\nDeze e-mail is uitsluitend bestemd voor de geadresseerde(n). Verstrekking aan en gebruik door anderen is niet toegestaan. Fortis sluit iedere aansprakelijkheid uit die voortvloeit uit electronische verzending. \n\nThis e-mail is intended exclusively for the addressee(s), and may not be passed on to, or made available for use by any person other than the addressee(s). Fortis rules out any and every liability resulting from any electronic transmission. \n******************************************************************************\n\n\n> TemporaryQueue communication not working\n> ----------------------------------------\n>\n>                 Key: AMQ-1142\n>                 URL: https://issues.apache.org/activemq/browse/AMQ-1142\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: Broker, JMS client\n>    Affects Versions: 4.1.0\n>         Environment: WinXP, Java 1.4.2\n>            Reporter: Jos\n>         Attachments: Client.java, Server.java\n>\n>\n> Assume a (old legacy, yes this is out of date and better solutions are available) point to point architecture:\n> Client creates temporary queues and send the queuename to the server by a fixed queue ('command queue')\n> Server uses the received queue names to send messages to the client.\n> See the attached Code snippets:\n> * Start the Server\n> * Start the Client\n> * The Client creates a temporary queue and sends the name to the server\n> * Server receives message and sends a test message back.\n> * Client NEVER receives the test message\n> This works 100% on IBM MQSeries (the only source diff is the line that has a MQSerie getQueue API call, instead a createQueue, because queus are defined, and not dynamical as in ActiveMQ)\n> ActiveMQ LOG information:\n> The temporary queue is logged, but later it says that there are no subscriptions\n> (..)\n> DEBUG AbstractRegion                 - Adding destination: topic://ActiveMQ.Advisory.Consumer.Queue.ID:TIGER-2615-1170064714906-1:0:1\n> DEBUG JournalPersistenceAdapter      - Checkpoint done.\n> DEBUG TIGER-2615-1170064714906-1:0:1 - No subscriptions registered, will not dispatch message at this time.\n> (...)\n> Can this bug be fixed?\n> Thank you, Jos\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-01-29T19:50:03Z"
  },
  "patches": [],
  "external_id": "AMQ-1142"
},{
  "_id": {
    "$oid": "5bdc01d916772b6055c7f9cb"
  },
  "message_id": "<JIRA.12824286.1430144694000.38299.1430145520454@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdc01c916772b6055c7f885"
    },
    {
      "$oid": "5bdc01c916772b6055c7f884"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdc01c916772b6055c7f884"
  },
  "from_id": {
    "$oid": "58c9de5702ca40f8bf20c1ae"
  },
  "to_ids": [
    {
      "$oid": "5bdc002135e3ea2b7bb8a8be"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KNOX-534) Audit log for failed authentication\n attempts",
  "body": "\n    [ https://issues.apache.org/jira/browse/KNOX-534?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14514214#comment-14514214 ] \n\nASF subversion and git services commented on KNOX-534:\n------------------------------------------------------\n\nCommit cc37cf2fd73ac678373f535248901586fac03084 in knox's branch refs/heads/master from [~sumit.gupta]\n[ https://git-wip-us.apache.org/repos/asf?p=knox.git;h=cc37cf2 ]\n\nKNOX-534 auditing shiro authentication exceptions\n\n\n> Audit log for failed authentication attempts\n> --------------------------------------------\n>\n>                 Key: KNOX-534\n>                 URL: https://issues.apache.org/jira/browse/KNOX-534\n>             Project: Apache Knox\n>          Issue Type: Bug\n>          Components: Server\n>    Affects Versions: 0.6.0, 0.7.0\n>            Reporter: Sumit Gupta\n>            Assignee: Sumit Gupta\n>             Fix For: 0.6.0, 0.7.0\n>\n>\n> The Shiro provider doesn't currently audit any failed authentication attempts when a user not found in the directory.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-04-27T14:38:40Z"
  },
  "patches": [],
  "external_id": "KNOX-534"
},{
  "_id": {
    "$oid": "5bc8658757a11257de56751a"
  },
  "message_id": "<JIRA.12768967.1421844992000.133782.1421845055315@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8658757a11257de567517"
    },
    {
      "$oid": "5bc8658757a11257de567518"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8658757a11257de567517"
  },
  "from_id": {
    "$oid": "5bc8658757674ee167d2aa53"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-2616) JVM crashes while trying to convert\n PDF to JPG image (only on Windows)",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-2616?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJochen Hebbrecht updated PDFBOX-2616:\n-------------------------------------\n    Attachment: screenshot-1.png\n\n> JVM crashes while trying to convert PDF to JPG image (only on Windows)\n> ----------------------------------------------------------------------\n>\n>                 Key: PDFBOX-2616\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-2616\n>             Project: PDFBox\n>          Issue Type: Bug\n>    Affects Versions: 1.8.8\n>         Environment: Windows 7 \n>            Reporter: Jochen Hebbrecht\n>         Attachments: screenshot-1.png\n>\n>\n> Hi,\n> We got the following exception when trying to convert a PDF to an image:\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-01-21T12:57:35Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2616"
},{
  "_id": {
    "$oid": "60fd85639445ff90d5e5bbaa"
  },
  "message_id": "<296854425.15220.1311845949527.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "60fd84bf9445ff90d5e5927e"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fd85639445ff90d5e5bba9"
  },
  "from_id": {
    "$oid": "58c9df0a02ca40f8bf20c25d"
  },
  "to_ids": [
    {
      "$oid": "5f27d08eaf02e2d6de9300d7"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (OWB-599) move getBeanXmls() back to Set<URL",
  "body": "\n     [ https://issues.apache.org/jira/browse/OWB-599?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMark Struberg resolved OWB-599.\n-------------------------------\n\n    Resolution: Fixed\n\nfixed.\n\n> move getBeanXmls() back to Set<URL\n> ----------------------------------\n>\n>                 Key: OWB-599\n>                 URL: https://issues.apache.org/jira/browse/OWB-599\n>             Project: OpenWebBeans\n>          Issue Type: Bug\n>          Components: Core\n>    Affects Versions: 1.1.0\n>            Reporter: Mark Struberg\n>            Assignee: Mark Struberg\n>             Fix For: 1.1.1\n>\n>\n> Since a few containers cannot reconstruct a new URL(stringUrl) from an URL.toExternalForm(), we need to switch back our SPI to Set<URL>.\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-07-28T09:39:09Z"
  },
  "patches": [],
  "external_id": "OWB-599"
},{
  "_id": {
    "$oid": "5bbdac15c764eb6c7a2639ad"
  },
  "message_id": "<JIRA.12819250.1428486592000.192642.1440755386036@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdaa78c764eb6c7a260358"
  },
  "reference_ids": [
    {
      "$oid": "5bbdac15c764eb6c7a2639ab"
    },
    {
      "$oid": "5bbdac15c764eb6c7a2639ac"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdac15c764eb6c7a2639ab"
  },
  "from_id": {
    "$oid": "5bbdaad357674ee167d07385"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce195"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FALCON-1144) Dynamic partitions not getting\n registered in Hcat",
  "body": "\n     [ https://issues.apache.org/jira/browse/FALCON-1144?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAjay Yadava updated FALCON-1144:\n--------------------------------\n    Affects Version/s:     (was: 0.6.1)\n                       0.6\n\n> Dynamic partitions not getting registered in Hcat\n> -------------------------------------------------\n>\n>                 Key: FALCON-1144\n>                 URL: https://issues.apache.org/jira/browse/FALCON-1144\n>             Project: Falcon\n>          Issue Type: Bug\n>          Components: common\n>    Affects Versions: 0.6\n>         Environment: QA\n>            Reporter: Pragya Mittal\n>            Assignee: Suhas Vasu\n>            Priority: Critical\n>             Fix For: 0.6.1\n>\n>         Attachments: FALCON-1144.patch\n>\n>\n> While registering dynamic partitions via the Monitoring plugin for a process, java.lang.NoSuchMethodError is thrown. Below is the stacktrace :\n> {code}\n> 2015-04-07 13:46:21,224 INFO  - [ActiveMQ Session Task:] ~ Creating HCatalog client object for metastore thrift://192.168.138.200:9083 using conf Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml (HiveCatalogService:85)\n> 2015-04-07 13:46:21,274 INFO  - [ActiveMQ Session Task:] ~ Trying to connect to metastore with URI thrift://192.168.138.200:9083 (metastore:297)\n> 2015-04-07 13:46:21,275 INFO  - [ActiveMQ Session Task:] ~ Connected to metastore. (metastore:385)\n> 2015-04-07 13:46:21,284 ERROR - [ActiveMQ Session Task:] ~ Error in listener org.apache.falcon.catalog.CatalogPartitionHandler (WorkflowJobEndNotificationService:104)\n> java.lang.NoSuchMethodError: org.apache.commons.lang.StringUtils.repeat(Ljava/lang/String;Ljava/lang/String;I)Ljava/lang/String;\n>         at org.apache.falcon.catalog.CatalogPartitionHandler.registerPartitions(CatalogPartitionHandler.java:181)\n>         at org.apache.falcon.catalog.CatalogPartitionHandler.onSuccess(CatalogPartitionHandler.java:155)\n>         at org.apache.falcon.workflow.WorkflowJobEndNotificationService.notifySuccess(WorkflowJobEndNotificationService.java:101)\n>         at org.apache.falcon.messaging.JMSMessageConsumer.onSuccess(JMSMessageConsumer.java:138)\n>         at org.apache.falcon.messaging.JMSMessageConsumer.onMessage(JMSMessageConsumer.java:110)\n>         at org.apache.activemq.ActiveMQMessageConsumer.dispatch(ActiveMQMessageConsumer.java:1229)\n>         at org.apache.activemq.ActiveMQSessionExecutor.dispatch(ActiveMQSessionExecutor.java:134)\n>         at org.apache.activemq.ActiveMQSessionExecutor.iterate(ActiveMQSessionExecutor.java:205)\n>         at org.apache.activemq.thread.PooledTaskRunner.runTask(PooledTaskRunner.java:122)\n>         at org.apache.activemq.thread.PooledTaskRunner$1.run(PooledTaskRunner.java:43)\n>         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n>         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n>         at java.lang.Thread.run(Thread.java:745)\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-08-28T09:49:46Z"
  },
  "patches": [],
  "external_id": "FALCON-1144"
},{
  "_id": {
    "$oid": "5f27be1ba7dc6ca79d80c0e1"
  },
  "message_id": "<JIRA.12690154.1390350983000.60737.1489951482565@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27bd66a7dc6ca79d809788"
  },
  "reference_ids": [
    {
      "$oid": "5f27bdf0a7dc6ca79d80b383"
    },
    {
      "$oid": "5f27bdf0a7dc6ca79d80b384"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bdf0a7dc6ca79d80b383"
  },
  "from_id": {
    "$oid": "59677a03aff2204b3cbd12b8"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c04"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (HTTPCLIENT-1451) HttpClient does not store\n response cookies on a 401",
  "body": "\n    [ https://issues.apache.org/jira/browse/HTTPCLIENT-1451?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15931895#comment-15931895 ] \n\nRichard Sand commented on HTTPCLIENT-1451:\n------------------------------------------\n\nThis issue has been reported 5 other times after this ticket all of which are marked as duplicates. Does this not indicate that the issue is important?\n\n-Richard\n\n> HttpClient does not store response cookies on a 401\n> ---------------------------------------------------\n>\n>                 Key: HTTPCLIENT-1451\n>                 URL: https://issues.apache.org/jira/browse/HTTPCLIENT-1451\n>             Project: HttpComponents HttpClient\n>          Issue Type: Improvement\n>          Components: HttpClient (classic)\n>    Affects Versions: 4.3.2\n>            Reporter: Richard Sand\n>            Priority: Minor\n>             Fix For: 5.0\n>\n>\n> Using HttpClient 4.3.2 to call a Web Service which is secured with BASIC authentication. The server responds to the initial request with a 401 response but also includes a cookie.\n> The HttpClient does not place response cookies into the cookie store until after it has completed the subsequent request with the Authorize header, but the server rejects the authentication if the cookie is missing. \n> To work around this I had to disable the authentication capability in the HttpClientContext and manually check for the 401 response code, and then send a followup request with a manually set Authorize header.\n> So in the use case where the HttpClient is automatically sending a followup request with credentials in response to a 401, the client should place the cookies from the original response into the cookie store immediately, rather than waiting for after the response to the credentials (the 2nd response).\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@hc.apache.org\nFor additional commands, e-mail: dev-help@hc.apache.org\n\n",
  "date": {
    "$date": "2017-03-19T19:24:42Z"
  },
  "patches": [],
  "external_id": "HTTPCLIENT-1451"
},{
  "_id": {
    "$oid": "5bacb419faaadd76f8aa5e25"
  },
  "message_id": "<899631347.257551263536094868.JavaMail.jira@brutus.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5bacb17557674ee167d60c12"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Resolved: (PIG-281) Support # for comment besides --",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-281?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAlan Gates resolved PIG-281.\n----------------------------\n\n    Resolution: Won't Fix\n\n# is the map dereference operator in Pig Latin and thus cannot be the comment operator too.\n\n> Support # for comment besides --\n> --------------------------------\n>\n>                 Key: PIG-281\n>                 URL: https://issues.apache.org/jira/browse/PIG-281\n>             Project: Pig\n>          Issue Type: Improvement\n>            Reporter: Amir Youssefi\n>            Priority: Trivial\n>\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-01-15T06:14:54Z"
  },
  "patches": [],
  "external_id": "PIG-281"
},{
  "_id": {
    "$oid": "5bf68ae3f36e975afa4e5beb"
  },
  "message_id": "<1488200095.9517.1346198167489.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5bf68976f36e975afa4e36af"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bf68adef36e975afa4e5aef"
  },
  "from_id": {
    "$oid": "58bfd16902ca40f8bf148400"
  },
  "to_ids": [
    {
      "$oid": "5bf6898e35e3ea2b7b1be76d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (GIRAPH-301) InputSplit Reservations are\n clumping, leaving many workers asleep while other process too many splits\n and get overloaded.",
  "body": "\n    [ https://issues.apache.org/jira/browse/GIRAPH-301?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13443671#comment-13443671 ] \n\nEli Reisman commented on GIRAPH-301:\n------------------------------------\n\nThats it exactly. The best approach after lots of versions was the 301-6 (now rebased to 301-7) which hashes where the worker will start iterating to distributed the workers across the list better, but places their local blocks at \"adjusted index 0\" if there are any. There is still a small chance a worker who did not find any local blocks of his/her own will hash to start iterating at a block that another worker finds to be local, but from the load in speeds I have gotten over many, many runs over the last week or so combined with the instrumented runs I did, everyone seems to get a split that is local if possible (since usually at least 2-3 candidates exist on any one for a worker to try to claim) and everyone regardless of locality seems to claim a split within 1-3 tests on the split list, which gets us through the input stage much faster.\n\nWhen you try to match splits to workers 1-to-1, often every worker gets a split, occasionally a few will still end up running the list and finding all reserved. This seemed to end up with the fewest wasted workers of anything I tried.\n\nAs for RESERVED change, the comment from 21/Aug/12 17:32 is still exactly what happens, and while I can't confirm I am right about why it works (although I think those comments sum it up) I can say I have tried a lot of weird # of splits / # of workers combos on this patch and it has never had trouble. So I think we're good there. I did (and can step it up if you like) add comments such that anyone trying to implement a recovery plan for failed input reader workers might want to revisit this in the future, but for now I think we're safe and this really cuts down the \"dead time\" when all splits are read and a bunch of workers took forever to figure out the superstep was already effectively over.\n\nThe iterator is a great idea, I can do that right now...patch up in a minute...\n\n\n                \n> InputSplit Reservations are clumping, leaving many workers asleep while other process too many splits and get overloaded.\n> -------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: GIRAPH-301\n>                 URL: https://issues.apache.org/jira/browse/GIRAPH-301\n>             Project: Giraph\n>          Issue Type: Improvement\n>          Components: bsp, graph, zookeeper\n>    Affects Versions: 0.2.0\n>            Reporter: Eli Reisman\n>            Assignee: Eli Reisman\n>              Labels: patch\n>             Fix For: 0.2.0\n>\n>         Attachments: GIRAPH-301-1.patch, GIRAPH-301-2.patch, GIRAPH-301-3.patch, GIRAPH-301-4.patch, GIRAPH-301-5.patch, GIRAPH-301-6.patch, GIRAPH-301-7.patch\n>\n>\n> With recent additions to the codebase, users here have noticed many workers are able to load input splits extremely quickly, and this has altered the behavior of Giraph during INPUT_SUPERSTEP when using the current algorithm for split reservations. A few workers process multiple splits (often overwhelming Netty and getting GC errors as they attempt to offload too much data too quick) while many (often most) of the others just sleep through the superstep, never successfully participating at all.\n> Essentially, the current algo is:\n> 1. scan input split list, skipping nodes that are marked \"Finsihed\"\n> 2. grab the first unfinished node in the list (reserved or not) and check its reserved status.\n> 3. if not reserved, attempt to reserve & return it if successful.\n> 4. if the first one you check is already taken, sleep for way too long and only wake up if another worker finishes a split, then contend with that worker for another split, while the majority of the split list might sit idle, not actually checked or claimed by anyone yet.\n> This does not work. By making a few simple changes (and acknowledging that ZK reads are cheap, only writes are not) this patch is able to get every worker involved, and keep them in the game, ensuring that the INPUT_SUPERSTEP passes quickly and painlessly, and without overwhelming Netty by spreading the memory load the split readers bear more evenly. If the giraph.splitmb and -w options are set correctly, behavior is now exactly as one would expect it to be.\n> This also results in INPUT_SUPERSTEP passing more quickly, and survive the INPUT_SUPERSTEP for a given data load on less Hadoop memory slots.\n>  \n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-08-29T10:56:07Z"
  },
  "patches": [],
  "external_id": "GIRAPH-301"
},{
  "_id": {
    "$oid": "5c5015766b85f47dd6c3c07b"
  },
  "message_id": "<1940731007.1226302724192.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5c5011fb6b85f47dd6c39f9c"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5c5015746b85f47dd6c3c062"
  },
  "from_id": {
    "$oid": "59bf949cf2a4565fe9f07fb0"
  },
  "to_ids": [
    {
      "$oid": "5c5012bb621a9a77b35fb11d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (JSPWIKI-418) ACL stored in metadata with a\n proper editor",
  "body": "\n    [ https://issues.apache.org/jira/browse/JSPWIKI-418?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12646187#action_12646187 ] \n\nJanne Jalkanen commented on JSPWIKI-418:\n----------------------------------------\n\nWell, yes, but the JCR repo will cache the property as an array, which means that every time a new WikiPage is instantiated, it needs to be reparsed.\n\nOtherwise, we'll need to build our own cache on top of everything, if speed is not enough.\n\n> ACL stored in metadata with a proper editor\n> -------------------------------------------\n>\n>                 Key: JSPWIKI-418\n>                 URL: https://issues.apache.org/jira/browse/JSPWIKI-418\n>             Project: JSPWiki\n>          Issue Type: Improvement\n>          Components: Authentication&Authorization, Default template\n>            Reporter: Janne Jalkanen\n>             Fix For: 3.0\n>\n>\n> Hopefully we can get a proper ACL editor for 3.0 - and also store the ACLs in page metadata.\n> Initial proposal: We could store the ACLs under a multi-valued String attribute \"wiki:acl\", with each Value having an string \"ALLOW <id> <permission>\".  Questions: is this too slow to parse?  I would rather stay away from serialization, it may take it difficult to access the repo later on with alternative tools.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-11-09T23:38:44Z"
  },
  "patches": [],
  "external_id": "JSPWIKI-418"
},{
  "_id": {
    "$oid": "5f27b93b641061285052a206"
  },
  "message_id": "<JIRA.12775105.1423893676000.286942.1437780965031@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b782641061285052110a"
    },
    {
      "$oid": "5f27b9236410612850529a25"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b782641061285052110a"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PHOENIX-1661) Implement built-in functions for\n JSON",
  "body": "\n    [ https://issues.apache.org/jira/browse/PHOENIX-1661?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14641242#comment-14641242 ] \n\nASF GitHub Bot commented on PHOENIX-1661:\n-----------------------------------------\n\nGithub user twdsilva commented on a diff in the pull request:\n\n    https://github.com/apache/phoenix/pull/101#discussion_r35475066\n  \n    --- Diff: phoenix-core/src/main/java/org/apache/phoenix/expression/JsonPathAsElementExpression.java ---\n    @@ -0,0 +1,85 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package org.apache.phoenix.expression;\n    +\n    +import java.sql.SQLException;\n    +import java.util.List;\n    +\n    +import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n    +import org.apache.phoenix.expression.visitor.ExpressionVisitor;\n    +import org.apache.phoenix.schema.json.PhoenixJson;\n    +import org.apache.phoenix.schema.tuple.Tuple;\n    +import org.apache.phoenix.schema.types.PDataType;\n    +import org.apache.phoenix.schema.types.PJson;\n    +import org.apache.phoenix.schema.types.PVarchar;\n    +\n    +\n    +public class JsonPathAsElementExpression extends BaseJSONExpression{\n    +\tprivate PDataType datatype=null;\n    +\tpublic JsonPathAsElementExpression(List<Expression> children) {\n    +        super(children);\n    +    }\n    +\tpublic JsonPathAsElementExpression() {\n    +    }\n    +\t@Override\n    +\tpublic boolean evaluate(Tuple tuple, ImmutableBytesWritable ptr)  {\n    +\t\tif (!children.get(1).evaluate(tuple, ptr)) {\n    +            return false;\n    +        }\n    +\t\tString[] pattern =decodePath((String)PVarchar.INSTANCE.toObject(ptr));\n    +\t\tif (!children.get(0).evaluate(tuple, ptr)) {\n    +\t        return false;\n    +\t    }\n    +\t\tPhoenixJson value = (PhoenixJson) PJson.INSTANCE.toObject(ptr);\n    +\t\ttry{\n    +\t\t\t\tPhoenixJson jsonValue=value.getPhoenixJson(pattern);\n    --- End diff --\n    \n    Do you need to add a null check whereever you can getPhoenixJson()?\n\n\n> Implement built-in functions for JSON\n> -------------------------------------\n>\n>                 Key: PHOENIX-1661\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-1661\n>             Project: Phoenix\n>          Issue Type: Sub-task\n>            Reporter: James Taylor\n>              Labels: JSON, Java, SQL, gsoc2015, mentor\n>         Attachments: PhoenixJSONSpecification-First-Draft.pdf\n>\n>\n> Take a look at the JSON built-in functions that are implemented in Postgres (http://www.postgresql.org/docs/9.3/static/functions-json.html) and implement the same for Phoenix in Java following this guide: http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html\n> Examples of functions include ARRAY_TO_JSON, ROW_TO_JSON, TO_JSON, etc. The implementation of these built-in functions will be impacted by how JSON is stored in Phoenix. See PHOENIX-628. An initial implementation could work off of a simple text-based JSON representation and then when a native JSON type is implemented, they could be reworked to be more efficient.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-07-24T23:36:05Z"
  },
  "patches": [],
  "external_id": "PHOENIX-1661"
},{
  "_id": {
    "$oid": "58bfd10215d83644fcc4fd8e"
  },
  "message_id": "<JIRA.12669943.1379894477044.821.1379894522722@arcas>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfd10215d83644fcc4fd8d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfd10215d83644fcc4fd8d"
  },
  "from_id": {
    "$oid": "58bfceb302ca40f8bf147f22"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (ZOOKEEPER-1762) Implement 'check' version cli\n command",
  "body": "Rakesh R created ZOOKEEPER-1762:\n-----------------------------------\n\n             Summary: Implement 'check' version cli command\n                 Key: ZOOKEEPER-1762\n                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-1762\n             Project: ZooKeeper\n          Issue Type: Sub-task\n          Components: java client\n            Reporter: Rakesh R\n            Assignee: Rakesh R\n             Fix For: 3.5.0\n\n\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-09-23T00:02:02Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-1762"
},{
  "_id": {
    "$oid": "5f27c444e2367af241b6ab27"
  },
  "message_id": "<JIRA.12767058.1421136244000.163685.1422065734933@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27c38ce2367af241b67514"
  },
  "reference_ids": [
    {
      "$oid": "5f27c43be2367af241b6a855"
    },
    {
      "$oid": "5f27c43be2367af241b6a854"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27c43be2367af241b6a854"
  },
  "from_id": {
    "$oid": "58bfcfd502ca40f8bf148123"
  },
  "to_ids": [
    {
      "$oid": "58bfd014e4f89451f55ce17b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (BIGTOP-1594) Upgrade Pig to 0.14.0",
  "body": "\n     [ https://issues.apache.org/jira/browse/BIGTOP-1594?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRoman Shaposhnik resolved BIGTOP-1594.\n--------------------------------------\n    Resolution: Fixed\n\n> Upgrade Pig to 0.14.0\n> ---------------------\n>\n>                 Key: BIGTOP-1594\n>                 URL: https://issues.apache.org/jira/browse/BIGTOP-1594\n>             Project: Bigtop\n>          Issue Type: Sub-task\n>          Components: build, debian, rpm\n>            Reporter: YoungWoo Kim\n>            Assignee: YoungWoo Kim\n>             Fix For: 0.9.0\n>\n>         Attachments: BIGTOP-1594.1.patch, BIGTOP-1594.2.patch, BIGTOP-1594.3.patch, BIGTOP-1594.4.patch, BIGTOP-1594.5.patch, BIGTOP-1594.6.patch\n>\n>\n> Bump up Apache Pig version to 0.14.0\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-01-24T02:15:34Z"
  },
  "patches": [],
  "external_id": "BIGTOP-1594"
},{
  "_id": {
    "$oid": "5bdc023916772b6055c801b4"
  },
  "message_id": "<JIRA.12710630.1398455673721.183597.1398456255308@arcas>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdc021116772b6055c7fe57"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdc021116772b6055c7fe57"
  },
  "from_id": {
    "$oid": "59677a36aff2204b3cbd12ff"
  },
  "to_ids": [
    {
      "$oid": "5bdc002135e3ea2b7bb8a8be"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KNOX-353) Support Hadoop java client WebHdfs\n URLs",
  "body": "\n    [ https://issues.apache.org/jira/browse/KNOX-353?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13981520#comment-13981520 ] \n\nLarry McCay commented on KNOX-353:\n----------------------------------\n\nInitial support will be done using redirection.\n\nAdding a specialized topology with a name that matches the configured (or default name) will result in a simple web application with only a web.xml that maps all requests to the GatewayRedirectServlet. Within the web.xml servlet definition an init param will reflect the configured (or default) context path to add to the incoming request URL and redirect to.\n\nThe client should follow the redirect and all further requests will reflect the URLs expected by Knox - including the redirects that are a result of various multi-step Hadoop operations.\n\nRewrites will continue to behave the same way.\n\nWe can consider whether or how this should evolve in a later jira.\n\nIn the mean time, this approach should satisfy usecases where mapreduce jobs would like to access resources through Knox from within the cluster.\n\n> Support Hadoop java client WebHdfs URLs\n> ---------------------------------------\n>\n>                 Key: KNOX-353\n>                 URL: https://issues.apache.org/jira/browse/KNOX-353\n>             Project: Apache Knox\n>          Issue Type: Improvement\n>          Components: Server\n>            Reporter: Larry McCay\n>            Assignee: Larry McCay\n>             Fix For: 0.5.0\n>\n>\n> In order for the gateway to be useful from within the Hadoop cluster, we need to support the URLs used by the Hadoop java client.\n> The WebHdfs URLs from the java client do not and cannot be configured to interject the gateway_path and cluster_name context path values. We therefore need to support a way to deploy a default application that will serve the WebHdfs requests or that will redirect to an application that will.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-04-25T20:04:15Z"
  },
  "patches": [],
  "external_id": "KNOX-353"
},{
  "_id": {
    "$oid": "5bc8504a57a11257de55f40d"
  },
  "message_id": "<JIRA.13149446.1522661548000.160055.1522666020230@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8504a57a11257de55f3fc"
    },
    {
      "$oid": "5bc8504a57a11257de55f3fd"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8504a57a11257de55f3fc"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Comment Edited] (PDFBOX-4176) PDImageXObject doesn't\n support TIFF although JavaDoc says it does",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-4176?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16422113#comment-16422113 ] \n\nTilman Hausherr edited comment on PDFBOX-4176 at 4/2/18 10:46 AM:\n------------------------------------------------------------------\n\nBecause there's a bug in the file type detector. The easiest way for you would either be to use {{PDImageXObject createFromFile()}} or to use {{LosslessFactory.createFromImage(ImageIO.read(file))}}. You'll have to use one of these anyway if your production TIFF files are not G3 or G4 encoded.\n\n\nwas (Author: tilman):\nBecause there's a bug in the file type detector. The easiest way for you would either be to use {{PDImageXObject createFromFile()}} or to use {{LosslessFactory.createFromImage(ImageIO.read(file))}}. You'll have to use one of These anyway if your production TIFF files are not G3 or G4 encoded.\n\n> PDImageXObject doesn't support TIFF although JavaDoc says it does\n> -----------------------------------------------------------------\n>\n>                 Key: PDFBOX-4176\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-4176\n>             Project: PDFBox\n>          Issue Type: Bug\n>    Affects Versions: 2.0.9\n>            Reporter: Kamil\n>            Priority: Major\n>         Attachments: tiff.tif\n>\n>\n> This code:\n> {code}\n> final PDImageXObject jpeg = PDImageXObject.createFromByteArray(doc, IOUtils.toByteArray(new FileInputStream(tiffFile)), \"tiff.tif\");\n> {code}\n> throws:\n> {code}\n> java.lang.IllegalArgumentException: Image type not supported: tiff.tif\n>         at org.apache.pdfbox.pdmodel.graphics.image.PDImageXObject.createFromByteArray(PDImageXObject.java:351)\n> {code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2018-04-02T10:47:00Z"
  },
  "patches": [],
  "external_id": "PDFBOX-4176"
},{
  "_id": {
    "$oid": "60fac2dbc290fbbd482790e1"
  },
  "message_id": "<JIRA.12736150.1408737607343.288.1408922997503@arcas>",
  "mailing_list_id": {
    "$oid": "60fac240c290fbbd4827667b"
  },
  "reference_ids": [
    {
      "$oid": "60fac2d0c290fbbd48278d7b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2d0c290fbbd48278d7b"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbdbe4f89451f55cdffa"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (CURATOR-144) TreeCache should use a builder for\n advanced options",
  "body": "\n    [ https://issues.apache.org/jira/browse/CURATOR-144?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14108627#comment-14108627 ] \n\nASF GitHub Bot commented on CURATOR-144:\n----------------------------------------\n\nGithub user cammckenzie commented on a diff in the pull request:\n\n    https://github.com/apache/curator/pull/41#discussion_r16638791\n  \n    --- Diff: curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java ---\n    @@ -65,6 +67,106 @@\n     {\n         private static final Logger LOG = LoggerFactory.getLogger(TreeCache.class);\n     \n    +    public static final class Builder {\n    --- End diff --\n    \n    nit: Formatting\n\n\n> TreeCache should use a builder for advanced options\n> ---------------------------------------------------\n>\n>                 Key: CURATOR-144\n>                 URL: https://issues.apache.org/jira/browse/CURATOR-144\n>             Project: Apache Curator\n>          Issue Type: Improvement\n>          Components: Recipes\n>            Reporter: Scott Blum\n>            Priority: Minor\n>   Original Estimate: 24h\n>  Remaining Estimate: 24h\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-08-24T23:29:57Z"
  },
  "patches": [],
  "external_id": "CURATOR-144"
},{
  "_id": {
    "$oid": "5f27cf23532b7277349bd887"
  },
  "message_id": "<JIRA.12698796.1393988635302.2970.1393988742484@arcas>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [
    {
      "$oid": "5f27ceb4532b7277349bb69e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ceb4532b7277349bb69e"
  },
  "from_id": {
    "$oid": "5f27ced2af02e2d6de870e6a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (AMQ-5082) ActiveMQ replicatedLevelDB cluster\n breaks, all nodes stop listening",
  "body": "Scott Feldstein created AMQ-5082:\n------------------------------------\n\n             Summary: ActiveMQ replicatedLevelDB cluster breaks, all nodes stop listening\n                 Key: AMQ-5082\n                 URL: https://issues.apache.org/jira/browse/AMQ-5082\n             Project: ActiveMQ\n          Issue Type: Bug\n          Components: activemq-leveldb-store\n    Affects Versions: 5.9.0, 5.10.0\n            Reporter: Scott Feldstein\n            Priority: Critical\n         Attachments: mq-node1-cluster.failure, mq-node2-cluster.failure, mq-node3-cluster.failure, zookeeper.out-cluster.failure\n\nI have a 3 node amq cluster and one zookeeper node using a replicatedLevelDB persistence adapter.\n\n{code}\n        <persistenceAdapter>\n            <replicatedLevelDB\n              directory=\"${activemq.data}/leveldb\"\n              replicas=\"3\"\n              bind=\"tcp://0.0.0.0:0\"\n              zkAddress=\"zookeep0:2181\"\n              zkPath=\"/activemq/leveldb-stores\"/>\n        </persistenceAdapter>\n{code}\n\nAfter about a day or so of sitting idle there are cascading failures and the cluster completely stops listening all together.\n\nI can reproduce this consistently on 5.9 and the latest 5.10 (commit 2360fb859694bacac1e48092e53a56b388e1d2f0).  I am going to attach logs from the three mq nodes and the zookeeper logs that reflect the time where the cluster starts having issues.\n\nThe cluster stops listening Mar 4, 2014 4:56:50 AM (within 5 seconds).\n\nThe OSs are all centos 5.9 on one esx server, so I doubt networking is an issue.\n\nIf you need more data it should be pretty easy to get whatever is needed since it is consistently reproducible.\n\nThis bug may be related to AMQ-5026, but looks different enough to file a separate issue.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-03-05T03:05:42Z"
  },
  "patches": [],
  "external_id": "AMQ-5082"
},{
  "_id": {
    "$oid": "5bbf40d9f81a617b14e5db57"
  },
  "message_id": "<JIRA.13164593.1528331829000.226365.1529712300119@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "5bbf403bf81a617b14e5d26c"
    },
    {
      "$oid": "5bbf403bf81a617b14e5d26b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf403bf81a617b14e5d26b"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a6"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-3057) Fix IPv6 literal usage",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-3057?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16520913#comment-16520913 ] \n\nHadoop QA commented on ZOOKEEPER-3057:\n--------------------------------------\n\n-1 overall.  GitHub Pull Request  Build\n      \n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 11 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 3.0.1) warnings.\n\n    -1 release audit.  The applied patch generated 1 release audit warnings (more than the trunk's current 0 warnings).\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/1859//testReport/\nRelease audit warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/1859//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt\nFindbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/1859//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/1859//console\n\nThis message is automatically generated.\n\n> Fix IPv6 literal usage\n> ----------------------\n>\n>                 Key: ZOOKEEPER-3057\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-3057\n>             Project: ZooKeeper\n>          Issue Type: Bug\n>          Components: other\n>    Affects Versions: 3.4.12\n>            Reporter: Mohamed Jeelani\n>            Priority: Minor\n>              Labels: pull-request-available\n>          Time Spent: 10m\n>  Remaining Estimate: 0h\n>\n> IPv6 literals are not parsed correctly and can lead to potential errors if not be an eye sore. Need to parse and display them correctly.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-06-23T00:05:00Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-3057"
},{
  "_id": {
    "$oid": "58bfcf6c15d83644fcc4a800"
  },
  "message_id": "<JIRA.12781000.1426025158000.33441.1465421301051@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfcf6915d83644fcc4a747"
    },
    {
      "$oid": "58bfcf6915d83644fcc4a748"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfcf6915d83644fcc4a747"
  },
  "from_id": {
    "$oid": "58bfceb102ca40f8bf147f1a"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (ZOOKEEPER-2137) Make testPortChange() less flaky",
  "body": "\n     [ https://issues.apache.org/jira/browse/ZOOKEEPER-2137?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMichael Han updated ZOOKEEPER-2137:\n-----------------------------------\n    Attachment: ZOOKEEPER-2137.patch\n\nRework patch based on feedback from Alex and Raul: separate creation of znode for reader / writer zookeeper handles using a dedicated function.\n\n> Make testPortChange() less flaky\n> --------------------------------\n>\n>                 Key: ZOOKEEPER-2137\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-2137\n>             Project: ZooKeeper\n>          Issue Type: Sub-task\n>            Reporter: Hongchao Deng\n>            Assignee: Michael Han\n>             Fix For: 3.5.2, 3.6.0\n>\n>         Attachments: ZOOKEEPER-2137-cb.patch, ZOOKEEPER-2137.patch, ZOOKEEPER-2137.patch, ZOOKEEPER-2137.patch, ZOOKEEPER-2137.patch\n>\n>\n> The cause of flaky failure of testPortChange() is a race in sync().\n> I figured out it could take some time to fix sync(). Meanwhile, we can make testPortChange() less flaky by doing reconfig on the leader. We can change this back in the fix of ZOOKEEPER-2136.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-06-08T21:28:21Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-2137"
},{
  "_id": {
    "$oid": "5f27d27c46816ce7cf503856"
  },
  "message_id": "<121529069.1131373386210.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5f27d1fdaf02e2d6de9b5ca5"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Created: (MEV-185) commons-logging-1.0.4 POM dependencies not using <optional> properly",
  "body": "commons-logging-1.0.4 POM dependencies not using <optional> properly\n--------------------------------------------------------------------\n\n         Key: MEV-185\n         URL: http://jira.codehaus.org/browse/MEV-185\n     Project: Maven Evangelism\n        Type: Bug\n  Components: Dependencies  \n    Reporter: Fabrice BELLINGARD\n\n\nIn the following dependency :\n\n    <dependency>\n      <groupId>avalon-framework</groupId>\n      <artifactId>avalon-framework</artifactId>\n      <version>4.1.3</version>\n      <optional/>\n    </dependency>\n\n\"<optional>true</optional>\" should be used.\n\nI haven't checked for the other versions of the artifact.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2005-11-07T08:23:06Z"
  },
  "patches": [],
  "external_id": "MEV-185"
},{
  "_id": {
    "$oid": "5bc85d036e373d4fe81c6280"
  },
  "message_id": "<1799473008.53628.1350421753926.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "58bfd04702ca40f8bf1481e7"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1da"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FLUME-1144) Update Flume User Guide with a\n section on Flume properties such as flume.called.from.service",
  "body": "\n     [ https://issues.apache.org/jira/browse/FLUME-1144?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nBrock Noland updated FLUME-1144:\n--------------------------------\n\n    Fix Version/s:     (was: v1.3.0)\n                   v1.4.0\n    \n> Update Flume User Guide with a section on Flume properties such as flume.called.from.service\n> --------------------------------------------------------------------------------------------\n>\n>                 Key: FLUME-1144\n>                 URL: https://issues.apache.org/jira/browse/FLUME-1144\n>             Project: Flume\n>          Issue Type: Bug\n>          Components: Docs\n>    Affects Versions: v1.2.0\n>            Reporter: Will McQueen\n>             Fix For: v1.4.0\n>\n>\n> Flume User Guide is at flume-ng-doc/xhtml/FlumeUserGuide.xhtml\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-10-16T21:09:13Z"
  },
  "patches": [],
  "external_id": "FLUME-1144"
},{
  "_id": {
    "$oid": "58bfd05f15d83644fcc4dac3"
  },
  "message_id": "<JIRA.12736199.1408749811000.109355.1411528834398@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfcf7315d83644fcc4a99a"
    },
    {
      "$oid": "58bfcf7315d83644fcc4a999"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfcf7315d83644fcc4a999"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a6"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-2019) Unhandled exception when\n setting invalid limits data in /zookeeper/quota/some/path/zookeeper_limits",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-2019?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14145820#comment-14145820 ] \n\nHadoop QA commented on ZOOKEEPER-2019:\n--------------------------------------\n\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12670883/ZOOKEEPER-2019-v3.patch\n  against trunk revision 1626008.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2348//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2348//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2348//console\n\nThis message is automatically generated.\n\n> Unhandled exception when setting invalid limits data in /zookeeper/quota/some/path/zookeeper_limits \n> ----------------------------------------------------------------------------------------------------\n>\n>                 Key: ZOOKEEPER-2019\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-2019\n>             Project: ZooKeeper\n>          Issue Type: Bug\n>          Components: server\n>            Reporter: Raul Gutierrez Segales\n>            Assignee: Raul Gutierrez Segales\n>             Fix For: 3.5.1\n>\n>         Attachments: ZOOKEEPER-2019-v2.patch, ZOOKEEPER-2019-v3.patch, ZOOKEEPER-2019-ver1.patch, ZOOKEEPER-2019.patch, ZOOKEEPER-2019.patch\n>\n>\n> If you have quotas properly set for a given path, i.e.:\n> {noformat}\n> create /zookeeper/quota/test/zookeeper_limits 'count=1,bytes=100'\n> create /zookeeper/quota/test/zookeeper_stats 'count=1,bytes=100'\n> {noformat}\n> and then you update the limits znode with bogus data, i.e.:\n> {noformat}\n> set /zookeeper/quota/test/zookeeper_limits ''\n> {noformat}\n> you'll crash the cluster because IllegalArgumentException isn't handled when dealing with quotas znodes:\n> https://github.com/apache/zookeeper/blob/ZOOKEEPER-823/src/java/main/org/apache/zookeeper/server/DataTree.java#L379\n> https://github.com/apache/zookeeper/blob/ZOOKEEPER-823/src/java/main/org/apache/zookeeper/server/DataTree.java#L425\n> We should handle IllegalArgumentException. Optionally, we should also throw BadArgumentsException from PrepRequestProcessor. \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-09-24T03:20:34Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-2019"
},{
  "_id": {
    "$oid": "5bbf0b3635bc96443481905e"
  },
  "message_id": "<JIRA.12915479.1448303560000.74028.1468217531062@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf090e35bc964434814f5a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0afe35bc964434818711"
    },
    {
      "$oid": "5bbf0afe35bc964434818710"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0afe35bc964434818710"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdff0"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (STORM-1277) port backtype.storm.daemon.executor\n to java",
  "body": "\n    [ https://issues.apache.org/jira/browse/STORM-1277?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15370235#comment-15370235 ] \n\nASF GitHub Bot commented on STORM-1277:\n---------------------------------------\n\nGithub user unsleepy22 commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1445#discussion_r70205892\n  \n    --- Diff: storm-core/src/jvm/org/apache/storm/executor/Executor.java ---\n    @@ -0,0 +1,567 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.storm.executor;\n    +\n    +import clojure.lang.IFn;\n    +import com.google.common.annotations.VisibleForTesting;\n    +import com.google.common.collect.Lists;\n    +import com.lmax.disruptor.EventHandler;\n    +import com.lmax.disruptor.dsl.ProducerType;\n    +import java.io.IOException;\n    +import java.net.UnknownHostException;\n    +import java.util.concurrent.atomic.AtomicBoolean;\n    +import java.util.concurrent.atomic.AtomicReference;\n    +import org.apache.storm.Config;\n    +import org.apache.storm.Constants;\n    +import org.apache.storm.StormTimer;\n    +import org.apache.storm.cluster.ClusterStateContext;\n    +import org.apache.storm.cluster.ClusterUtils;\n    +import org.apache.storm.cluster.DaemonType;\n    +import org.apache.storm.cluster.IStormClusterState;\n    +import org.apache.storm.daemon.GrouperFactory;\n    +import org.apache.storm.daemon.StormCommon;\n    +import org.apache.storm.daemon.Task;\n    +import org.apache.storm.executor.bolt.BoltExecutor;\n    +import org.apache.storm.executor.error.IReportError;\n    +import org.apache.storm.executor.error.ReportError;\n    +import org.apache.storm.executor.error.ReportErrorAndDie;\n    +import org.apache.storm.executor.spout.SpoutExecutor;\n    +import org.apache.storm.generated.Bolt;\n    +import org.apache.storm.generated.DebugOptions;\n    +import org.apache.storm.generated.Grouping;\n    +import org.apache.storm.generated.SpoutSpec;\n    +import org.apache.storm.generated.StormTopology;\n    +import org.apache.storm.grouping.LoadAwareCustomStreamGrouping;\n    +import org.apache.storm.metric.api.IMetric;\n    +import org.apache.storm.metric.api.IMetricsConsumer;\n    +import org.apache.storm.stats.BoltExecutorStats;\n    +import org.apache.storm.stats.CommonStats;\n    +import org.apache.storm.stats.SpoutExecutorStats;\n    +import org.apache.storm.stats.StatsUtil;\n    +import org.apache.storm.task.WorkerTopologyContext;\n    +import org.apache.storm.tuple.AddressedTuple;\n    +import org.apache.storm.tuple.Fields;\n    +import org.apache.storm.tuple.Tuple;\n    +import org.apache.storm.tuple.TupleImpl;\n    +import org.apache.storm.tuple.Values;\n    +import org.apache.storm.utils.ConfigUtils;\n    +import org.apache.storm.utils.DisruptorBackpressureCallback;\n    +import org.apache.storm.utils.DisruptorQueue;\n    +import org.apache.storm.utils.Time;\n    +import org.apache.storm.utils.Utils;\n    +import org.apache.storm.utils.WorkerBackpressureThread;\n    +import org.json.simple.JSONValue;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import java.util.*;\n    +import java.util.concurrent.Callable;\n    +\n    +public abstract class Executor implements Callable, EventHandler {\n    +\n    +    private static final Logger LOG = LoggerFactory.getLogger(Executor.class);\n    +\n    +    protected final Map workerData;\n    +    protected final WorkerTopologyContext workerTopologyContext;\n    +    protected final List<Long> executorId;\n    +    protected final List<Integer> taskIds;\n    +    protected final String componentId;\n    +    protected final AtomicBoolean openOrPrepareWasCalled;\n    +    protected final Map stormConf;\n    +    protected final Map conf;\n    +    protected final String stormId;\n    +    protected final HashMap sharedExecutorData;\n    +    protected final AtomicBoolean stormActive;\n    +    protected final AtomicReference<Map<String, DebugOptions>> stormComponentDebug;\n    +    protected final Runnable suicideFn;\n    +    protected final IStormClusterState stormClusterState;\n    +    protected final Map<Integer, String> taskToComponent;\n    +    protected CommonStats stats;\n    +    protected final Map<Integer, Map<Integer, Map<String, IMetric>>> intervalToTaskToMetricToRegistry;\n    +    protected final Map<String, Map<String, LoadAwareCustomStreamGrouping>> streamToComponentToGrouper;\n    +    protected final ReportErrorAndDie reportErrorDie;\n    +    protected final Callable<Boolean> sampler;\n    +    protected final AtomicBoolean backpressure;\n    +    protected ExecutorTransfer executorTransfer;\n    +    protected final String type;\n    +    protected final AtomicBoolean throttleOn;\n    +    protected IFn transferFn;\n    +\n    +    protected final IReportError reportError;\n    +    protected final Random rand;\n    +    protected final DisruptorQueue transferQueue;\n    +    protected final DisruptorQueue receiveQueue;\n    +    protected Map<Integer, Task> idToTask;\n    +    protected final Map<String, String> credentials;\n    +    protected final Boolean isDebug;\n    +    protected final Boolean isEventLoggers;\n    +    protected String hostname;\n    +\n    +    protected Executor(Map workerData, List<Long> executorId, Map<String, String> credentials) {\n    +        this.workerData = workerData;\n    +        this.executorId = executorId;\n    +        this.workerTopologyContext = StormCommon.makeWorkerContext(workerData);\n    +        this.taskIds = StormCommon.executorIdToTasks(executorId);\n    +        this.componentId = workerTopologyContext.getComponentId(taskIds.get(0));\n    +        this.openOrPrepareWasCalled = new AtomicBoolean(false);\n    +        this.stormConf = normalizedComponentConf((Map) workerData.get(\"storm-conf\"), workerTopologyContext, componentId);\n    +        this.receiveQueue = (DisruptorQueue) (((Map) workerData.get(\"executor-receive-queue-map\")).get(executorId));\n    +        this.stormId = (String) workerData.get(\"storm-id\");\n    +        this.conf = (Map) workerData.get(\"conf\");\n    +        this.sharedExecutorData = new HashMap();\n    +        this.stormActive = (AtomicBoolean) workerData.get(\"storm-active-atom\");\n    +        this.stormComponentDebug = (AtomicReference<Map<String, DebugOptions>>) workerData.get(\"storm-component->debug-atom\");\n    +\n    +        this.transferQueue = mkExecutorBatchQueue(stormConf, executorId);\n    +        this.transferFn = (IFn) workerData.get(\"transfer-fn\");\n    +        this.executorTransfer = new ExecutorTransfer(workerTopologyContext, transferQueue, stormConf, transferFn);\n    +\n    +        this.suicideFn = (Runnable) workerData.get(\"suicide-fn\");\n    +        try {\n    +            this.stormClusterState = ClusterUtils.mkStormClusterState(workerData.get(\"state-store\"), Utils.getWorkerACL(stormConf),\n    +                    new ClusterStateContext(DaemonType.SUPERVISOR));\n    +        } catch (Exception e) {\n    +            throw Utils.wrapInRuntime(e);\n    +        }\n    +\n    +        StormTopology topology = workerTopologyContext.getRawTopology();\n    +        Map<String, SpoutSpec> spouts = topology.get_spouts();\n    +        Map<String, Bolt> bolts = topology.get_bolts();\n    +        if (spouts.containsKey(componentId)) {\n    +            this.type = StatsUtil.SPOUT;\n    +            this.stats = new SpoutExecutorStats(ConfigUtils.samplingRate(stormConf));\n    +        } else if (bolts.containsKey(componentId)) {\n    +            this.type = StatsUtil.BOLT;\n    +            this.stats = new BoltExecutorStats(ConfigUtils.samplingRate(stormConf));\n    +        } else {\n    +            throw new RuntimeException(\"Could not find \" + componentId + \" in \" + topology);\n    +        }\n    +\n    +        this.intervalToTaskToMetricToRegistry = new HashMap<>();\n    +        this.taskToComponent = (Map<Integer, String>) workerData.get(\"task->component\");\n    +        this.streamToComponentToGrouper = outboundComponents(workerTopologyContext, componentId, stormConf);\n    +        this.reportError = new ReportError(stormConf, stormClusterState, stormId, componentId, workerTopologyContext);\n    +        this.reportErrorDie = new ReportErrorAndDie(reportError, suicideFn);\n    +        this.sampler = ConfigUtils.mkStatsSampler(stormConf);\n    +        this.backpressure = new AtomicBoolean(false);\n    +        this.throttleOn = (AtomicBoolean) workerData.get(\"throttle-on\");\n    +        this.isDebug = Utils.getBoolean(stormConf.get(Config.TOPOLOGY_DEBUG), false);\n    +        this.rand = new Random(Utils.secureRandomLong());\n    +        this.credentials = credentials;\n    +        this.isEventLoggers = StormCommon.hasEventLoggers(stormConf);\n    +\n    +        try {\n    +            this.hostname = Utils.hostname(stormConf);\n    +        } catch (UnknownHostException ignored) {\n    +            this.hostname = \"\";\n    +        }\n    +    }\n    +\n    +    public static Executor mkExecutor(Map workerData, List<Long> executorId, Map<String, String> credentials) {\n    +        Executor executor;\n    +\n    +        Map<String, Object> convertedWorkerData = Utils.convertMap(workerData);\n    +        WorkerTopologyContext workerTopologyContext = StormCommon.makeWorkerContext(convertedWorkerData);\n    +        List<Integer> taskIds = StormCommon.executorIdToTasks(executorId);\n    +        String componentId = workerTopologyContext.getComponentId(taskIds.get(0));\n    +\n    +        String type = getExecutorType(workerTopologyContext, componentId);\n    +        if (StatsUtil.SPOUT.equals(type)) {\n    +            executor = new SpoutExecutor(convertedWorkerData, executorId, credentials);\n    +            executor.stats = new SpoutExecutorStats(ConfigUtils.samplingRate(executor.getStormConf()));\n    +        } else {\n    +            executor = new BoltExecutor(convertedWorkerData, executorId, credentials);\n    +            executor.stats = new BoltExecutorStats(ConfigUtils.samplingRate(executor.getStormConf()));\n    +        }\n    +\n    +        Map<Integer, Task> idToTask = new HashMap<>();\n    +        for (Integer taskId : taskIds) {\n    +            try {\n    +                Task task = new Task(executor, taskId);\n    +                executor.sendUnanchored(task, StormCommon.SYSTEM_STREAM_ID, new Values(\"startup\"), executor.getExecutorTransfer());\n    +                idToTask.put(taskId, task);\n    +            } catch (IOException ex) {\n    +                throw Utils.wrapInRuntime(ex);\n    +            }\n    +        }\n    +        executor.init(idToTask);\n    +\n    +        return executor;\n    +    }\n    +\n    +    private static String getExecutorType(WorkerTopologyContext workerTopologyContext, String componentId) {\n    +        StormTopology topology = workerTopologyContext.getRawTopology();\n    +        Map<String, SpoutSpec> spouts = topology.get_spouts();\n    +        Map<String, Bolt> bolts = topology.get_bolts();\n    +        if (spouts.containsKey(componentId)) {\n    +            return StatsUtil.SPOUT;\n    +        } else if (bolts.containsKey(componentId)) {\n    +            return StatsUtil.BOLT;\n    +        } else {\n    +            throw new RuntimeException(\"Could not find \" + componentId + \" in \" + topology);\n    +        }\n    +    }\n    +\n    +    /**\n    +     * separated from mkExecutor in order to replace executor transfer in executor data for testing\n    +     */\n    +    public ExecutorShutdown execute() throws Exception {\n    +        LOG.info(\"Loading executor tasks \" + componentId + \":\" + executorId);\n    +\n    +        registerBackpressure();\n    +        Utils.SmartThread systemThreads =\n    +                Utils.asyncLoop(executorTransfer, executorTransfer.getName(), reportErrorDie);\n    +\n    +        String handlerName = componentId + \"-executor\" + executorId;\n    +        Utils.SmartThread handlers = Utils.asyncLoop(this, false, reportErrorDie, Thread.NORM_PRIORITY, false, true, handlerName);\n    +        setupTicks(StatsUtil.SPOUT.equals(type));\n    +        LOG.info(\"Finished loading executor \" + componentId + \":\" + executorId);\n    +        return new ExecutorShutdown(this, Lists.newArrayList(systemThreads, handlers), idToTask);\n    +    }\n    +\n    +    public abstract void tupleActionFn(int taskId, TupleImpl tuple) throws Exception;\n    +\n    +    public abstract void init(Map<Integer, Task> idToTask);\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    @Override\n    +    public void onEvent(Object event, long seq, boolean endOfBatch) throws Exception {\n    +        ArrayList<AddressedTuple> addressedTuples = (ArrayList<AddressedTuple>) event;\n    +        for (AddressedTuple addressedTuple : addressedTuples) {\n    +            TupleImpl tuple = (TupleImpl) addressedTuple.getTuple();\n    +            int taskId = addressedTuple.getDest();\n    +            if (isDebug) {\n    +                LOG.info(\"Processing received message FOR {} TUPLE: {}\", taskId, tuple);\n    +            }\n    +            if (taskId != AddressedTuple.BROADCAST_DEST) {\n    +                tupleActionFn(taskId, tuple);\n    +            } else {\n    +                for (Integer t : taskIds) {\n    +                    tupleActionFn(t, tuple);\n    +                }\n    +            }\n    +        }\n    +    }\n    +\n    +    public void metricsTick(Task taskData, TupleImpl tuple) {\n    +        try {\n    +            Integer interval = tuple.getInteger(0);\n    +            int taskId = taskData.getTaskId();\n    +            Map<Integer, Map<String, IMetric>> taskToMetricToRegistry = intervalToTaskToMetricToRegistry.get(interval);\n    +            Map<String, IMetric> nameToRegistry = null;\n    +            if (taskToMetricToRegistry != null) {\n    +                nameToRegistry = taskToMetricToRegistry.get(taskId);\n    +            }\n    +            if (nameToRegistry != null) {\n    +                IMetricsConsumer.TaskInfo taskInfo = new IMetricsConsumer.TaskInfo(hostname, workerTopologyContext.getThisWorkerPort(),\n    +                        componentId, taskId, Time.currentTimeSecs(), interval);\n    +                List<IMetricsConsumer.DataPoint> dataPoints = new ArrayList<>();\n    +                for (Map.Entry<String, IMetric> entry : nameToRegistry.entrySet()) {\n    +                    IMetric metric = entry.getValue();\n    +                    Object value = metric.getValueAndReset();\n    +                    if (value != null) {\n    +                        IMetricsConsumer.DataPoint dataPoint = new IMetricsConsumer.DataPoint(entry.getKey(), value);\n    +                        dataPoints.add(dataPoint);\n    +                    }\n    +                }\n    +                if (!dataPoints.isEmpty()) {\n    +                    sendUnanchored(taskData, Constants.METRICS_STREAM_ID, new Values(taskInfo, dataPoints), executorTransfer);\n    +                }\n    +            }\n    +        } catch (Exception e) {\n    +            throw Utils.wrapInRuntime(e);\n    +        }\n    +    }\n    +\n    +    protected void setupMetrics() {\n    +        for (final Integer interval : intervalToTaskToMetricToRegistry.keySet()) {\n    +            StormTimer timerTask = (StormTimer) workerData.get(\"user-timer\");\n    +            timerTask.scheduleRecurring(interval, interval, new Runnable() {\n    +                @Override\n    +                public void run() {\n    +                    TupleImpl tuple =\n    +                            new TupleImpl(workerTopologyContext, new Values(interval), (int) Constants.SYSTEM_TASK_ID, Constants.METRICS_TICK_STREAM_ID);\n    +                    List<AddressedTuple> metricsTickTuple = Lists.newArrayList(new AddressedTuple(AddressedTuple.BROADCAST_DEST, tuple));\n    +                    receiveQueue.publish(metricsTickTuple);\n    +                }\n    +            });\n    +        }\n    +    }\n    +\n    +    public void sendUnanchored(Task task, String stream, List<Object> values, ExecutorTransfer transfer) {\n    +        Tuple tuple = task.getTuple(stream, values);\n    +        List<Integer> tasks = task.getOutgoingTasks(stream, values);\n    +        if (tasks.size() == 0) {\n    +            return;\n    +        }\n    +        for (Integer t : tasks) {\n    +            transfer.transfer(t, tuple);\n    +        }\n    +    }\n    +\n    +    /**\n    +     * Send sampled data to the eventlogger if the global or component level debug flag is set (via nimbus api).\n    +     */\n    +    public void sendToEventLogger(Executor executor, Task taskData, List values, String componentId, Object messageId, Random random) {\n    +        Map<String, DebugOptions> componentDebug = executor.getStormComponentDebug().get();\n    +        DebugOptions debugOptions = componentDebug.get(componentId);\n    --- End diff --\n    \n    since `componentDebug.get(componentId)` will be called twice, I personally prefer current approach, what do you think?\n\n\n> port backtype.storm.daemon.executor to java\n> -------------------------------------------\n>\n>                 Key: STORM-1277\n>                 URL: https://issues.apache.org/jira/browse/STORM-1277\n>             Project: Apache Storm\n>          Issue Type: New Feature\n>          Components: storm-core\n>            Reporter: Robert Joseph Evans\n>            Assignee: Cody\n>              Labels: java-migration, jstorm-merger\n>\n> https://github.com/apache/storm/tree/jstorm-import/jstorm-core/src/main/java/com/alibaba/jstorm/task kind of.  Tasks and executors are combined in jstorm.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-07-11T06:12:11Z"
  },
  "patches": [],
  "external_id": "STORM-1277"
},{
  "_id": {
    "$oid": "5bbf110cb79d666cbb233599"
  },
  "message_id": "<JIRA.12626087.1357327194128.115016.1357845493708@arcas>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0ff5b79d666cbb231574"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0ff5b79d666cbb231574"
  },
  "from_id": {
    "$oid": "58bfd20902ca40f8bf148601"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-683) Fix correlation ids in all requests\n sent to kafka",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-683?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13549953#comment-13549953 ] \n\nJun Rao commented on KAFKA-683:\n-------------------------------\n\nI agree that seeing the whole request is not useful, especially Message, since it's binary. Maybe what we should do is to fix the toString() method in each request so that it only prints out meaningful info. This can be fixed in a separate jira. One benefit of this is that we can remove the extra deserialization of those 3 special fields in RequestChannel, which depends on all requests having those 3 fields in the same order and seems brittle.\n                \n> Fix correlation ids in all requests sent to kafka\n> -------------------------------------------------\n>\n>                 Key: KAFKA-683\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-683\n>             Project: Kafka\n>          Issue Type: Improvement\n>    Affects Versions: 0.8\n>            Reporter: Neha Narkhede\n>            Assignee: Neha Narkhede\n>            Priority: Critical\n>              Labels: improvement, replication\n>         Attachments: kafka-683-v1.patch, kafka-683-v2.patch, kafka-683-v2-rebased.patch\n>\n>\n> We should fix the correlation ids in every request sent to Kafka and fix the request log on the broker to specify not only the type of request and who sent it, but also the correlation id. This will be very helpful while troubleshooting problems in production.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-01-10T19:18:13Z"
  },
  "patches": [],
  "external_id": "KAFKA-683"
},{
  "_id": {
    "$oid": "5bacc4ae56f6a00b020937b7"
  },
  "message_id": "<JIRA.12910980.1446796864000.1352.1446835331110@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc4a056f6a00b02093386"
    },
    {
      "$oid": "5bacc4a056f6a00b02093387"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc4a056f6a00b02093386"
  },
  "from_id": {
    "$oid": "5bacc47157674ee167dcadb8"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (MAHOUT-1785) Replace\n 'spark.kryoserializer.buffer.mb' from Spark config",
  "body": "\n    [ https://issues.apache.org/jira/browse/MAHOUT-1785?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14994178#comment-14994178 ] \n\nPat Ferrel commented on MAHOUT-1785:\n------------------------------------\n\nThis happens because Spark is changing the way a conf param is being used. The warning seems to apply into Spark 1.6-SNAPSHOT so if it works, not a blocker and we have to have it for Spark 1.4.1 or below.\n\nSo we can't do anything about this until we require 1.5.1, which is not in Mahout 0.11.1 so defer this.\n\n> Replace 'spark.kryoserializer.buffer.mb' from Spark config\n> ----------------------------------------------------------\n>\n>                 Key: MAHOUT-1785\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-1785\n>             Project: Mahout\n>          Issue Type: Improvement\n>          Components: Mahout spark shell\n>    Affects Versions: 0.11.0\n>            Reporter: Suneel Marthi\n>            Assignee: Suneel Marthi\n>            Priority: Trivial\n>             Fix For: 0.12.0\n>\n>\n> 'spark.kryoserializer.buffer.mb' has been deprecated as of spark 1.4 and should be replaced by 'spark.kryoserializer.buffer'\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-11-06T18:42:11Z"
  },
  "patches": [],
  "external_id": "MAHOUT-1785"
},{
  "_id": {
    "$oid": "60fac487d907ab79037ee8c8"
  },
  "message_id": "<450919628.1250325675361.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac466d907ab79037edf84"
  },
  "from_id": {
    "$oid": "59bfaa3af2a4565fe90974cd"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DIRSERVER-1010) Should the order in which hot\n partitions are connected to matter?",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSERVER-1010?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEmmanuel Lecharny updated DIRSERVER-1010:\n-----------------------------------------\n\n    Fix Version/s:     (was: 1.5.9)\n                   2.0.0-RC1\n\nAffected to 2.0.0-RC1, we won't release a 1.5.9\n\n> Should the order in which hot partitions are connected to matter?\n> -----------------------------------------------------------------\n>\n>                 Key: DIRSERVER-1010\n>                 URL: https://issues.apache.org/jira/browse/DIRSERVER-1010\n>             Project: Directory ApacheDS\n>          Issue Type: Bug\n>    Affects Versions: 1.5.0\n>            Reporter: Ole Ersoy\n>             Fix For: 2.0.0-RC1\n>\n>\n> If I connect in this order:\n>         dasContext = \n>             adsEmbeddedConnection.\n>             connect( \n>                 configuration.getDasPartitionName() );\n>         schemaContext = \n>             adsEmbeddedConnection.\n>             connect( \n>                 configuration.getSchemaPartitionName() );\n> I don't get any exceptions.\n> If I flip the two, I get this exception:\n> org.apache.directory.shared.ldap.exception.LdapNameNotFoundException: ou=das\n> \tat org.apache.directory.server.core.partition.DefaultPartitionNexus.getBackend(DefaultPartitionNexus.java:1064)\n> \tat org.apache.directory.server.core.partition.DefaultPartitionNexus.hasEntry(DefaultPartitionNexus.java:988)\n> \tat org.apache.directory.server.core.interceptor.InterceptorChain$1.hasEntry(InterceptorChain.java:147)\n> \tat org.apache.directory.server.core.interceptor.InterceptorChain$Entry$1.hasEntry(InterceptorChain.java:1246)\n> \tat org.apache.directory.server.core.interceptor.BaseInterceptor.hasEntry(BaseInterceptor.java:130)\n> \tat org.apache.directory.server.core.interceptor.InterceptorChain$Entry$1.hasEntry(InterceptorChain.java:1246)\n> \tat org.apache.directory.server.core.interceptor.BaseInterceptor.hasEntry(BaseInterceptor.java:130)\n> .....\n> I'll check in the working DAS in a few days and in the package:\n> package org.apache.tuscany.das.ldap.emf.test;\n> There is a test called LdapDASHelperTest that can be run to see this.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2009-08-15T01:41:15Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-1010"
},{
  "_id": {
    "$oid": "5bc867d757a11257de568b89"
  },
  "message_id": "<JIRA.12752055.1414785534000.385237.1414785813678@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8667557a11257de567c71"
    },
    {
      "$oid": "5bc8667557a11257de567c70"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8667557a11257de567c70"
  },
  "from_id": {
    "$oid": "5bc8667957674ee167d3060d"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PDFBOX-2471) AES encryption failing to write\n Acroform field names and values",
  "body": "\n    [ https://issues.apache.org/jira/browse/PDFBOX-2471?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14192363#comment-14192363 ] \n\nStephen Hendrix commented on PDFBOX-2471:\n-----------------------------------------\n\nUsing pdf.save as follows:\n\n{code}\nAccessPermission ap = new AccessPermission();\nStandardProtectionPolicy policy = new StandardProtectionPolicy(password, password, ap);\n\npolicy.setEncryptionKeyLength(256)\npdf.protect(policy);\npdf.save(outputStream)\n{code}\n\n> AES encryption failing to write Acroform field names and values\n> ---------------------------------------------------------------\n>\n>                 Key: PDFBOX-2471\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-2471\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: AcroForm, Writing\n>    Affects Versions: 2.0.0\n>            Reporter: Stephen Hendrix\n>\n> When writing a PDF using AES 256 bit encryption, the field names / values from the Acroform are not being persisted correctly. If I encrypt using RC4 128 bit, they are persisted correctly. I am using snapshot pdfbox:2.0.0-20141023.180319-636.\n> I dug into this, and it looks to me the problem is with COSWriter.visitFromString, which invokes  SecurityHandler.decryptString (there is no SecurityHandler.encryptString).\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-10-31T20:03:33Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2471"
},{
  "_id": {
    "$oid": "5bacb37efaaadd76f8aa39e8"
  },
  "message_id": "<630041363.1085.1303774443275.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb36cfaaadd76f8aa3583"
  },
  "from_id": {
    "$oid": "58bfc8f202ca40f8bf147929"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PIG-1824) Support import modules in Jython UDF",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-1824?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nWoody Anderson updated PIG-1824:\n--------------------------------\n\n    Attachment: 1824c.patch\n\n> Support import modules in Jython UDF\n> ------------------------------------\n>\n>                 Key: PIG-1824\n>                 URL: https://issues.apache.org/jira/browse/PIG-1824\n>             Project: Pig\n>          Issue Type: Improvement\n>    Affects Versions: 0.8.0, 0.9.0\n>            Reporter: Richard Ding\n>            Assignee: Woody Anderson\n>             Fix For: 0.9.0\n>\n>         Attachments: 1824.patch, 1824a.patch, 1824b.patch, 1824c.patch\n>\n>\n> Currently, Jython UDF script doesn't support Jython import statement as in the following example:\n> {code}\n> #!/usr/bin/python\n> import re\n> @outputSchema(\"word:chararray\")\n> def resplit(content, regex, index):\n>         return re.compile(regex).split(content)[index]\n> {code}\n> Can Pig automatically locate the Jython module file and ship it to the backend? Or should we add a ship clause to let user explicitly specify the module to ship? \n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-04-25T23:34:03Z"
  },
  "patches": [],
  "external_id": "PIG-1824"
},{
  "_id": {
    "$oid": "5f27cf8c442ab9b9860f77c2"
  },
  "message_id": "<1484662467.2443.1344876518174.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27cf8c442ab9b9860f77b8"
  },
  "from_id": {
    "$oid": "59bfae13f2a4565fe910881a"
  },
  "to_ids": [
    {
      "$oid": "5bc58f4f57674ee167e6e3ec"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (OOZIE-948) Add support for Oozie coordinator to\n work in an UTC offset",
  "body": "\n    [ https://issues.apache.org/jira/browse/OOZIE-948?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13433281#comment-13433281 ] \n\nAlejandro Abdelnur commented on OOZIE-948:\n------------------------------------------\n\nUpdated the RB patch based on Virag's feedback.\n                \n> Add support for Oozie coordinator to work in an UTC offset\n> ----------------------------------------------------------\n>\n>                 Key: OOZIE-948\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-948\n>             Project: Oozie\n>          Issue Type: Improvement\n>          Components: bundle, coordinator\n>    Affects Versions: 3.2.0\n>            Reporter: Alejandro Abdelnur\n>            Assignee: Alejandro Abdelnur\n>            Priority: Critical\n>             Fix For: trunk\n>\n>\n> Current Oozie coordinator expects and resolves dates in UTC (ie {{2009-08-10T00:00Z}}). UTC datetimes are used for start/end/pause of jobs, datasets initial-instance and to resolve dataset instance URI templates.\n> Adding support for a non UTC timezone it would enable deployments where they use a timezone different than UTC as standard. This seems quite common in countries where they don't observe DST changes.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-08-14T03:48:38Z"
  },
  "patches": [],
  "external_id": "OOZIE-948"
},{
  "_id": {
    "$oid": "5f27cfbe532b7277349c04fd"
  },
  "message_id": "<2116219557.54949.1350435243057.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27cfbe532b7277349c04fb"
  },
  "from_id": {
    "$oid": "5f27cf19af02e2d6de89aa1f"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (AMQ-4051) IOException: Could not locate data\n file .\\db-10.log",
  "body": "\n    [ https://issues.apache.org/jira/browse/AMQ-4051?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13477506#comment-13477506 ] \n\nEdwin Yu commented on AMQ-4051:\n-------------------------------\n\nHi Claudio,\n\nThanks for looking into this issue.  To answer your questions:\n\n1) You can ignore carinaIOExceptionHandler for the purpose of this jira issue.  We needed it because we need to cleanly shut down the JVM when the persistent directory is filled up, or when the persistent directory is on a network drive that becomes unreachable.  I don't think it's involved for this issue.\n\n2) In our use case, there were two nodes, each of which embedded an ActiveMQ broker with persistent storage.  A mule jms outbound connector heavily sent messages to a queue on the first node, which defined a network connector that dynamically included the destination to a second node.  A mule jms inbound connector on the second node listened to this queue.  This jms inbound connector is the only consumer of this queue.  It permanently connected.  In the healthy operation, this consumer was able to keep up consuming the incoming messages.  The queue size stayed under 2 most of the time.  When this error occurred, messages from the first node were still being delivered to the second node just fine.  It was the second node that complained about the missing data file, and the queue size started increasing and the dequeue count stayed flat.\n\n3) The mule jms inbound connector used the vm protocol.  It was defined like below.\n    <jms:activemq-connector\n            name=\"jmsExternalConnector\"\n            specification=\"1.1\"\n            disableTemporaryReplyToDestinations=\"true\"\n            brokerURL=\"vm://xps-amq-broker_HPC-HAIFA-HAIFA-HAIFA\"\n            numberOfConsumers=\"1\"\n            maxRedelivery=\"1\"\n            persistentDelivery=\"true\">\n    </jms:activemq-connector>\n\n\n4) We're planning to do upgrade to v5.7, but it'll take couple months before we roll out to our performance lab to do load testing.  Plus, it may take few weeks of operation to see it happen again.\n\nLastly, I still have the remaining journal data in the persistent folder from the error case.  If they are useful for your investigation, I can try to upload to this jira issue.  It's about 100MB in total.  They are named:\n\ndb.data\ndb.redo\ndb-11.log\ndb-12.log\ndb-13.log\nlock\n\nThanks,\n-Edwin\n                \n> IOException: Could not locate data file .\\db-10.log\n> ---------------------------------------------------\n>\n>                 Key: AMQ-4051\n>                 URL: https://issues.apache.org/jira/browse/AMQ-4051\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: Broker\n>    Affects Versions: 5.6.0\n>         Environment: Windows Server 2008 R2, 8GB, Intel Core 2 Duo CPU 64 bit\n> JDK 1.6.0_26, 64 bit\n>            Reporter: Edwin Yu\n>            Priority: Critical\n>             Fix For: 5.8.0\n>\n>         Attachments: activemq.xml\n>\n>\n> We're doing load testing on the ActiveMQ 5.6.0 broker-to-broker connection, pumping in about 80,000 messages per hour into the queue.  One of the brokers stops dequeueing messages to the consumer while its queue size increases.  The log contains repeating errors about \"could not locate the data file.\"\n> We encountered this same issue couple weeks ago.  I found AMQ-3120 and added ignoreMissingJouralFiles=\"true\" like that issue suggested.  After many more days of testing, today we countered this same problem again.  Our disk space is plenty.  The log is listed below.  I attached our activemq configuration from the troubled broker to this issue.\n> ------------------------------------------------------------------\n> 2012-09-14 16:01:47,354 ERROR | BrokerService[xps-amq-broker_HPC-HAIFA-HAIFA-HAIFA] Task-18 | [Journal:352] Looking for key 10 but not found in fileMap: {11=db-11.log number = 11 , length = 499163}\n> 2012-09-14 16:01:47,354 ERROR | BrokerService[xps-amq-broker_HPC-HAIFA-HAIFA-HAIFA] Task-18 | [AbstractStoreCursor:279] org.apache.activemq.broker.region.cursors.QueueStorePrefetch@5fae7502:n4.HPC/HAIFA/HAIFA/HAIFA,batchResetNeeded=false,storeHasMessages=true,size=100,cacheEnabled=false - Failed to fill batch\n> java.io.IOException: Could not locate data file .\\esb\\amq_N4-PERFORM70\\db-10.log\n> \tat org.apache.kahadb.journal.Journal.getDataFile(Journal.java:353)\n> \tat org.apache.kahadb.journal.Journal.read(Journal.java:600)\n> \tat org.apache.activemq.store.kahadb.MessageDatabase.load(MessageDatabase.java:924)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore.loadMessage(KahaDBStore.java:1015)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore$4.execute(KahaDBStore.java:556)\n> \tat org.apache.kahadb.page.Transaction.execute(Transaction.java:769)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore.recoverNextMessages(KahaDBStore.java:545)\n> \tat org.apache.activemq.store.ProxyMessageStore.recoverNextMessages(ProxyMessageStore.java:106)\n> \tat org.apache.activemq.broker.region.cursors.QueueStorePrefetch.doFillBatch(QueueStorePrefetch.java:97)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:277)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.reset(AbstractStoreCursor.java:113)\n> \tat org.apache.activemq.broker.region.cursors.StoreQueueCursor.reset(StoreQueueCursor.java:157)\n> \tat org.apache.activemq.broker.region.Queue.doPageInForDispatch(Queue.java:1766)\n> \tat org.apache.activemq.broker.region.Queue.pageInMessages(Queue.java:1995)\n> \tat org.apache.activemq.broker.region.Queue.iterate(Queue.java:1488)\n> \tat org.apache.activemq.thread.PooledTaskRunner.runTask(PooledTaskRunner.java:122)\n> \tat org.apache.activemq.thread.PooledTaskRunner$1.run(PooledTaskRunner.java:43)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n> \tat java.lang.Thread.run(Thread.java:662)\n> 2012-09-14 16:01:47,354 ERROR | BrokerService[xps-amq-broker_HPC-HAIFA-HAIFA-HAIFA] Task-18 | [AbstractStoreCursor:115] org.apache.activemq.broker.region.cursors.QueueStorePrefetch@5fae7502:n4.HPC/HAIFA/HAIFA/HAIFA,batchResetNeeded=false,storeHasMessages=true,size=100,cacheEnabled=false - Failed to fill batch\n> java.lang.RuntimeException: java.io.IOException: Could not locate data file .\\esb\\amq_N4-PERFORM70\\db-10.log\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:280)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.reset(AbstractStoreCursor.java:113)\n> \tat org.apache.activemq.broker.region.cursors.StoreQueueCursor.reset(StoreQueueCursor.java:157)\n> \tat org.apache.activemq.broker.region.Queue.doPageInForDispatch(Queue.java:1766)\n> \tat org.apache.activemq.broker.region.Queue.pageInMessages(Queue.java:1995)\n> \tat org.apache.activemq.broker.region.Queue.iterate(Queue.java:1488)\n> \tat org.apache.activemq.thread.PooledTaskRunner.runTask(PooledTaskRunner.java:122)\n> \tat org.apache.activemq.thread.PooledTaskRunner$1.run(PooledTaskRunner.java:43)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n> \tat java.lang.Thread.run(Thread.java:662)\n> Caused by: java.io.IOException: Could not locate data file .\\esb\\amq_N4-PERFORM70\\db-10.log\n> \tat org.apache.kahadb.journal.Journal.getDataFile(Journal.java:353)\n> \tat org.apache.kahadb.journal.Journal.read(Journal.java:600)\n> \tat org.apache.activemq.store.kahadb.MessageDatabase.load(MessageDatabase.java:924)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore.loadMessage(KahaDBStore.java:1015)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore$4.execute(KahaDBStore.java:556)\n> \tat org.apache.kahadb.page.Transaction.execute(Transaction.java:769)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore.recoverNextMessages(KahaDBStore.java:545)\n> \tat org.apache.activemq.store.ProxyMessageStore.recoverNextMessages(ProxyMessageStore.java:106)\n> \tat org.apache.activemq.broker.region.cursors.QueueStorePrefetch.doFillBatch(QueueStorePrefetch.java:97)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:277)\n> \t... 10 more\n> 2012-09-14 16:01:47,354 ERROR | BrokerService[xps-amq-broker_HPC-HAIFA-HAIFA-HAIFA] Task-18 | [Queue:1491] Failed to page in more queue messages \n> java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: Could not locate data file .\\esb\\amq_N4-PERFORM70\\db-10.log\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.reset(AbstractStoreCursor.java:116)\n> \tat org.apache.activemq.broker.region.cursors.StoreQueueCursor.reset(StoreQueueCursor.java:157)\n> \tat org.apache.activemq.broker.region.Queue.doPageInForDispatch(Queue.java:1766)\n> \tat org.apache.activemq.broker.region.Queue.pageInMessages(Queue.java:1995)\n> \tat org.apache.activemq.broker.region.Queue.iterate(Queue.java:1488)\n> \tat org.apache.activemq.thread.PooledTaskRunner.runTask(PooledTaskRunner.java:122)\n> \tat org.apache.activemq.thread.PooledTaskRunner$1.run(PooledTaskRunner.java:43)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n> \tat java.lang.Thread.run(Thread.java:662)\n> Caused by: java.lang.RuntimeException: java.io.IOException: Could not locate data file .\\esb\\amq_N4-PERFORM70\\db-10.log\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:280)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.reset(AbstractStoreCursor.java:113)\n> \t... 9 more\n> Caused by: java.io.IOException: Could not locate data file .\\esb\\amq_N4-PERFORM70\\db-10.log\n> \tat org.apache.kahadb.journal.Journal.getDataFile(Journal.java:353)\n> \tat org.apache.kahadb.journal.Journal.read(Journal.java:600)\n> \tat org.apache.activemq.store.kahadb.MessageDatabase.load(MessageDatabase.java:924)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore.loadMessage(KahaDBStore.java:1015)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore$4.execute(KahaDBStore.java:556)\n> \tat org.apache.kahadb.page.Transaction.execute(Transaction.java:769)\n> \tat org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore.recoverNextMessages(KahaDBStore.java:545)\n> \tat org.apache.activemq.store.ProxyMessageStore.recoverNextMessages(ProxyMessageStore.java:106)\n> \tat org.apache.activemq.broker.region.cursors.QueueStorePrefetch.doFillBatch(QueueStorePrefetch.java:97)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:277)\n> \t... 10 more\n> 2012-09-14 16:01:47,354 INFO  | ActiveMQ Session Task-26 | [LogComponent:41] \n> --------------------------------------------------------------------------\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-10-17T00:54:03Z"
  },
  "patches": [],
  "external_id": "AMQ-4051"
},{
  "_id": {
    "$oid": "58c11efb6d2aba458ddf7233"
  },
  "message_id": "<1657094.1160072900812.JavaMail.root@brutus>",
  "mailing_list_id": {
    "$oid": "58c117176d2aba458dde60bb"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "58c11efa6d2aba458ddf7217"
  },
  "from_id": {
    "$oid": "58c11e1302ca40f8bfb1f6d4"
  },
  "to_ids": [
    {
      "$oid": "58c11420e4f89451f51d76f1"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (VALIDATOR-202) URL Validator isValid method\n fails with java.lang.ArrayIndexOutOfBoundsException at\n org.apache.commons.validator.UrlValidator.isValidAuthority(UrlValidator.java:367)",
  "body": "    [ http://issues.apache.org/jira/browse/VALIDATOR-202?page=comments#action_12440201 ] \n            \nHenri Yandell commented on VALIDATOR-202:\n-----------------------------------------\n\nSee VALIDATOR-203 for Jira issue to RECODE ME.\n\n> URL Validator isValid method fails with java.lang.ArrayIndexOutOfBoundsException at org.apache.commons.validator.UrlValidator.isValidAuthority(UrlValidator.java:367)\n> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: VALIDATOR-202\n>                 URL: http://issues.apache.org/jira/browse/VALIDATOR-202\n>             Project: Commons Validator\n>          Issue Type: Bug\n>         Environment: JBOSS Running on Linux\n>            Reporter: Ben\n>         Attachments: 202-fix.patch, 202-test.patch\n>\n>\n> Validating the following URL will throw an ArrayIndexOutOfBoundsException exception\n> http://www.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.log\n> Code:\n>   String[] schemes = {\"http\",\"https\"}.\n>   UrlValidator urlValidator = new UrlValidator(schemes, UrlValidator.NO_FRAGMENTS) ;\n>   urlValidator.isValid(\"http://www.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.logoworks.comwww.log\") ;\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: commons-dev-unsubscribe@jakarta.apache.org\nFor additional commands, e-mail: commons-dev-help@jakarta.apache.org\n\n",
  "date": {
    "$date": "2006-10-05T11:28:20Z"
  },
  "patches": [],
  "external_id": "VALIDATOR-202"
},{
  "_id": {
    "$oid": "5f27ce81014d3531c6cc6b70"
  },
  "message_id": "<3738826.1195692523048.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27ce6d014d3531c6cc65cd"
  },
  "from_id": {
    "$oid": "58c11e3f02ca40f8bfb1f764"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Resolved: (JCR-1112) CacheManager interval between\n recalculation of cache sizes should be configurable",
  "body": "\n     [ https://issues.apache.org/jira/browse/JCR-1112?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJukka Zitting resolved JCR-1112.\n--------------------------------\n\n       Resolution: Fixed\n    Fix Version/s: 1.4\n\nResolving as fixed for 1.4 based Przemo's changes in revision 592950.\n\n> CacheManager interval between recalculation of cache sizes should be configurable\n> ---------------------------------------------------------------------------------\n>\n>                 Key: JCR-1112\n>                 URL: https://issues.apache.org/jira/browse/JCR-1112\n>             Project: Jackrabbit\n>          Issue Type: New Feature\n>          Components: jackrabbit-core\n>            Reporter: Przemo Pakulski\n>            Assignee: Thomas Mueller\n>            Priority: Minor\n>             Fix For: 1.4\n>\n>         Attachments: JCR-1112.txt\n>\n>\n> Currently interval between recaluclation of cahce size is hard coded to 1000 ms. Resizing/recalculation of cache size is quite expensive method (especially getMemoryUsed on MLRUItemStateCache is time consuming)\n> Depending on the configuration, we realized that under some load up to 10-15% percent of CPU time (profiler metrics) could be spend doing such recalculations. It does not seem to be needed to resize cache every second. Best this interval should be configurable in external config. file with other cache settings (like memory sizes).\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-11-21T16:48:43Z"
  },
  "patches": [],
  "external_id": "JCR-1112"
},{
  "_id": {
    "$oid": "5bbdf74ee8113566f664f56e"
  },
  "message_id": "<22772189.1189966652858.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbdf735e8113566f664f1f5"
  },
  "from_id": {
    "$oid": "59bfb67df2a4565fe9260575"
  },
  "to_ids": [
    {
      "$oid": "59bf9516f2a4565fe9f28f88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (NUTCH-555) StackOverflowError in DomContentUtils",
  "body": "\n     [ https://issues.apache.org/jira/browse/NUTCH-555?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKarsten Dello updated NUTCH-555:\n--------------------------------\n\n    Attachment: stacktrace.txt\n\n> StackOverflowError in DomContentUtils\n> -------------------------------------\n>\n>                 Key: NUTCH-555\n>                 URL: https://issues.apache.org/jira/browse/NUTCH-555\n>             Project: Nutch\n>          Issue Type: Bug\n>    Affects Versions: 0.9.0\n>            Reporter: Karsten Dello\n>         Attachments: readseg.txt, stacktrace.txt\n>\n>\n> Parsing certain pages (which expose very bad html) causes an stackoverflow error, as the recursion depth is too high (more then 1000).\n> But parsing should be stable, it is probably better to just skip pages like this. \n> Attached it\n> a) the stacktrace\n> b) the segmentreader-get output for the url where the exception is thrown\n> Possible fixes:\n> parseOutlinks in DomContentUtils is implemented recursive. \n> An iterative implementation would fix this, but maybe it is easier to simply  limit the recursion to a reasonable depth. \n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-09-16T11:17:32Z"
  },
  "patches": [],
  "external_id": "NUTCH-555"
},{
  "_id": {
    "$oid": "5bacb31ffaaadd76f8aa242b"
  },
  "message_id": "<1021902083.26616.1332901642427.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb30ffaaadd76f8aa20b3"
  },
  "from_id": {
    "$oid": "5bacb30d57674ee167d90f7a"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PIG-2257) AvroStorage doesn't recognize\n schema_file field when JSON isn't used in the constructor",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-2257?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13240117#comment-13240117 ] \n\nBill Graham commented on PIG-2257:\n----------------------------------\n\nThis is actually a bug that should be patched. All constructs to {{AvroStorage}} should work whether being passed as json or a string array, but {{schema_file}} was only implemented in the former.\n                \n> AvroStorage doesn't recognize schema_file field when JSON isn't used in the constructor\n> ---------------------------------------------------------------------------------------\n>\n>                 Key: PIG-2257\n>                 URL: https://issues.apache.org/jira/browse/PIG-2257\n>             Project: Pig\n>          Issue Type: Bug\n>            Reporter: Bill Graham\n>            Assignee: Bill Graham\n>         Attachments: PIG-2257_1.patch\n>\n>\n> PIG-2195 introduced the {{schema_file}} constructor param to {{AvroStorage}}. This field is currently only supported when passing constructor data via JSON though (a different code path is used when constructor data is a String array).\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-03-28T02:27:22Z"
  },
  "patches": [],
  "external_id": "PIG-2257"
},{
  "_id": {
    "$oid": "5f27cc87014d3531c6cbd628"
  },
  "message_id": "<JIRA.13251391.1566046499000.2289.1566231900655@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [
    {
      "$oid": "5f27cc74014d3531c6cbcedd"
    },
    {
      "$oid": "5f27cc74014d3531c6cbcedc"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cc74014d3531c6cbcedc"
  },
  "from_id": {
    "$oid": "5f27cc6eaf02e2d6de787029"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JCRVLT-355) False error in case embedded\n file/subpackage is overwritten by jcrRootSourceDirectory",
  "body": "\n    [ https://issues.apache.org/jira/browse/JCRVLT-355?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16910533#comment-16910533 ] \n\nStefan Seifert commented on JCRVLT-355:\n---------------------------------------\n\ni've tested it on windows 10\n\n> False error in case embedded file/subpackage is overwritten by jcrRootSourceDirectory\n> -------------------------------------------------------------------------------------\n>\n>                 Key: JCRVLT-355\n>                 URL: https://issues.apache.org/jira/browse/JCRVLT-355\n>             Project: Jackrabbit FileVault\n>          Issue Type: Bug\n>          Components: package maven plugin\n>    Affects Versions: package-maven-plugin-1.0.4\n>            Reporter: Stefan Seifert\n>            Assignee: Konrad Windszus\n>            Priority: Major\n>\n> i've found problem with the check introduced with JCRVLT-279 - or i do not understand why the check leads to a failure in my case.\n> i've two projects where this happens:\n> 1. conf-content:\n> https://github.com/stefanseifert/filevault-package-maven-plugin-1.0.4-validation-issues/tree/master/content-packages/conf-content\n> fails with:\n> {noformat}\n> [ERROR] Failed to execute goal org.apache.jackrabbit:filevault-package-maven-plugin:1.0.4:package (default-package) on project mycompany.myprojectgroup.myproject1.conf-content: org.apache.maven.plugin.MojoFailureException: Found duplicate files in content package, most probably you have overlapping filter roots or you embed a file which is already there in 'jcrRootSourceDirectory'. For details check the nested exception!: Duplicate file jcr_root\\conf\\myproject1\\settings\\wcm\\policies was found and the duplicate attribute is 'fail'. {noformat}\n> 2. ui.apps\n> https://github.com/stefanseifert/filevault-package-maven-plugin-1.0.4-validation-issues/tree/master/content-packages/ui.apps\n> fails with:\n> {noformat}\n> [ERROR] Failed to execute goal org.apache.jackrabbit:filevault-package-maven-plugin:1.0.4:package (default-package) on project mycompany.myprojectgroup.myproject1.ui.apps: org.apache.maven.plugin.MojoFailureException: Found duplicate files in content package, most probably you have overlapping filter roots or you embed a file which is already there in 'jcrRootSourceDirectory'. For details check the nested exception!: Duplicate file jcr_root\\apps\\epsilon65Project\\clientlibs\\.content.xml was found and the duplicate attribute is 'fail'.\n> {noformat}\n> the second project is a special case because in some files in the \"clientlibs\" folder placeholders are replaced using maven resource filtering, if the corresponding build-helper-maven-plugin definition is removed the build works.\n> but the first project is really simple project.\n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.2#803003)\n",
  "date": {
    "$date": "2019-08-19T16:25:00Z"
  },
  "patches": [],
  "external_id": "JCRVLT-355"
},{
  "_id": {
    "$oid": "5bbf0d43b79d666cbb22baec"
  },
  "message_id": "<JIRA.12547718.1332462246000.224097.1437178385688@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0c74b79d666cbb229ac1"
    },
    {
      "$oid": "5bbf0c74b79d666cbb229ac2"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0c74b79d666cbb229ac1"
  },
  "from_id": {
    "$oid": "5bbe0efc57674ee167933827"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-313) Add JSON/CSV output and looping\n options to ConsumerGroupCommand",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-313?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14632153#comment-14632153 ] \n\nAshish K Singh commented on KAFKA-313:\n--------------------------------------\n\n[~nehanarkhede] pinging for review.\n\n> Add JSON/CSV output and looping options to ConsumerGroupCommand\n> ---------------------------------------------------------------\n>\n>                 Key: KAFKA-313\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-313\n>             Project: Kafka\n>          Issue Type: Improvement\n>            Reporter: Dave DeMaagd\n>            Assignee: Ashish K Singh\n>            Priority: Minor\n>              Labels: newbie, patch\n>             Fix For: 0.8.3\n>\n>         Attachments: KAFKA-313-2012032200.diff, KAFKA-313.1.patch, KAFKA-313.patch, KAFKA-313_2015-02-23_18:11:32.patch, KAFKA-313_2015-06-24_11:14:24.patch\n>\n>\n> Adds:\n> * '--loop N' - causes the program to loop forever, sleeping for up to N seconds between loops (loop time minus collection time, unless that's less than 0, at which point it will just run again immediately)\n> * '--asjson' - display as a JSON string instead of the more human readable output format.\n> Neither of the above  depend on each other (you can loop in the human readable output, or do a single shot execution with JSON output).  Existing behavior/output maintained if neither of the above are used.  Diff Attached.\n> Impacted files:\n> core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-07-18T00:13:05Z"
  },
  "patches": [],
  "external_id": "KAFKA-313"
},{
  "_id": {
    "$oid": "5cb58553520bcf17358bcad7"
  },
  "message_id": "<JIRA.13130142.1515664555000.285569.1519735560209@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5cb584eb520bcf17358bc781"
  },
  "reference_ids": [
    {
      "$oid": "5cb58514520bcf17358bc83b"
    },
    {
      "$oid": "5cb58514520bcf17358bc83a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5cb58514520bcf17358bc83a"
  },
  "from_id": {
    "$oid": "58bfc8c302ca40f8bf14789e"
  },
  "to_ids": [
    {
      "$oid": "5bbdf53d57674ee16787bc0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (GORA-530) Reinstate exception throwing at\n Query#execute()",
  "body": "\n    [ https://issues.apache.org/jira/browse/GORA-530?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16378515#comment-16378515 ] \n\nHudson commented on GORA-530:\n-----------------------------\n\nSUCCESS: Integrated in Jenkins build gora-trunk #1744 (See [https://builds.apache.org/job/gora-trunk/1744/])\nGORA-530 : Reinstated exception throwing in DataStore and Query (alfonso.nishikawa: rev b06da5f32ec572c88f7ec5245a4b573c73ae8c22)\n* (edit) gora-core/src/test/java/org/apache/gora/store/DataStoreTestUtil.java\n* (edit) gora-cassandra/src/test/java/org/apache/gora/cassandra/store/TestNativeSerializationWithUDT.java\n* (edit) gora-hbase/src/main/java/org/apache/gora/hbase/store/HBaseStore.java\n* (edit) gora-jcache/src/main/java/org/apache/gora/jcache/store/JCacheCacheLoader.java\n* (edit) gora-accumulo/src/main/java/org/apache/gora/accumulo/store/AccumuloStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/store/ws/impl/WSBackedDataStoreBase.java\n* (edit) gora-jcache/src/main/java/org/apache/gora/jcache/store/JCacheStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/avro/store/AvroStore.java\n* (edit) gora-infinispan/src/test/java/org/apache/gora/infinispan/Utils.java\n* (edit) gora-core/src/main/java/org/apache/gora/query/Query.java\n* (edit) gora-mongodb/src/test/java/org/apache/gora/mongodb/store/TestMongoStore.java\n* (edit) gora-core/src/test/java/org/apache/gora/mock/store/MockDataStore.java\n* (edit) gora-jcache/src/main/java/org/apache/gora/jcache/store/JCacheCacheWriter.java\n* (edit) gora-cassandra/src/main/java/org/apache/gora/cassandra/store/CassandraStore.java\n* (edit) gora-couchdb/src/main/java/org/apache/gora/couchdb/store/CouchDBStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/query/impl/QueryBase.java\n* (edit) gora-core/src/main/java/org/apache/gora/store/impl/FileBackedDataStoreBase.java\n* (edit) gora-cassandra/src/main/java/org/apache/gora/cassandra/serializers/AvroSerializer.java\n* (edit) gora-dynamodb/src/main/java/org/apache/gora/dynamodb/store/DynamoDBStore.java\n* (edit) gora-orientdb/src/main/java/org/apache/gora/orientdb/store/OrientDBStore.java\n* (edit) gora-dynamodb/src/main/java/org/apache/gora/dynamodb/store/DynamoDBAvroStore.java\n* (edit) gora-cassandra/src/main/java/org/apache/gora/cassandra/serializers/NativeSerializer.java\n* (edit) gora-core/src/main/java/org/apache/gora/store/impl/DataStoreBase.java\n* (edit) gora-core/src/main/java/org/apache/gora/query/ws/impl/QueryWSBase.java\n* (edit) gora-aerospike/src/main/java/org/apache/gora/aerospike/store/AerospikeStore.java\n* (edit) gora-dynamodb/src/main/java/org/apache/gora/dynamodb/store/DynamoDBNativeStore.java\n* (edit) gora-solr/src/main/java/org/apache/gora/solr/query/SolrResult.java\n* (edit) gora-cassandra/src/test/java/org/apache/gora/cassandra/store/TestCassandraStoreWithCassandraKey.java\n* (edit) gora-cassandra/src/test/java/org/apache/gora/cassandra/store/TestCassandraStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/memory/store/MemStore.java\n* (edit) gora-infinispan/src/main/java/org/apache/gora/infinispan/store/InfinispanStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/persistency/impl/BeanFactoryImpl.java\n* (edit) gora-cassandra/src/test/java/org/apache/gora/cassandra/store/TestAvroSerializationWithUDT.java\n* (edit) gora-cassandra/src/main/java/org/apache/gora/cassandra/serializers/CassandraSerializer.java\n* (edit) gora-mongodb/src/main/java/org/apache/gora/mongodb/store/MongoStore.java\n* (edit) gora-tutorial/src/main/java/org/apache/gora/tutorial/log/DistributedLogManager.java\n* (edit) gora-core/src/main/java/org/apache/gora/avro/store/DataFileAvroStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/store/DataStore.java\n* (edit) gora-solr/src/main/java/org/apache/gora/solr/store/SolrStore.java\n* (edit) gora-cassandra/src/test/java/org/apache/gora/cassandra/store/TestCassandraStoreWithNativeSerialization.java\n* (edit) gora-core/src/main/java/org/apache/gora/store/ws/impl/WSDataStoreBase.java\n* (edit) gora-couchdb/src/test/java/org/apache/gora/couchdb/store/TestCouchDBStore.java\n* (edit) gora-infinispan/src/main/java/org/apache/gora/infinispan/store/InfinispanClient.java\n* (edit) gora-core/src/test/java/org/apache/gora/memory/store/MemStoreTest.java\nGORA-530: Fixed bug at AccumuloStore#deleteSchema() failing when the (alfonso.nishikawa: rev 73db5970c0248433d25be914308f76b03fdef175)\n* (edit) gora-accumulo/src/main/java/org/apache/gora/accumulo/store/AccumuloStore.java\n* (edit) gora-core/src/main/java/org/apache/gora/store/DataStore.java\n\n\n> Reinstate exception throwing at Query#execute()\n> -----------------------------------------------\n>\n>                 Key: GORA-530\n>                 URL: https://issues.apache.org/jira/browse/GORA-530\n>             Project: Apache Gora\n>          Issue Type: Improvement\n>          Components: gora-core\n>    Affects Versions: 0.8\n>            Reporter: Alfonso Nishikawa\n>            Assignee: Alfonso Nishikawa\n>            Priority: Minor\n>              Labels: query\n>             Fix For: 0.9\n>\n>\n> When executing a query, the exceptions are being shadowed and the execution returns {{null}}.\n> This makes any client to be ignorant of the cause, because it is only logged into the server.\n> The objective is to reinstate the exception throwing, where the interface throws GoraException to make any client aware of the underlying problem.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-02-27T12:46:00Z"
  },
  "patches": [],
  "external_id": "GORA-530"
},{
  "_id": {
    "$oid": "5f27d28746816ce7cf503b66"
  },
  "message_id": "<143653803.1132085886548.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d28346816ce7cf503a49"
  },
  "from_id": {
    "$oid": "5f27d1d9af02e2d6de9b0784"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (MEV-197) Improvements for Commons Chain pom",
  "body": "     [ http://jira.codehaus.org/browse/MEV-197?page=all ]\n\nCarlos Sanchez updated MEV-197:\n-------------------------------\n\n    Attachment:     (was: chain-1.0.diff)\n\n> Improvements for Commons Chain pom\n> ----------------------------------\n>\n>          Key: MEV-197\n>          URL: http://jira.codehaus.org/browse/MEV-197\n>      Project: Maven Evangelism\n>         Type: Improvement\n>   Components: Dependencies\n>     Reporter: Wendy Smoak\n>  Attachments: chain-1.0-20051115.diff\n>\n>\n> Making some dependencies optional, changing to standard groupId for Sun jars\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2005-11-15T14:18:06Z"
  },
  "patches": [],
  "external_id": "MEV-197"
},{
  "_id": {
    "$oid": "60fac57dd907ab79037f2c34"
  },
  "message_id": "<239650.1164640522483.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac573d907ab79037f2985"
  },
  "from_id": {
    "$oid": "5f27c218af02e2d6de6ea2f8"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DIRSTUDIO-2) The DSML Parser doesn't parse well\n base64 Data",
  "body": "     [ http://issues.apache.org/jira/browse/DIRSTUDIO-2?page=all ]\n\nPierre-Arnaud Marcelot updated DIRSTUDIO-2:\n-------------------------------------------\n\n    Component/s: ldapstudio-dsml-parser\n\n> The DSML Parser doesn't parse well base64 Data\n> ----------------------------------------------\n>\n>                 Key: DIRSTUDIO-2\n>                 URL: http://issues.apache.org/jira/browse/DIRSTUDIO-2\n>             Project: Directory LDAP Studio\n>          Issue Type: Bug\n>          Components: ldapstudio-dsml-parser\n>            Reporter: Pierre-Arnaud Marcelot\n>         Assigned To: Pierre-Arnaud Marcelot\n>\n> The DSML Parser doesn't parse well base64 Data. Especially in the ExtendedRequest. \n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2006-11-27T07:15:22Z"
  },
  "patches": [],
  "external_id": "DIRSTUDIO-2"
},{
  "_id": {
    "$oid": "60fac27cc290fbbd482770a1"
  },
  "message_id": "<JIRA.13206395.1545776560000.35658.1546467360236@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac240c290fbbd4827667b"
  },
  "reference_ids": [
    {
      "$oid": "60fac27bc290fbbd48277048"
    },
    {
      "$oid": "60fac27bc290fbbd48277049"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac27bc290fbbd48277048"
  },
  "from_id": {
    "$oid": "58bfceb102ca40f8bf147f16"
  },
  "to_ids": [
    {
      "$oid": "58bfcbdbe4f89451f55cdffa"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (CURATOR-498) LeaderLatch deletes leader and\n leaves it hung besides a second leader",
  "body": "\n    [ https://issues.apache.org/jira/browse/CURATOR-498?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16732470#comment-16732470 ] \n\nJordan Zimmerman commented on CURATOR-498:\n------------------------------------------\n\nI'm working on a new PR that uses a Watcher on the found node. I don't think there's any way around this.\n\n> LeaderLatch deletes leader and leaves it hung besides a second leader\n> ---------------------------------------------------------------------\n>\n>                 Key: CURATOR-498\n>                 URL: https://issues.apache.org/jira/browse/CURATOR-498\n>             Project: Apache Curator\n>          Issue Type: Bug\n>    Affects Versions: 4.0.1, 4.1.0\n>         Environment: ZooKeeper 3.4.13, Curator 4.1.0 (selecting explicitly 3.4.13), Linux\n>            Reporter: Shay Shimony\n>            Assignee: Jordan Zimmerman\n>            Priority: Blocker\n>         Attachments: CURATOR-498.png, HaWatcher.log, LeaderLatch0.java, ha.tar.gz, logs.tar.gz, reproduction.tar.gz\n>\n>\n> The Curator app I am working on uses the LeaderLatch to select a leader out of 6 clients.\n> While testing my app, I noticed that when I make ZK lose its quorum for a while and then restore it, then after Curator in my app restores it's connection to ZK - sometimes not all the 6 clients are found in the latch path (using zkCli.sh). That is, I have 5 instead of 6.\n> After investigating a little, I have a suspicion that LeaderLatch deleted the leader in method setNode.\n> To investigate it I copied the LeaderLatch code and added some log messages, and from them it seems like very old create() background callback was surprisingly scheduled and corrupted the current leader with its stale path name. Meaning, this old one called setNode with its stale name, and set itself instead of the leader and deleted the leader. This leaves client running, thinking it is the leader, while another leader is selected.\n> If my analysis is correct then it seems like we need to make this obsolete create callback cancelled (I think its session was suspended on 22:38:54 and then lost on 22:39:04 - so on SUSPENDED cancel ongoing callbacks).\n> Please see attached log file and modified LeaderLatch0.\n>  \n> In the log, note that on 22:39:26 it shows that 0000000485 is replaced by 0000000480 and then probably deleted.\n> Note also that at 22:38:52, 34 seconds before, we can see that it was in the reset() method (\"RESET OUR PATH\") and possibly triggered the creation of 0000000480 then.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2019-01-02T22:16:00Z"
  },
  "patches": [],
  "external_id": "CURATOR-498"
},{
  "_id": {
    "$oid": "5c57f17c149eba7f382176e7"
  },
  "message_id": "<JIRA.12744670.1411998069000.145606.1412002354055@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c57eed8149eba7f3821534a"
  },
  "reference_ids": [
    {
      "$oid": "5c57f050149eba7f38216769"
    },
    {
      "$oid": "5c57f050149eba7f38216768"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c57f050149eba7f38216768"
  },
  "from_id": {
    "$oid": "596779baaff2204b3cbd1281"
  },
  "to_ids": [
    {
      "$oid": "59677b275005bf27642aa642"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Comment Edited] (CONNECTORS-1052) AlfrescoConnector test is\n ignored",
  "body": "\n    [ https://issues.apache.org/jira/browse/CONNECTORS-1052?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14151744#comment-14151744 ] \n\nKarl Wright edited comment on CONNECTORS-1052 at 9/29/14 2:51 PM:\n------------------------------------------------------------------\n\nHi Abe-san,\n\nWe build the alfresco.war using a Maven pom file that was supplied by Alfresco.  The pom file is complex, interacting with multiple repositories.  It is not a simple download, because the war must be modified to include other components that are not part of the original war.\n\nSo far, none of our ant builds invoke Maven; we have not listed Maven as a requirement for building MCF.  If we are going to invoke it, we must therefore check for its existence first.\n\n\n\n\nwas (Author: kwright@metacarta.com):\nHi Abe-san,\n\nWe build the alfresco.war using a Maven pom file that was supplied by Alfresco.  The pom file is complex, interacting with multiple repositories.  It is not a simple download, because the war must be modified to include other components that are not part of the original war.\n\n\n\n> AlfrescoConnector test is ignored\n> ---------------------------------\n>\n>                 Key: CONNECTORS-1052\n>                 URL: https://issues.apache.org/jira/browse/CONNECTORS-1052\n>             Project: ManifoldCF\n>          Issue Type: Bug\n>          Components: Alfresco connector\n>            Reporter: Shinichiro Abe\n>\n> This test is skipped when we use ant. \n> {noformat}\n> $ ant make-core-deps make-deps build test\n> Or\n> $ cd connectors/alfresco\n> $ ant run-IT-HSQLDB\n> ...\n> pretest-warn:\n>      [echo] Alfresco Connector integration tests cannot be be performed without alfresco.war\n> {noformat}\n> Also, it seems there is a difference between build.xml and pom.xml about test content(alfresco war/client version).\n> I'm not sure what is correct.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-09-29T14:52:34Z"
  },
  "patches": [],
  "external_id": "CONNECTORS-1052"
},{
  "_id": {
    "$oid": "5f27b6f2641061285051de5d"
  },
  "message_id": "<JIRA.13128957.1515199833000.38193.1581374640352@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b6e6641061285051da18"
    },
    {
      "$oid": "5f27b6e6641061285051da19"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b6e6641061285051da18"
  },
  "from_id": {
    "$oid": "5f27b6e6af02e2d6de4fa9b0"
  },
  "to_ids": [
    {
      "$oid": "5bacb13357674ee167d59296"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PHOENIX-4521) Allow Pherf scenario to define per\n table max allowed query duration after which thread is interrupted",
  "body": "\n     [ https://issues.apache.org/jira/browse/PHOENIX-4521?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nChristine Feng updated PHOENIX-4521:\n------------------------------------\n    Description: \nSome clients interrupt the client thread if it doesn't complete in a required amount of time. It would be good if Pherf supported setting this up so we mimic client behavior more closely, as we're theorizing this may be causing some issues.\n\n \n\nPLAN\n # Make necessary changes so new timeoutDuration property is recognized and parsed correctly from the scenario .xml file\n # Implement a timeout based on each table's timeoutDuration\n ** Timeout each individual job? Loading schema, executing queries, etc.\n ** General timeout for all jobs?\n # Test\n\n  was:\nSome clients interrupt the client thread if it doesn't complete in a required amount of time. It would be good if Pherf supported setting this up so we mimic client behavior more closely, as we're theorizing this may be causing some issues.\n\n \n\nPLAN\n # Make necessary changes so new timeoutDuration property is recognized and parsed correctly from the scenario .xml file\n # Implement a timeout based on each table's timeoutDuration\n ** Timeout each individual job? Loading schema, executing queries, etc.\n ** General timeout for all jobs?\n\n\n> Allow Pherf scenario to define per table max allowed query duration after which thread is interrupted\n> -----------------------------------------------------------------------------------------------------\n>\n>                 Key: PHOENIX-4521\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-4521\n>             Project: Phoenix\n>          Issue Type: Improvement\n>            Reporter: James R. Taylor\n>            Assignee: Christine Feng\n>            Priority: Major\n>              Labels: phoenix-hardening\n>\n> Some clients interrupt the client thread if it doesn't complete in a required amount of time. It would be good if Pherf supported setting this up so we mimic client behavior more closely, as we're theorizing this may be causing some issues.\n>  \n> PLAN\n>  # Make necessary changes so new timeoutDuration property is recognized and parsed correctly from the scenario .xml file\n>  # Implement a timeout based on each table's timeoutDuration\n>  ** Timeout each individual job? Loading schema, executing queries, etc.\n>  ** General timeout for all jobs?\n>  # Test\n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.4#803005)\n",
  "date": {
    "$date": "2020-02-10T22:44:00Z"
  },
  "patches": [],
  "external_id": "PHOENIX-4521"
},{
  "_id": {
    "$oid": "58bfc97aaea7c7604a49ebe6"
  },
  "message_id": "<1232636503.1221500264370.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "58bfc8a9aea7c7604a49c2e7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "58bfc97aaea7c7604a49ebdb"
  },
  "from_id": {
    "$oid": "58bfc8c002ca40f8bf147880"
  },
  "to_ids": [
    {
      "$oid": "58bfc8c002ca40f8bf147881"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (ZOOKEEPER-124) StatCallback is broken in trunk",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-124?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12631076#action_12631076 ] \n\nMahadev konar commented on ZOOKEEPER-124:\n-----------------------------------------\n\nthanks stu. i apologize, i might have forgotten to un comment the lines you mentioned. ill take a look at this ... thanks again. .. \n\n> StatCallback is broken in trunk\n> -------------------------------\n>\n>                 Key: ZOOKEEPER-124\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-124\n>             Project: Zookeeper\n>          Issue Type: Bug\n>          Components: java client, server, tests\n>         Environment: Linux corner-cube 2.6.24-19-generic #1 SMP Fri Jul 11 23:41:49 UTC 2008 i686 GNU/Linux\n> java version \"1.6.0_06\"\n> Java(TM) SE Runtime Environment (build 1.6.0_06-b02)\n> Java HotSpot(TM) Client VM (build 10.0-b22, mixed mode, sharing)\n>            Reporter: Stu Hood\n>             Fix For: 3.0.0\n>\n>         Attachments: stat-callback-fail-test.diff, stat-callback-test.diff\n>\n>\n> StatCallback appears to be broken in trunk. I'll attach a patch for AsyncTest that triggers the behaviour.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-09-15T10:37:44Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-124"
},{
  "_id": {
    "$oid": "60fac2b4c290fbbd48278387"
  },
  "message_id": "<JIRA.12929163.1452515739000.72292.1452515739930@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac240c290fbbd4827667b"
  },
  "reference_ids": [
    {
      "$oid": "60fac2b3c290fbbd4827830a"
    },
    {
      "$oid": "60fac2b3c290fbbd4827830b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2b3c290fbbd4827830a"
  },
  "from_id": {
    "$oid": "58c11f2c02ca40f8bfb1f978"
  },
  "to_ids": [
    {
      "$oid": "58bfcbdbe4f89451f55cdffa"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (CURATOR-286) Memory leak in service discovery",
  "body": "Joe Littlejohn created CURATOR-286:\n--------------------------------------\n\n             Summary: Memory leak in service discovery\n                 Key: CURATOR-286\n                 URL: https://issues.apache.org/jira/browse/CURATOR-286\n             Project: Apache Curator\n          Issue Type: Bug\n          Components: Recipes\n    Affects Versions: 2.9.1\n            Reporter: Joe Littlejohn\n            Priority: Critical\n\n\nHi\n\nI'm seeing a memory leak in my application which makes use of service discovery.\n\nI've taken heap dumps and I see:\n\n* Hundreds of thousands of NamespaceWatcher instances. The client, actualWatcher and curatorWatcher fields are all null, so these are closed NamespaceWatchers.\n* Thousands of PathChildrenCache instances. Each one has a 'path' value that refers to one of the services I'm lookup up (using a service provider). The state fields shows that all these PathChildrenCache instances are CLOSED.\n\nIn my application I'm using a service provider to get an instance, then closing that service provider. It seems that even after doing this, there's still a reference to the NamespaceWatcher held in ZKWatchManager field childWatches (and this refers to the the PathChildrenCache.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-01-11T12:35:39Z"
  },
  "patches": [],
  "external_id": "CURATOR-286"
},{
  "_id": {
    "$oid": "5bbe0eec272f7b1f6830e231"
  },
  "message_id": "<JIRA.12920735.1449744832000.7650.1449964666618@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e12272f7b1f6830bd59"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0eec272f7b1f6830e218"
    },
    {
      "$oid": "5bbe0eec272f7b1f6830e217"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0eec272f7b1f6830e217"
  },
  "from_id": {
    "$oid": "5bacb0ea57674ee167d506c5"
  },
  "to_ids": [
    {
      "$oid": "5bbe0e2757674ee1679173c5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PARQUET-402) Apache Pig cannot store Map data\n type into Parquet format",
  "body": "\n    [ https://issues.apache.org/jira/browse/PARQUET-402?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15054721#comment-15054721 ] \n\nJulien Le Dem commented on PARQUET-402:\n---------------------------------------\n\nIn the pig UDF you could reuse information from the input schema to add more info in the output Map schema (as seen in Dmitriy's comment).\nIn any case, this is a Pig bug.\n\n> Apache Pig cannot store Map data type into Parquet format\n> ---------------------------------------------------------\n>\n>                 Key: PARQUET-402\n>                 URL: https://issues.apache.org/jira/browse/PARQUET-402\n>             Project: Parquet\n>          Issue Type: Bug\n>          Components: parquet-pig\n>    Affects Versions: 1.6.0, 1.8.1\n>            Reporter: Jerry Ylilammi\n>\n> Trying to store simple map with two entries gives me following exception:\n> {code}table_with_map_data: {my_map: map[]}\n> 2015-12-10 11:58:54,478 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n> 2015-12-10 11:58:54,498 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. Invalid map Schema, schema should contain exactly one field: my_map: map{code}\n> For example taking any input and doing this gives me the exception:\n> {code}table_with_map_data = FOREACH random_data GENERATE TOMAP('123', 'hello', '456', 'world') as (my_map);\n> DESCRIBE table_with_map_data;\n> STORE table_with_map_data INTO '...' USING ParquetStorer();{code}\n> I'm using latest version of Pig: Apache Pig version 0.15.0 (r1682971) compiled Jun 01 2015, 11:44:35\n> and Parquet: parquet-pig-bundle-1.6.0.jar\n> EDIT: I noticed Parquet 1.8.1 is out. I switched to it and were forced to update the pig script to use full path with ParquetStorer. However this gives me same error as 1.6.0.\n> {code}STORE table_with_map_data INTO '/Users/jerry/tmp/parquet/output/parquet' USING org.apache.parquet.pig.ParquetStorer();{code}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-12-12T23:57:46Z"
  },
  "patches": [],
  "external_id": "PARQUET-402"
},{
  "_id": {
    "$oid": "5bc8700b57a11257de56ccb8"
  },
  "message_id": "<1984633082.3987.1329913193345.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc8700857a11257de56ccab"
  },
  "from_id": {
    "$oid": "5bc86fbd57674ee167d77565"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (PDFBOX-1229) PDFBox: PDFReader invokes heap memory\n exception while loading big documents",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAndreas Lehmkühler closed PDFBOX-1229.\n--------------------------------------\n\n    Resolution: Won't Fix\n      Assignee: Andreas Lehmkühler\n\nJIRA is a bugtracker and not a Q&A tool. \n\nPlease follow up with your question on the mailing list [1]\n\n[1] http://pdfbox.apache.org/mail-lists.html\n                \n> PDFBox: PDFReader invokes heap memory exception while loading big documents\n> ---------------------------------------------------------------------------\n>\n>                 Key: PDFBOX-1229\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1229\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Parsing\n>         Environment: Using pdfbox1.6.0 on Windows 2003 server with java heap memory to 600 (which is maximum allowed), also tested with development environment windows Windows XP.\n>            Reporter: Sanket Sangodkar\n>            Assignee: Andreas Lehmkühler\n>\n> I am trying to load load bigger document using PDFReader component of PDFBox with version 1.6.0, if I have not set any maximum heap memory then load method invokes heap memory exception. On setting java max heap to 800 resolve the issue.\n> But if I use the same PDFReader component of PDFBox with version 1.4.0 to load the same document with default java min and max heap memory it loads the pdf properly.\n> Is it possible to to investigate why memory is leaked with pdfbox component version higher than 1.4.\n> Thanks\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n       \n",
  "date": {
    "$date": "2012-02-22T12:19:53Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1229"
},{
  "_id": {
    "$oid": "5bbf0ba4b79d666cbb227a1d"
  },
  "message_id": "<JIRA.12951172.1458218175000.48285.1458221073443@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0ba4b79d666cbb227a14"
    },
    {
      "$oid": "5bbf0ba4b79d666cbb227a15"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0ba4b79d666cbb227a14"
  },
  "from_id": {
    "$oid": "59677d48aff2204b3cbd1732"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (KAFKA-3414) Return of\n MetadataCache.getAliveBrokers should not be mutated by cache updates",
  "body": "\n     [ https://issues.apache.org/jira/browse/KAFKA-3414?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nIsmael Juma updated KAFKA-3414:\n-------------------------------\n    Status: Patch Available  (was: Open)\n\n> Return of MetadataCache.getAliveBrokers should not be mutated by cache updates\n> ------------------------------------------------------------------------------\n>\n>                 Key: KAFKA-3414\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-3414\n>             Project: Kafka\n>          Issue Type: Bug\n>            Reporter: Ismael Juma\n>            Assignee: Ismael Juma\n>            Priority: Critical\n>             Fix For: 0.10.0.0\n>\n>\n> This is a regression from the recent work on optimising `MetadataCache`.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-03-17T13:24:33Z"
  },
  "patches": [],
  "external_id": "KAFKA-3414"
},{
  "_id": {
    "$oid": "5bacc5f256f6a00b0209963f"
  },
  "message_id": "<1041126177.1087.1311435429610.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacc5dc56f6a00b02099057"
  },
  "from_id": {
    "$oid": "5bacc4b357674ee167dd46f5"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (MAHOUT-672) Implementation of Conjugate\n Gradient for solving large linear systems",
  "body": "\n    [ https://issues.apache.org/jira/browse/MAHOUT-672?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13069984#comment-13069984 ] \n\nSean Owen commented on MAHOUT-672:\n----------------------------------\n\nThis is a big patch and I'm not qualified to review it. But I noticed a few small issues. Look at DistributedRowMatrix for instance -- these changes should not be applied. There are also some spurious whitespace and import changes.\n\nIt's also a pretty big patch. Are there more opportunities for reuse? thinking of AbstractLinearOperator for instance.\n\n> Implementation of Conjugate Gradient for solving large linear systems\n> ---------------------------------------------------------------------\n>\n>                 Key: MAHOUT-672\n>                 URL: https://issues.apache.org/jira/browse/MAHOUT-672\n>             Project: Mahout\n>          Issue Type: New Feature\n>          Components: Math\n>    Affects Versions: 0.5\n>            Reporter: Jonathan Traupman\n>            Priority: Minor\n>             Fix For: 0.6\n>\n>         Attachments: 0001-MAHOUT-672-LSMR-iterative-linear-solver.patch, 0001-MAHOUT-672-LSMR-iterative-linear-solver.patch, MAHOUT-672.patch, MAHOUT-672.patch\n>\n>   Original Estimate: 48h\n>  Remaining Estimate: 48h\n>\n> This patch contains an implementation of conjugate gradient, an iterative algorithm for solving large linear systems. In particular, it is well suited for large sparse systems where a traditional QR or Cholesky decomposition is infeasible. Conjugate gradient only works for matrices that are square, symmetric, and positive definite (basically the same types where Cholesky decomposition is applicable). Systems like these commonly occur in statistics and machine learning problems (e.g. regression). \n> Both a standard (in memory) solver and a distributed hadoop-based solver (basically the standard solver run using a DistributedRowMatrix a la DistributedLanczosSolver) are included.\n> There is already a version of this algorithm in taste package, but it doesn't operate on standard mahout matrix/vector objects, nor does it implement a distributed version. I believe this implementation will be more generically useful to the community than the specialized one in taste.\n> This implementation solves the following types of systems:\n> Ax = b, where A is square, symmetric, and positive definite\n> A'Ax = b where A is arbitrary but A'A is positive definite. Directly solving this system is more efficient than computing A'A explicitly then solving.\n> (A + lambda * I)x = b and (A'A + lambda * I)x = b, for systems where A or A'A is singular and/or not full rank. This occurs commonly if A is large and sparse. Solving a system of this form is used, for example, in ridge regression.\n> In addition to the normal conjugate gradient solver, this implementation also handles preconditioning, and has a sample Jacobi preconditioner included as an example. More work will be needed to build more advanced preconditioners if desired.\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-07-23T15:37:09Z"
  },
  "patches": [],
  "external_id": "MAHOUT-672"
},{
  "_id": {
    "$oid": "5bbf0c72b79d666cbb229a51"
  },
  "message_id": "<JIRA.12912971.1447448868000.66412.1447448890944@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0c40b79d666cbb2292b5"
    },
    {
      "$oid": "5bbf0c40b79d666cbb2292b4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0c40b79d666cbb2292b4"
  },
  "from_id": {
    "$oid": "5bbf077557674ee16730da57"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (KAFKA-2835) FAILING TEST: LogCleaner",
  "body": "Gwen Shapira created KAFKA-2835:\n-----------------------------------\n\n             Summary: FAILING TEST: LogCleaner\n                 Key: KAFKA-2835\n                 URL: https://issues.apache.org/jira/browse/KAFKA-2835\n             Project: Kafka\n          Issue Type: Bug\n            Reporter: Gwen Shapira\n\n\nkafka.log.LogCleanerIntegrationTest > cleanerTest[2] FAILED\n    java.lang.AssertionError: log cleaner should have processed up to offset 599\n        at org.junit.Assert.fail(Assert.java:88)\n        at org.junit.Assert.assertTrue(Assert.java:41)\n        at kafka.log.LogCleanerIntegrationTest.cleanerTest(LogCleanerIntegrationTest.scala:76)\n\nhttps://builds.apache.org/job/kafka-trunk-jdk7/817/console\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-11-13T21:08:10Z"
  },
  "patches": [],
  "external_id": "KAFKA-2835"
},{
  "_id": {
    "$oid": "5bbdf59ce8113566f664b485"
  },
  "message_id": "<JIRA.12627218.1357926057015.103080.1378758895204@arcas>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [
    {
      "$oid": "5bbdf375e8113566f6649722"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdf375e8113566f6649722"
  },
  "from_id": {
    "$oid": "59bfa6f8f2a4565fe9039953"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd1e4f89451f55cdfd2"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (NUTCH-1517) CloudSearch indexer",
  "body": "\n    [ https://issues.apache.org/jira/browse/NUTCH-1517?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13762251#comment-13762251 ] \n\nTom Hill commented on NUTCH-1517:\n---------------------------------\n\nIt seems to print that message for me, even when it works. I may have not done something correctly. \n\nPlease check your logs in the logs directory, and see what it says. Or check your cloudsearch domain, and see if the documents made it there.\n                \n> CloudSearch indexer\n> -------------------\n>\n>                 Key: NUTCH-1517\n>                 URL: https://issues.apache.org/jira/browse/NUTCH-1517\n>             Project: Nutch\n>          Issue Type: New Feature\n>          Components: indexer\n>            Reporter: Julien Nioche\n>             Fix For: 1.9\n>\n>         Attachments: 0023883254_1377197869_indexer-cloudsearch.patch\n>\n>\n> Once we have made the indexers pluggable, we should add a plugin for Amazon CloudSearch. See http://aws.amazon.com/cloudsearch/. Apparently it uses a JSON based representation Search Data Format (SDF), which we could reuse for a file based indexer.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-09-09T20:34:55Z"
  },
  "patches": [],
  "external_id": "NUTCH-1517"
},{
  "_id": {
    "$oid": "60fd85239445ff90d5e5a76f"
  },
  "message_id": "<JIRA.12936156.1454452912000.282054.1454453260166@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fd84bf9445ff90d5e5927e"
  },
  "reference_ids": [
    {
      "$oid": "60fd85239445ff90d5e5a76a"
    },
    {
      "$oid": "60fd85239445ff90d5e5a76b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fd85239445ff90d5e5a76a"
  },
  "from_id": {
    "$oid": "58c9df0a02ca40f8bf20c25d"
  },
  "to_ids": [
    {
      "$oid": "5f27d08eaf02e2d6de9300d7"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (OWB-1114) Please delete old releases from\n mirroring system",
  "body": "\n     [ https://issues.apache.org/jira/browse/OWB-1114?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMark Struberg resolved OWB-1114.\n--------------------------------\n    Resolution: Fixed\n      Assignee: Mark Struberg\n\n> Please delete old releases from mirroring system\n> ------------------------------------------------\n>\n>                 Key: OWB-1114\n>                 URL: https://issues.apache.org/jira/browse/OWB-1114\n>             Project: OpenWebBeans\n>          Issue Type: Bug\n>    Affects Versions: 1.2.1\n>            Reporter: Sebb\n>            Assignee: Mark Struberg\n>\n> To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\n> Please can you remove all non-current releases?\n> i.e. the ones listed as affected.\n> Thanks!\n> Also, if you have a release guide, perhaps you could add a cleanup stage to it?\n> [1] http://www.apache.org/dev/release.html#when-to-archive\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-02-02T22:47:40Z"
  },
  "patches": [],
  "external_id": "OWB-1114"
},{
  "_id": {
    "$oid": "60fa84027bda55dc18d500ca"
  },
  "message_id": "<JIRA.13191479.1539572427000.166491.1540189260041@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fa83a87bda55dc18d4f888"
  },
  "reference_ids": [
    {
      "$oid": "60fa84027bda55dc18d500b7"
    },
    {
      "$oid": "60fa84027bda55dc18d500b6"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fa84027bda55dc18d500b6"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58c11cfae4f89451f51d7cf3"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (OPENJPA-2752) More libraries can be updated",
  "body": "\n    [ https://issues.apache.org/jira/browse/OPENJPA-2752?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16658625#comment-16658625 ] \n\nASF GitHub Bot commented on OPENJPA-2752:\n-----------------------------------------\n\nilgrosso commented on issue #23: [OPENJPA-2752] libraries are updated to most recent versions\nURL: https://github.com/apache/openjpa/pull/23#issuecomment-431749046\n \n \n   @solomax I confirm I can see either [your commit](https://github.com/apache/openjpa/commit/9a5096308c29e278cd59253922110bdf3464951f) and the [PR merge](https://github.com/apache/openjpa/commit/b3edd42ee81194c419a395f0f6ea22d48be66105) on GitHub.\n   \n   Moreover, I can see both as well at [ASF GIT](https://gitbox.apache.org/repos/asf?p=openjpa.git;a=shortlog;h=refs/heads/master).\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n\n\n> More libraries can be updated\n> -----------------------------\n>\n>                 Key: OPENJPA-2752\n>                 URL: https://issues.apache.org/jira/browse/OPENJPA-2752\n>             Project: OpenJPA\n>          Issue Type: Improvement\n>          Components: build / infrastructure\n>    Affects Versions: 3.0.0\n>            Reporter: Maxim Solodovnik\n>            Priority: Major\n>             Fix For: 3.0.1\n>\n>\n> * mysql-connector-java\n> * postgresql\n> * derby\n> * slf4j\n> * httpunit\n> * commons-jci-rhino\n> * xbean-asm6-shaded\n> * log4j\n> * simple-jndi\n> * docbook-xml\n> * jmock\n> * findbugs-annotations\n> * maven-javadoc-plugin\n> * maven-surefire-plugin\n> * maven-compiler-plugin\n> * maven-assembly-plugin\n> * findbugs-maven-plugin\n> * apache-rat-plugin\n> * maven-dependency-plugin\n> * ant-jsch\n> * maven-shade-plugin\n> * maven-checkstyle-plugin\n> * maven-jar-plugin\n> * maven-bundle-plugin\n> * karaf-maven-plugin\n> * maven-project-info-reports-plugin\n> * Various maven warnings\n> This seems to be tricky:\n> * antlr\n> This must be untouched\n> * jaxb\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-10-22T06:21:00Z"
  },
  "patches": [],
  "external_id": "OPENJPA-2752"
},{
  "_id": {
    "$oid": "5c58027d4a42fc39432f5bef"
  },
  "message_id": "<JIRA.13006374.1474432699000.25037.1476850138332@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c5801454a42fc39432f4d53"
  },
  "reference_ids": [
    {
      "$oid": "5c58027d4a42fc39432f5bed"
    },
    {
      "$oid": "5c58027d4a42fc39432f5bee"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c58027d4a42fc39432f5bed"
  },
  "from_id": {
    "$oid": "5c58018d621a9a77b3bd9e2a"
  },
  "to_ids": [
    {
      "$oid": "5bbf12e657674ee16747f845"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (EAGLE-558) Use dynamical loaded jarPath as default\n value in ApplicationEntity",
  "body": "\n     [ https://issues.apache.org/jira/browse/EAGLE-558?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nwujinhu closed EAGLE-558.\n-------------------------\n    Resolution: Resolved\n\n> Use dynamical loaded jarPath as default value in ApplicationEntity\n> ------------------------------------------------------------------\n>\n>                 Key: EAGLE-558\n>                 URL: https://issues.apache.org/jira/browse/EAGLE-558\n>             Project: Eagle\n>          Issue Type: Improvement\n>    Affects Versions: v0.5.0\n>            Reporter: Hao Chen\n>            Assignee: wujinhu\n>             Fix For: v0.5.0\n>\n>\n> Use dynamical loaded jarPath as default value in ApplicationEntity, which could also be forcefully replaced with InstallOperation\n> It's an improvement from patch: https://github.com/apache/incubator-eagle/commit/6fc103338fa3e3275a1357144dfab2feb1a103ec#diff-c9c5f683c463429e5e1c01b8b6fa71b7\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-10-19T04:08:58Z"
  },
  "patches": [],
  "external_id": "EAGLE-558"
},{
  "_id": {
    "$oid": "5bf68a3ff36e975afa4e4920"
  },
  "message_id": "<JIRA.12652539.1371085446894.107268.1371085820325@arcas>",
  "mailing_list_id": {
    "$oid": "5bf68976f36e975afa4e36af"
  },
  "reference_ids": [
    {
      "$oid": "5bf68a3ef36e975afa4e491d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bf68a3ef36e975afa4e491d"
  },
  "from_id": {
    "$oid": "58bfd16902ca40f8bf148400"
  },
  "to_ids": [
    {
      "$oid": "5bf6898e35e3ea2b7b1be76d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (GIRAPH-688) Make sure Giraph builds against all\n compatible YARN-enabled Hadoop versions, warns if none set, works w/new\n 1.1.0 line",
  "body": "\n     [ https://issues.apache.org/jira/browse/GIRAPH-688?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEli Reisman updated GIRAPH-688:\n-------------------------------\n\n    Summary: Make sure Giraph builds against all compatible YARN-enabled Hadoop versions, warns if none set, works w/new 1.1.0 line  (was: Make sure YARN builds against all compatible Giraph versions, warns if none set, works w/new 1.1.0 line)\n    \n> Make sure Giraph builds against all compatible YARN-enabled Hadoop versions, warns if none set, works w/new 1.1.0 line\n> ----------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: GIRAPH-688\n>                 URL: https://issues.apache.org/jira/browse/GIRAPH-688\n>             Project: Giraph\n>          Issue Type: Bug\n>            Reporter: Eli Reisman\n>            Assignee: Eli Reisman\n>            Priority: Minor\n>         Attachments: GIRAPH-688-1.patch\n>\n>\n> This makes the hadoop-yarn branch build again against all compatible Hadoop versions, warns (in a crude but accurate way) what to do if user did not set hadoop.version at the mvn command line...and passes mvn clean verify etc.\n> I have removed a hardcoded version setting and replaced it with the destined-to-fail warning to allow/force folks to stay on top of which version they will build against (the 2.x Hadoop line is growing quickly!)\n> The correct way (thanks Eugene!) to build our YARN branch against any compatible Hadoop, as of now, is this:\n> mvn -Phadoop_yarn -Dhadoop.version=2.0.3-alpha clean install\n> Where 2.0.3 can be any 2.0.x line, or Hadoop trunk if you like. Consult our POM.XML files to see the various profiles we support for newer Hadoops, and select the hadoop.version you see in your favorite to build, as shown above.\n> Thats it. Enjoy.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-06-13T01:10:20Z"
  },
  "patches": [],
  "external_id": "GIRAPH-688"
},{
  "_id": {
    "$oid": "5bea99339e73d744d411e751"
  },
  "message_id": "<JIRA.12402281.1218619818164.77539.1364578515534@arcas>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [
    {
      "$oid": "5bea98fe9e73d744d411dd9a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bea98fe9e73d744d411dd9a"
  },
  "from_id": {
    "$oid": "5bea97da35e3ea2b7b4de42a"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DERBY-3838) Convert derbynet/DerbyNetAutoStart\n to JUnit",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-3838?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13617548#comment-13617548 ] \n\nMyrna van Lunteren commented on DERBY-3838:\n-------------------------------------------\n\nI also saw a 10.9 build failure from Jenkins but I don't see how that could be caused by my check-in.\nAlso see that there are test failures in the runs at Oracle with 10.9 and jdk 8 on one platform with 10.9 after my check-in, but the same failures happened after the previous check-in, so I'm ignoring those too.\n                \n> Convert derbynet/DerbyNetAutoStart to JUnit\n> -------------------------------------------\n>\n>                 Key: DERBY-3838\n>                 URL: https://issues.apache.org/jira/browse/DERBY-3838\n>             Project: Derby\n>          Issue Type: Improvement\n>          Components: Test\n>    Affects Versions: 10.5.1.1\n>            Reporter: Erlend Birkenes\n>            Assignee: Myrna van Lunteren\n>            Priority: Minor\n>         Attachments: DERBY-3838_2.diff, DERBY-3838_3.diff, DERBY-3838_3.stat, DERBY-3838.diff, DERBY-3838.diff\n>\n>\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-03-29T17:35:15Z"
  },
  "patches": [],
  "external_id": "DERBY-3838"
},{
  "_id": {
    "$oid": "5f27ce8d442ab9b9860f3149"
  },
  "message_id": "<JIRA.12831912.1432234820000.6559.1432268297195@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ce02442ab9b9860f07b4"
    },
    {
      "$oid": "5f27ce02442ab9b9860f07b5"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ce02442ab9b9860f07b4"
  },
  "from_id": {
    "$oid": "5bacb14557674ee167d5b2e1"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (OOZIE-2244) Oozie should mask passwords in the\n logs when logging command arguments",
  "body": "\n     [ https://issues.apache.org/jira/browse/OOZIE-2244?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nShwetha G S updated OOZIE-2244:\n-------------------------------\n    Fix Version/s:     (was: 4.2)\n\n> Oozie should mask passwords in the logs when logging command arguments\n> ----------------------------------------------------------------------\n>\n>                 Key: OOZIE-2244\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-2244\n>             Project: Oozie\n>          Issue Type: Bug\n>    Affects Versions: 4.1.0, 4.0.1\n>         Environment: All\n>            Reporter: Venkat Ranganathan\n>            Assignee: Venkat Ranganathan\n>            Priority: Critical\n>             Fix For: trunk\n>\n>\n> Users have complained that oozie logging the password related argument values in the launcher log is a security hole and want it to be masked in the output.   Even password aliases in keystore are considered to be a security hole.\n> The fix is to mask any argument values if option name contains the string password (which is true for Sqoop).   We do this in multiple places, in Sqoop main, in Launcher Mapper, in JavaMain as well.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-05-22T04:18:17Z"
  },
  "patches": [],
  "external_id": "OOZIE-2244"
},{
  "_id": {
    "$oid": "5f27beb0a7dc6ca79d80f025"
  },
  "message_id": "<726541104.33449.1313161167195.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27bd66a7dc6ca79d809788"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27beb0a7dc6ca79d80f024"
  },
  "from_id": {
    "$oid": "59677c72aff2204b3cbd1640"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c04"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (HTTPCLIENT-1076) [GSoC 2011] Fluent API to\n HttpClient",
  "body": "\n     [ https://issues.apache.org/jira/browse/HTTPCLIENT-1076?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nXu Lilu updated HTTPCLIENT-1076:\n--------------------------------\n\n    Attachment:     (was: fluenthc.jar)\n\n> [GSoC 2011] Fluent API to HttpClient\n> ------------------------------------\n>\n>                 Key: HTTPCLIENT-1076\n>                 URL: https://issues.apache.org/jira/browse/HTTPCLIENT-1076\n>             Project: HttpComponents HttpClient\n>          Issue Type: New Feature\n>            Reporter: Oleg Kalnichevski\n>            Assignee: Oleg Kalnichevski\n>              Labels: gsoc2011, mentor\n>             Fix For: Future\n>\n>         Attachments: fluenthc.jar\n>\n>\n> Develop fluent API / facade to HttpClient based on code currently maintained by Apache Stanbol and Apache Sling projects. \n> For details see \n> http://markmail.org/message/mmyljtgjp3za6kyz\n> or contact Apache HttpComponents committers at dev@hc.apache.org\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@hc.apache.org\nFor additional commands, e-mail: dev-help@hc.apache.org\n\n",
  "date": {
    "$date": "2011-08-12T14:59:27Z"
  },
  "patches": [],
  "external_id": "HTTPCLIENT-1076"
},{
  "_id": {
    "$oid": "5bea9d6c9e73d744d412c116"
  },
  "message_id": "<28263636.1184101085010.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9cd79e73d744d412a5d0"
  },
  "from_id": {
    "$oid": "59bfb647f2a4565fe9259fce"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (DERBY-716) Re-enable VTIs",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-716?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12511570 ] \n\nDaniel John Debrunner commented on DERBY-716:\n---------------------------------------------\n\nI see the type system details in the functional spec, but it's lacking some details. With a multiset type\n\n - What infomation will be stored in TypeDescriptorImpl, e.g. scale, precision, type name etc.\n\n - how does code access the types  & names in the  multiset?\n\n\n> Re-enable VTIs\n> --------------\n>\n>                 Key: DERBY-716\n>                 URL: https://issues.apache.org/jira/browse/DERBY-716\n>             Project: Derby\n>          Issue Type: New Feature\n>          Components: SQL\n>            Reporter: Rick Hillegas\n>            Assignee: Rick Hillegas\n>         Attachments: derby-716-01-basic-aa.diff, functionTables.html, functionTables.html, functionTables.html\n>\n>\n> Cloudscape used to expose Virtual Table Interfaces, by which any class which implemented ResultSet could be included in a query's FROM list. Derby still exposes a number of these VTIs as diagnostic tools. However, Derby now prevents customers from declaring their own VTIs. The parser raises an error if a VTI's package isn't one of the Derby diagnostic packages.\n> This is a very powerful feature which customers can use to solve many problems. We should discuss the reasons that it was disabled and come up with a plan for putting this power back into our customers' hands.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-07-10T13:58:05Z"
  },
  "patches": [],
  "external_id": "DERBY-716"
},{
  "_id": {
    "$oid": "5c58030d4a42fc39432f62c5"
  },
  "message_id": "<JIRA.12995890.1470734260000.258792.1470734660489@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c5801454a42fc39432f4d53"
  },
  "reference_ids": [
    {
      "$oid": "5c5802364a42fc39432f5864"
    },
    {
      "$oid": "5c5802364a42fc39432f5865"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c5802364a42fc39432f5864"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "5bbf12e657674ee16747f845"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (EAGLE-436) Decouple Application Class-based\n Configuration",
  "body": "\n    [ https://issues.apache.org/jira/browse/EAGLE-436?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15413261#comment-15413261 ] \n\nASF GitHub Bot commented on EAGLE-436:\n--------------------------------------\n\nGitHub user haoch opened a pull request:\n\n    https://github.com/apache/incubator-eagle/pull/314\n\n    EAGLE-436 Decouple Application Class-based Configuration\n\n    https://issues.apache.org/jira/browse/EAGLE-436\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/haoch/incubator-eagle EAGLE-436\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/incubator-eagle/pull/314.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #314\n    \n----\ncommit 5b7cf17335e8472a317ecfb8c9de0d2d50ece196\nAuthor: Hao Chen <hao@apache.org>\nDate:   2016-08-09T09:20:25Z\n\n    EAGLE-436 Decouple Application Class-based Configuration A\n\n----\n\n\n> Decouple Application Class-based Configuration\n> ----------------------------------------------\n>\n>                 Key: EAGLE-436\n>                 URL: https://issues.apache.org/jira/browse/EAGLE-436\n>             Project: Eagle\n>          Issue Type: New Feature\n>    Affects Versions: v0.5.0\n>            Reporter: Hao Chen\n>            Assignee: Hao Chen\n>             Fix For: v0.5.0\n>\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-08-09T09:24:20Z"
  },
  "patches": [],
  "external_id": "EAGLE-436"
},{
  "_id": {
    "$oid": "5bc8563557a11257de5612c0"
  },
  "message_id": "<JIRA.13070939.1494448180000.180502.1494449164308@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc8563457a11257de5612bb"
    },
    {
      "$oid": "5bc8563457a11257de5612bc"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc8563457a11257de5612bb"
  },
  "from_id": {
    "$oid": "5bbdf51257674ee16787993a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-3789) Some text missing in rendering",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-3789?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTilman Hausherr updated PDFBOX-3789:\n------------------------------------\n    Description: The text in the table is missing, it was there in 2.0.5. I suspect it is due to the missing width (Adobe mentions it). The file is truncated but is parsed; the error happens also when saving the parsed file and rendering that one.  (was: The text in the table is missing. I suspect it is due to the missing width (Adobe mentions it). The file is truncated but is parsed; the error happens also when saving the parsed file and rendering that one.)\n\n> Some text missing in rendering\n> ------------------------------\n>\n>                 Key: PDFBOX-3789\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-3789\n>             Project: PDFBox\n>          Issue Type: Bug\n>    Affects Versions: 2.0.6\n>            Reporter: Tilman Hausherr\n>            Assignee: Tilman Hausherr\n>              Labels: regression\n>         Attachments: PDFBOX-3789-4KBI7ITHG6MSXR7DOTKZX6DQZJ5UF64V.pdf, PDFBOX-3789-4KBI7ITHG6MSXR7DOTKZX6DQZJ5UF64V_unc.pdf\n>\n>\n> The text in the table is missing, it was there in 2.0.5. I suspect it is due to the missing width (Adobe mentions it). The file is truncated but is parsed; the error happens also when saving the parsed file and rendering that one.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@pdfbox.apache.org\nFor additional commands, e-mail: dev-help@pdfbox.apache.org\n\n",
  "date": {
    "$date": "2017-05-10T20:46:04Z"
  },
  "patches": [],
  "external_id": "PDFBOX-3789"
},{
  "_id": {
    "$oid": "5bea9ca09e73d744d4129a54"
  },
  "message_id": "<898686822.1206532284980.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9c669e73d744d4128f04"
  },
  "from_id": {
    "$oid": "5bea9b5235e3ea2b7b55ddc6"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Resolved: (DERBY-3382) Replication: Slave must inform master\n if DBs are out of sync.",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-3382?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJørgen Løland resolved DERBY-3382.\n----------------------------------\n\n    Resolution: Fixed\n\nResolving as fixed - Issue fixed, and regression test committed.\n\n> Replication: Slave must inform master if DBs are out of sync.\n> -------------------------------------------------------------\n>\n>                 Key: DERBY-3382\n>                 URL: https://issues.apache.org/jira/browse/DERBY-3382\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Replication\n>    Affects Versions: 10.4.0.0\n>            Reporter: Øystein Grøvlen\n>            Assignee: Jørgen Løland\n>             Fix For: 10.4.0.0\n>\n>         Attachments: derby-3382-1a.diff, derby-3382-1a.stat, derby-3382-1b.diff, derby-3382-1b.stat, derby-3382-test-1a.diff, derby-3382-test-1a.stat, derby-3382-test-1b.diff, derby-3382-test-1b.stat\n>\n>\n> If I copy the database to the slave before booting the master, slave will be out of sync with the master since new log records are created during booting.  The slave will then stop replication, but the master will not be notified.\n> If I then try to stop or failover the master the master will hang.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-03-26T04:51:24Z"
  },
  "patches": [],
  "external_id": "DERBY-3382"
},{
  "_id": {
    "$oid": "5f27b9ff641061285052e020"
  },
  "message_id": "<JIRA.12723505.1403649977329.41328.1403649985092@arcas>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b99c641061285052c034"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b99c641061285052c034"
  },
  "from_id": {
    "$oid": "5f27b9efaf02e2d6de5729db"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PHOENIX-1049) Secondary indices can cause \"ERROR\n 203 (22005): Type mismatch\" in queries involving joins",
  "body": "Andrew Liles created PHOENIX-1049:\n-------------------------------------\n\n             Summary: Secondary indices can cause \"ERROR 203 (22005): Type mismatch\" in queries involving joins\n                 Key: PHOENIX-1049\n                 URL: https://issues.apache.org/jira/browse/PHOENIX-1049\n             Project: Phoenix\n          Issue Type: Bug\n    Affects Versions: 4.0.0\n            Reporter: Andrew Liles\n\n\nThe following SQL will execute correctly and produce an execution plan:\n====\nDROP TABLE IF EXISTS A;\nDROP TABLE IF EXISTS B;\n\nCREATE TABLE A (\n  K BIGINT NOT NULL,\n  CF2.C1 VARCHAR(100), \n  CF2.C2 VARCHAR(100), \n  CONSTRAINT FB_PK PRIMARY KEY (K)\n);\n\nCREATE TABLE B (\n  K BIGINT NOT NULL\n  CONSTRAINT FB_PK PRIMARY KEY (K)\n);\nEXPLAIN SELECT * FROM A INNER JOIN B ON A.K = B.K;\n====\n\nNow add an index & re-run the execution plan:\n====\nCREATE INDEX IDXA2 ON A (CF2.C1);\nEXPLAIN SELECT * FROM A INNER JOIN B ON A.K = B.K;\n====\nYou get the error:\n\nError: ERROR 203 (22005): Type mismatch. expected: LONG but was: VARCHAR at column: A.K (state=22005,code=203)\n\nThis seems related to PHOENIX-61.  Following James' insight in the first comment I have tried changing the BIGINT > DECIMAL.  But no difference.\n\nWhat is curious is that if you remove column A.C2 the issue goes away.\n\nI have tried a work around using hints to avoid the \"bad\" index:\nEXPLAIN SELECT /* NO_INDEX */ * FROM A INNER JOIN B ON A.K = B.K;\nbut that too doesn't avoid the fault.  (Is my use of the hint syntax correct?)\n\nI'm using Phoenix 4.0.0 on HBase 0.96.1.1+cdh5.0.1+68\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-06-24T22:46:25Z"
  },
  "patches": [],
  "external_id": "PHOENIX-1049"
},{
  "_id": {
    "$oid": "5bbdab04c764eb697f537949"
  },
  "message_id": "<351CFE3B-66C9-4483-A210-1AFF23ED81D0@ish.com.au>",
  "mailing_list_id": {
    "$oid": "5bbdaa4fc764eb697f536362"
  },
  "reference_ids": [
    {
      "$oid": "5bbdab04c764eb697f537945"
    },
    {
      "$oid": "5bbdab04c764eb697f537946"
    },
    {
      "$oid": "5bbdab04c764eb697f537944"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdab04c764eb697f537946"
  },
  "from_id": {
    "$oid": "59bfd98ef2a4565fe9348c95"
  },
  "to_ids": [
    {
      "$oid": "5bbdaa7557674ee167cfd9b3"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [jira] Commented: (CAY-400) Support for user properties of DataMap objects.",
  "body": "\nOn 14/04/2009, at 6:19 PM, Andrus Adamchik wrote:\n\n> One correction... Reading Ari's comment attached to CAY-400, I  \n> realized that I forgot that we also need to store it in XML, so the  \n> Map will be Map<String, String>\n\nAh, read this after I pressed send on my email. This leads me to think  \nI'd like to get the XML schema [1] linked to the output of Cayenne  \nModeler, but I can't find anywhere in the modeler which writes out the  \nXML. I assume it is buried into Cayenne core somewhere (although why?).\n\nI want this to go into the top of the output file:\n\n<?xml version=\"1.0\"?>\n<xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\ntargetNamespace=\"http://cayenne.apache.org/schema/3.0/modelMap.xsd\"\nxmlns=\"http://cayenne.apache.org/schema/3.0/modelMap.xsd\"\nelementFormDefault=\"qualified\">\n\n\nWe can then get the modeler to validate the XML with something like  \nthis where the SAX parser is initialised:\n\nDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\nfactory.setNamespaceAware(true);\nfactory.setValidating(true);\n\n\n\n\nAri\n\n\n[1] http://cayenne.apache.org/schema/3.0/modelMap.xsd\n\n-------------------------->\nish\nhttp://www.ish.com.au\nLevel 1, 30 Wilson Street Newtown 2042 Australia\nphone +61 2 9550 5001   fax +61 2 9550 4001\nGPG fingerprint CBFB 84B4 738D 4E87 5E5C  5EFA EF6A 7D2E 3E49 102A\n\n\n",
  "date": {
    "$date": "2009-04-14T18:31:47Z"
  },
  "patches": [],
  "external_id": "CAY-400"
},{
  "_id": {
    "$oid": "5f27bccca9368823397d9c4b"
  },
  "message_id": "<JIRA.12613643.1351254113964.260776.1360252513301@arcas>",
  "mailing_list_id": {
    "$oid": "5f27bb47a9368823397d2ab3"
  },
  "reference_ids": [
    {
      "$oid": "5f27bccca9368823397d9c46"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bccca9368823397d9c46"
  },
  "from_id": {
    "$oid": "58bfc8c302ca40f8bf14789e"
  },
  "to_ids": [
    {
      "$oid": "5bbe40b857674ee167452f7d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JENA-340) Quad rewrite of NOT EXISTS does not\n notice inner GRAPH",
  "body": "\n    [ https://issues.apache.org/jira/browse/JENA-340?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13573612#comment-13573612 ] \n\nHudson commented on JENA-340:\n-----------------------------\n\nIntegrated in Jena__Development_Test #476 (See [https://builds.apache.org/job/Jena__Development_Test/476/])\n    JENA-340\nTransformation of the NOT EXISTS expression needs the quad stack before/after visitors. (Revision 1443530)\n\n     Result = SUCCESS\nandy : \nFiles : \n* /jena/trunk/jena-arq/src/main/java/com/hp/hpl/jena/sparql/algebra/Transformer.java\n* /jena/trunk/jena-arq/src/main/java/com/hp/hpl/jena/sparql/algebra/optimize/ExprTransformApplyTransform.java\n* /jena/trunk/jena-arq/src/main/java/com/hp/hpl/jena/sparql/algebra/optimize/TransformApplyInsideExprFunctionOp.java\n* /jena/trunk/jena-arq/src/test/java/com/hp/hpl/jena/sparql/algebra/TestTransformQuads.java\n\n                \n> Quad rewrite of NOT EXISTS does not notice inner GRAPH\n> ------------------------------------------------------\n>\n>                 Key: JENA-340\n>                 URL: https://issues.apache.org/jira/browse/JENA-340\n>             Project: Apache Jena\n>          Issue Type: Bug\n>          Components: ARQ, TDB\n>    Affects Versions: ARQ 2.9.4\n>            Reporter: Andy Seaborne\n>            Assignee: Andy Seaborne\n>\n> Test case:\n> PREFIX  :   <http://example/>\n> SELECT *\n> {\n>    FILTER NOT EXISTS{\n>      GRAPH :foo { ?s ?p ?o } \n> }}\n> ==>\n> (prefix ((: <http://example/>))\n>   (filter (notexists\n>              (quadpattern (quad <urn:x-arq:DefaultGraphNode> ?s ?p ?o)))\n>     (table unit)))\n> Should be (quad :foo ?s ?p ?o)\n> Triple-based execution works; quad based execution does not.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-02-07T15:55:13Z"
  },
  "patches": [],
  "external_id": "JENA-340"
},{
  "_id": {
    "$oid": "60fac4ced907ab79037efd58"
  },
  "message_id": "<484830392.1215608851803.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac466d907ab79037edfab"
  },
  "from_id": {
    "$oid": "59bfaa3af2a4565fe90974cd"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DIRSERVER-1097) Only send net changes during\n replication",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSERVER-1097?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEmmanuel Lecharny updated DIRSERVER-1097:\n-----------------------------------------\n\n    Fix Version/s: 1.5.5\n\nCan we work on this issue as soon as bigbang is over ? \n\n> Only send net changes during replication\n> ----------------------------------------\n>\n>                 Key: DIRSERVER-1097\n>                 URL: https://issues.apache.org/jira/browse/DIRSERVER-1097\n>             Project: Directory ApacheDS\n>          Issue Type: Improvement\n>          Components: mitosis\n>    Affects Versions: 1.5.1\n>            Reporter: Martin Alderson\n>            Priority: Minor\n>             Fix For: 1.5.5\n>\n>\n> During replication we currently send all changes that have occurred since the target replica was last brought up to date.  We can optimise this by only sending the net changes.  As an example, if an entry is created, modified several times then deleted at one server, we do not need to tell other replicas anything regarding this entry.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-07-09T06:07:31Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-1097"
},{
  "_id": {
    "$oid": "5f27d159532b7277349c72b0"
  },
  "message_id": "<21208580.1173376101414.JavaMail.root@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d159532b7277349c72ac"
  },
  "from_id": {
    "$oid": "58bfc98602ca40f8bf1479bd"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Resolved: (AMQ-1197) delete message not supported for web\n console",
  "body": "\n     [ https://issues.apache.org/activemq/browse/AMQ-1197?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\njames strachan resolved AMQ-1197.\n---------------------------------\n\n    Resolution: Fixed\n\nfixed in rev 516117\n\n> delete message not supported for web console\n> --------------------------------------------\n>\n>                 Key: AMQ-1197\n>                 URL: https://issues.apache.org/activemq/browse/AMQ-1197\n>             Project: ActiveMQ\n>          Issue Type: Improvement\n>            Reporter: james strachan\n>         Assigned To: james strachan\n>             Fix For: 4.2.0\n>\n>\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-03-08T09:48:21Z"
  },
  "patches": [],
  "external_id": "AMQ-1197"
},{
  "_id": {
    "$oid": "5f27b97a641061285052b566"
  },
  "message_id": "<JIRA.12694555.1392144211000.55410.1426017398581@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b962641061285052ae1e"
    },
    {
      "$oid": "5f27b962641061285052ae1d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b962641061285052ae1d"
  },
  "from_id": {
    "$oid": "5bbf0e3f57674ee16740a8b2"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PHOENIX-39) Add sustained load tester that\n measures throughput",
  "body": "\n    [ https://issues.apache.org/jira/browse/PHOENIX-39?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14355576#comment-14355576 ] \n\nJesse Yates commented on PHOENIX-39:\n------------------------------------\n\n+1 overall; this is massive though, so I'd want another committer who knows the added code more closely to also +1, i.e. [~mujtabachohan].\n\nThere are probably some nits we can cleanup, but that will likely be an as-we-go kind of thing.\n\n> Add sustained load tester that measures throughput\n> --------------------------------------------------\n>\n>                 Key: PHOENIX-39\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-39\n>             Project: Phoenix\n>          Issue Type: Improvement\n>            Reporter: James Taylor\n>            Assignee: Cody Marcel\n>\n> We should add a YCSB-like [1] sustained load tester that measures throughput over an extended time period for a fully loaded cluster using Phoenix. Ideally, we'd want to be able to dial up/down the read/write percentages, and control the types of queries being run (scan, aggregate, joins, array usage, etc). Another interesting dimension is simultaneous users and on top of that multi-tenant views.\n> This would be a big effort, but we can stage it and increase the knobs and dials as we go.\n> [1] http://hbase.apache.org/book/apd.html\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-03-10T19:56:38Z"
  },
  "patches": [],
  "external_id": "PHOENIX-39"
},{
  "_id": {
    "$oid": "5bea98129e73d744d411b01e"
  },
  "message_id": "<JIRA.12329149.1140261496000.20292.1410210268869@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [
    {
      "$oid": "5bea98129e73d744d411b01c"
    },
    {
      "$oid": "5bea98129e73d744d411b01d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bea98129e73d744d411b01c"
  },
  "from_id": {
    "$oid": "5bea97a635e3ea2b7b4d6d98"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (DERBY-1008) Derby JARs downloadable / archiving\n footprint can be reduced dramatically using Java 5 Pack200 Compression\n capability - Deployment Enhancement Documentation",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-1008?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKim Haase closed DERBY-1008.\n----------------------------\n    Resolution: Won't Fix\n\nThis is a Java SE feature not strictly related to Derby, and no one seems to have felt strongly enough about it to push for it to be documented.\n\n> Derby JARs downloadable / archiving footprint can be reduced dramatically using Java 5 Pack200 Compression capability - Deployment Enhancement Documentation\n> ------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-1008\n>                 URL: https://issues.apache.org/jira/browse/DERBY-1008\n>             Project: Derby\n>          Issue Type: Improvement\n>          Components: Documentation\n>    Affects Versions: 10.2.1.6\n>            Reporter: Francois Orsini\n>            Priority: Minor\n>\n> PACK200 (JSR-200) is a very interesting feature that is available as part of J2SE 1.5.\n> In order to increase network server, availability and bandwidth , two new compression formats have been made available to Java deployment of applications and applets: gzip and Pack200.\n> HTTP compression is implemented in Java Web Start and Java Plug-in in compliance with HTTP 1.1 RFC 2616. If a client does not support this type of compression then a web server will send a non-compressed JAR or a format that the client can process.\n> More information about Pack200 at:\n> http://java.sun.com/j2se/1.5.0/docs/guide/deployment/deployment-guide/pack200.html\n> I have compressed the Apache Derby engine JAR file (derby.jar) using pack200 and the resulting JAR went from 2,131,777 to 630,520 bytes (~2MB down to 600K for the database core engine!). Uncompression happens on the client side automatically once the pack200 compressed JAR has been fetched and the client is capable of handling pack200 archives.\n> To compress/pack the JAR archive:\n> $ pack200 derby.jar.pack.gz derby.jar\n> It then can be stored on a HTTP 1.1 compliant web server and if a web client support pack200-gzip encoding (specified as part of the HTTP request), then the server will send that compressed JAR down - otherwise the server will stream/send a format that the client can process (compressed {gzip} or not {.jar}).\n> To unpack (manually):\n> $ unpack200 derby.jar.pack.gz derby.jar (the unpack operation is actually pretty fast)\n> This is quite good when one needs to deploy Apache Derby over a network (i.e. Applet using Derby with the Java plug-in or plain HTTP download, etc) or as part of some offline installer who would want to pack JARs then unpack them during install (unpack200{.exe} is a standalone executable that can be included as part of an installer).\n> pack200 and unpack200 are pure executable(s) that do not require a JVM to be present or run at the time an archive is compressed or uncompressed (i.e. during installation)\n> unpack200 (Unix) or unpack200.exe(Windows) included as part of the JRE as well (under bin) - at least for Sun J2SE.\n> Pack200 also has a Java API (java.util.jar.Pack200):\n> http://java.sun.com/j2se/1.5.0/docs/api/java/util/jar/Pack200.html\n> The packer engine is used by application developers to deploy or host JAR files on a website. The unpacker engine is used by deployment applications to transform the byte-stream back to JAR format.\n> This does not reduce the runtime footprint of Apache Derby - it basically provides means to reduce the downloable footprint of JARs during install. Pack200 also supports the packing of *signed* JARs.\n> I'm entering this JIRA hoping that we can document this deployment capability in the Derby Developer's Guide (for instance) under some Deployment section. I believe that Derby deployment enhancements should be worth mentioning in some developer's guide.\n> Other interesting uses would be:\n> - to pack200 a JAR containing some Derby database along with the application and else...reducing the downloadble/shippable footprint.\n> - to allow pack200 jars to be stored within Derby - could be unpacked on the fly using the java.util.jar.Pack200 API.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2014-09-08T21:04:28Z"
  },
  "patches": [],
  "external_id": "DERBY-1008"
},{
  "_id": {
    "$oid": "5f27b87a64106128505262ee"
  },
  "message_id": "<JIRA.12987352.1467851958000.49243.1467906850997@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b87a64106128505262d0"
    },
    {
      "$oid": "5f27b87a64106128505262cf"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b87a64106128505262cf"
  },
  "from_id": {
    "$oid": "59bfb3eff2a4565fe9210dbd"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PHOENIX-3056) Incorrect error message when\n deleting a record from table with async index creation in progress",
  "body": "\n     [ https://issues.apache.org/jira/browse/PHOENIX-3056?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nThomas D'Silva updated PHOENIX-3056:\n------------------------------------\n    Assignee: James Taylor  (was: Thomas D'Silva)\n\n> Incorrect error message when deleting a record from table with async index creation in progress\n> -----------------------------------------------------------------------------------------------\n>\n>                 Key: PHOENIX-3056\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-3056\n>             Project: Phoenix\n>          Issue Type: Bug\n>    Affects Versions: 4.7.0\n>            Reporter: Brian Esserlieu\n>            Assignee: James Taylor\n>            Priority: Minor\n>             Fix For: 4.8.0\n>\n>\n> Repro:\n> DROP TABLE IF EXISTS TEST_TABLE;\n> CREATE TABLE IF NOT EXISTS TEST_TABLE (\n> pk1 VARCHAR NOT NULL,\n> pk2 VARCHAR NOT NULL,\n> pk3 VARCHAR\n> CONSTRAINT PK PRIMARY KEY \n> (\n> pk1,\n> pk2,\n> pk3\n> )\n> ) MULTI_TENANT=true,IMMUTABLE_ROWS=true;\n> CREATE INDEX TEST_INDEX ON TEST_TABLE (pk3, pk2) ASYNC;\n> upsert into TEST_TABLE (pk1, pk2, pk3) values ('a', '1', 'value1');\n> upsert into TEST_TABLE (pk1, pk2, pk3) values ('a', '2', 'value2');\n> select * from test_table;\n> delete from TEST_TABLE where pk1 = 'a';\n> When I run the above I get the following error on the delete statement:\n> \"Error: ERROR 1027 (42Y86): All columns referenced in a WHERE clause must be available in every index for a table with immutable rows. tableName=TEST_TABLE\n> SQLState:  42Y86\n> ErrorCode: 1027\n> Error occurred in:\n> delete from TEST_TABLE where pk1 = 'a'\"\n> Notice the SQL works simply by removing the ASYNC keyword from the index creation statement.\n> The error message should reflect that the index is being created so deletes are blocked until that completes.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-07-07T15:54:11Z"
  },
  "patches": [],
  "external_id": "PHOENIX-3056"
},{
  "_id": {
    "$oid": "5bea9ab79e73d744d4123423"
  },
  "message_id": "<1131880.97891288179921973.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9aa09e73d744d4122f83"
  },
  "from_id": {
    "$oid": "5bea97ad35e3ea2b7b4d7d4f"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Assigned: (DERBY-4772) Data truncation error with\n XPLAIN-functionality enabled",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-4772?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKristian Waagan reassigned DERBY-4772:\n--------------------------------------\n\n    Assignee: Kristian Waagan\n\nThanks, Knut.\nThat sounds reasonable, and another point supporting that approach is my assumption that not a lot of people have started using the feature yet.\n\nI need XPLAIN to work to be able to write a test for a different issue, so I'll go ahead and write a patch.\nMy first step was to clean up some code in XPLAINSystemTableVisitor (unused imports, unused variables, removed printStackTrace). Committed to trunk with revision 1027921.\n\n> Data truncation error with XPLAIN-functionality enabled\n> -------------------------------------------------------\n>\n>                 Key: DERBY-4772\n>                 URL: https://issues.apache.org/jira/browse/DERBY-4772\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: SQL\n>    Affects Versions: 10.6.2.1, 10.7.1.0\n>            Reporter: Kristian Waagan\n>            Assignee: Kristian Waagan\n>         Attachments: derby-4771-1a-prototype_code_dump.stat\n>\n>\n> When running a modified version of lang.OrderByAndSortAvoidance I get the following error:\n> java.sql.SQLDataException: A truncation error was encountered trying to shrink CHAR 'Thread[DRDAConnThread_3,5,derby.daemons]' to length 32.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:79)\n> \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Util.java:256)\n> \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(TransactionResourceImpl.java:391)\n> \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(TransactionResourceImpl.java:346)\n> \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(EmbedConnection.java:2269)\n> \tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(ConnectionChild.java:81)\n> \tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1321)\n> \tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1673)\n> \tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(EmbedPreparedStatement.java:303)\n> \tat org.apache.derby.impl.sql.execute.xplain.XPLAINSystemTableVisitor.addStmtDescriptorsToSystemCatalog(XPLAINSystemTableVisitor.java:390)\n> \tat org.apache.derby.impl.sql.execute.xplain.XPLAINSystemTableVisitor.doXPLAIN(XPLAINSystemTableVisitor.java:317)\n> \tat org.apache.derby.impl.sql.execute.NoPutResultSetImpl.close(NoPutResultSetImpl.java:179)\n> \tat org.apache.derby.impl.sql.execute.SortResultSet.close(SortResultSet.java:467)\n> \tat org.apache.derby.impl.jdbc.EmbedResultSet.close(EmbedResultSet.java:575)\n> \tat org.apache.derby.impl.drda.DRDAResultSet.close(DRDAResultSet.java:338)\n> \tat org.apache.derby.impl.drda.DRDAStatement.rsClose(DRDAStatement.java:995)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.doneData(DRDAConnThread.java:7446)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.writeFDODTA(DRDAConnThread.java:7026)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.writeQRYDTA(DRDAConnThread.java:6910)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.processCommands(DRDAConnThread.java:870)\n> \tat org.apache.derby.impl.drda.DRDAConnThread.run(DRDAConnThread.java:294)\n> Caused by: java.sql.SQLException: A truncation error was encountered trying to shrink CHAR 'Thread[DRDAConnThread_3,5,derby.daemons]' to length 32.\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(SQLExceptionFactory.java:45)\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(SQLExceptionFactory40.java:119)\n> \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:70)\n> \t... 20 more\n> Caused by: ERROR 22001: A truncation error was encountered trying to shrink CHAR 'Thread[DRDAConnThread_3,5,derby.daemons]' to length 32.\n> \tat org.apache.derby.iapi.error.StandardException.newException(StandardException.java:343)\n> \tat org.apache.derby.iapi.types.SQLChar.hasNonBlankChars(SQLChar.java:1767)\n> \tat org.apache.derby.iapi.types.SQLChar.normalize(SQLChar.java:1743)\n> \tat org.apache.derby.iapi.types.SQLChar.normalize(SQLChar.java:1695)\n> \tat org.apache.derby.iapi.types.DataTypeDescriptor.normalize(DataTypeDescriptor.java:648)\n> \tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeColumn(NormalizeResultSet.java:329)\n> \tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(NormalizeResultSet.java:373)\n> \tat org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(NormalizeResultSet.java:188)\n> \tat org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(DMLWriteResultSet.java:127)\n> \tat org.apache.derby.impl.sql.execute.InsertResultSet.open(InsertResultSet.java:504)\n> \tat org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:436)\n> \tat org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:317)\n> \tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1232)\n> \t... 14 more\n> I suspect the error can be triggered easily in client/server, but for convenience I'll attach the patch for the test where I see the issue.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-10-27T07:45:21Z"
  },
  "patches": [],
  "external_id": "DERBY-4772"
},{
  "_id": {
    "$oid": "5f27d0f7532b7277349c5982"
  },
  "message_id": "<740143076.1220865832604.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d0e7532b7277349c5526"
  },
  "from_id": {
    "$oid": "5f27c25faf02e2d6de6f4c0a"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Assigned: (AMQ-1489) discoveryUri in transportConnector and\n uri in networkConnector partially ignored if multicast",
  "body": "\n     [ https://issues.apache.org/activemq/browse/AMQ-1489?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nGary Tully reassigned AMQ-1489:\n-------------------------------\n\n    Assignee: Gary Tully  (was: Rob Davies)\n\n> discoveryUri in transportConnector and uri in networkConnector partially ignored if multicast\n> ---------------------------------------------------------------------------------------------\n>\n>                 Key: AMQ-1489\n>                 URL: https://issues.apache.org/activemq/browse/AMQ-1489\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: Connector\n>    Affects Versions: 4.1.1, 5.0.0\n>            Reporter: Gerald Loeffler\n>            Assignee: Gary Tully\n>             Fix For: 5.2.0\n>\n>         Attachments: patchfile.txt\n>\n>\n> This bug relates to the MulticastDiscoveryAgent and was discovered when \n> 1. the discoveryUri of a transportConnector is a multicast-uri such as\n> {code:xml}\n> <transportConnector uri=\"...\" discoveryUri=\"multicast://239.3.7.0:37000\" />\n> {code}\n> or\n> 2. the uri of a networkConnector is a multicast-uri such as\n> {code:xml}\n> <networkConnector name=\"...\" uri=\"multicast://239.3.7.0:37000\">\n> {code}\n> In these cases, the uri is partially ignored by the MulticastDiscoveryAgent: the host-name of the uri (239.3.7.0) is extracted and used as the group name by the MulticastDiscoveryAgent. But the actual multicast group IP is always 239.255.2.3 and the multicast port is always 6155, regardless of what the actual uri is in the configuration.\n> The reason for this is that MulticastDiscoveryAgentFactory creates a new MulticastDiscoveryAgent and sets the group based on the uri's host, but fails to set the discoveryURI itself. MulticastDiscoveryAgentFactory should be corrected to do this:\n> {code:java}\n> MulticastDiscoveryAgent rc = new MulticastDiscoveryAgent();\n> rc.setDiscoveryURI(uri); // missing\n> rc.setGroup(uri.getHost());\n> {code}\n> Without the above setting of the discoveryURI the MulticastDiscoveryAgent always uses the DEFAULT_DISCOVERY_URI_STRING which is \"multicast://239.255.2.3:6155\", explaining the erroneous behaviour described above.\n>   kind regards,\n>   gerald\n> http://www.gerald-loeffler.net\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-09-08T02:23:52Z"
  },
  "patches": [],
  "external_id": "AMQ-1489"
},{
  "_id": {
    "$oid": "5bea9c4a9e73d744d412897a"
  },
  "message_id": "<185233736.1213131825109.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9b009e73d744d4124300"
  },
  "from_id": {
    "$oid": "5bea981135e3ea2b7b4e616e"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-3678) StackOverflowException in deadlock\n trace",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-3678?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDag H. Wanvik updated DERBY-3678:\n---------------------------------\n\n    Attachment: derby-3678-2.stat\n                derby-3678-2.diff\n\nThanks for looking at this, Knut Anders. Fixed the javadoc and the\naccidental whitespace thing.  As for your second, comment, I agree it\nis probably better to use the idiom for reading the index row under\nread uncommitted mode, so I did that.\n\nAdditionally, I modified the existing sanity code which asserts that\nthe base row is always there to just run when repeatable read is used,\nand added an extra check for this case for read uncommitted mode in\nwhich case I just throw an error (RAWSTORE_RECORD_VANISHED).  This\nshould be very unlikely, but if it happens we have seen that TimeOut\nswallows exceptions when building TableNameInfo and prints the lock\ntable even without table information, so I think it is ok. We could\npossibly catch this exception in hashAllTableDescriptorsByTableId and\njust skip that table (which has gone anyway since its schema has\ngone...:), but I didn't.\n\nI also added javadoc to say that only repeatable read (normal case)\nand read uncommitted are supported (in the new signatures).\n\nRe-running tests.\n\n> StackOverflowException in deadlock trace\n> ----------------------------------------\n>\n>                 Key: DERBY-3678\n>                 URL: https://issues.apache.org/jira/browse/DERBY-3678\n>             Project: Derby\n>          Issue Type: Bug\n>    Affects Versions: 10.3.2.1\n>         Environment: (this is actually in version 10.3.2.2)\n> MacOS 10, JDK 1.6\n>            Reporter: geoff hendrey\n>            Assignee: Dag H. Wanvik\n>             Fix For: 10.5.0.0\n>\n>         Attachments: derby-3678-1.diff, derby-3678-1.stat, derby-3678-2.diff, derby-3678-2.stat\n>\n>\n> I am getting a deadlock in SYSTABLE. When I turn on Dderby.locks.deadlockTrace=true, I get a StackOverflowException\n> Derby version The Apache Software Foundation - Apache Derby - 10.3.2.2\n> - (618335): instance 80220011-0119-f93f-b912-00000000bced\n> on database directory /db/domains/geoff  \n> Database Class Loader started - derby.database.classpath=''\n> 2008-05-17 23:44:36.380\n> GMT Thread[btpool0-2,5,main] (XID = 7556), (SESSIONID = 4), (DATABASE =\n> domains/geoff), (DRDAID = null), Cleanup action starting\n> java.lang.StackOverflowError\n>         at org.apache.derby.impl.sql.execute.GenericExecutionFactory.getValueRow(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.SYSCONGLOMERATESRowFactory.makeRow(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.SYSCONGLOMERATESRowFactory.makeEmptyRow(Unknown\n> Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.hashAllConglomerateDescriptorsByNumber(Unknown\n> Source)\n>         at org.apache.derby.impl.services.locks.TableNameInfo.<init>(Unknown Source)\n>         at org.apache.derby.impl.services.locks.Timeout.buildLockTableString(Unknown Source)\n>         at org.apache.derby.impl.services.locks.Timeout.createException(Unknown Source)\n>         at org.apache.derby.impl.services.locks.Timeout.buildException(Unknown Source)\n>         at org.apache.derby.impl.services.locks.ConcurrentLockSet.lockObject(Unknown Source)\n>         at org.apache.derby.impl.services.locks.AbstractPool.lockObject(Unknown Source)\n>         at org.apache.derby.impl.services.locks.ConcurrentPool.lockObject(Unknown Source)\n>         at org.apache.derby.impl.store.raw.xact.RowLocking3.lockRecordForRead(Unknown Source)\n>         at org.apache.derby.impl.store.access.heap.HeapController.lockRow(Unknown Source)\n>         at org.apache.derby.impl.store.access.heap.HeapController.lockRow(Unknown Source)\n>         at org.apache.derby.impl.store.access.btree.index.B2IRowLocking3.lockRowOnPage(Unknown\n> Source)\n>         at org.apache.derby.impl.store.access.btree.index.B2IRowLocking3._lockScanRow(Unknown\n> Source)\n>         at org.apache.derby.impl.store.access.btree.index.B2IRowLockingRR.lockScanRow(Unknown\n> Source)\n>         at org.apache.derby.impl.store.access.btree.BTreeForwardScan.fetchRows(Unknown Source)\n>         at org.apache.derby.impl.store.access.btree.BTreeScan.next(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getDescriptorViaIndex(Unknown\n> Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.locateSchemaRow(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getSchemaDescriptor(Unknown\n> Source)\n>         at org.apache.derby.impl.sql.catalog.SYSTABLESRowFactory.buildDescriptor(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.hashAllTableDescriptorsByTableId(Unknown\n> Source)\n>         at org.apache.derby.impl.services.locks.TableNameInfo.<init>(Unknown Source)\n>         at org.apache.derby.impl.services.locks.Timeout.buildLockTableString(Unknown Source)\n>         at org.apache.derby.impl.services.locks.Timeout.createException(Unknown Source)\n>         at org.apache.derby.impl.services.locks.Timeout.buildException(Unknown Source)\n>         at org.apache.derby.impl.services.locks.ConcurrentLockSet.lockObject(Unknown Source)\n>         at org.apache.derby.impl.services.locks.AbstractPool.lockObject(Unknown Source)\n>         at org.apache.derby.impl.services.locks.ConcurrentPool.lockObject(Unknown Source)\n>         at org.apache.derby.impl.store.raw.xact.RowLocking3.lockRecordForRead(Unknown Source)\n>         at org.apache.derby.impl.store.access.heap.HeapController.lockRow(Unknown Source)\n>         at org.apache.derby.impl.store.access.heap.HeapController.lockRow(Unknown Source)\n>         at org.apache.derby.impl.store.access.btree.index.B2IRowLocking3.lockRowOnPage(Unknown\n> Source)\n>         at org.apache.derby.impl.store.access.btree.index.B2IRowLocking3._lockScanRow(Unknown\n> Source)\n>         at org.apache.derby.impl.store.access.btree.index.B2IRowLockingRR.lockScanRow(Unknown\n> Source)\n>         at org.apache.derby.impl.store.access.btree.BTreeForwardScan.fetchRows(Unknown Source)\n>         at org.apache.derby.impl.store.access.btree.BTreeScan.next(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getDescriptorViaIndex(Unknown\n> Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.locateSchemaRow(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getSchemaDescriptor(Unknown\n> Source)\n>         at org.apache.derby.impl.sql.catalog.SYSTABLESRowFactory.buildDescriptor(Unknown Source)\n>         at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.hashAllTableDescriptorsByTableId(Unknown\n> Source)\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-06-10T14:03:45Z"
  },
  "patches": [],
  "external_id": "DERBY-3678"
},{
  "_id": {
    "$oid": "5bc86fb657a11257de56ca31"
  },
  "message_id": "<282895092.3311.1337609321117.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc86fb657a11257de56ca2d"
  },
  "from_id": {
    "$oid": "5967685caff2204b3cbaa3a0"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (PDFBOX-1299) BaseParser.readUntilEndOfStream can\n stop too early, causing IOException on valid PDFs",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1299?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTimo Boehme closed PDFBOX-1299.\n-------------------------------\n\n       Resolution: Fixed\n    Fix Version/s: 1.7.0\n\nWhile the new NonSequentialPDFParser uses length information in every case the sequential working PDFParser did not since in most cases length information is an indirect object with its value provided later. However there are some cases where this information is present at point of parsing the stream (like in the example) and for these cases I applied your patch with small modification. I only kept length information if defined directly because in case of indirect object we do not know if this will be revised later on (since PDFParser hasn't read xref table at this time) and thus we would read wrong number of bytes. However this limitation shouldn't be a real one since indirect defined length comes typically after stream object.\n\nFixed in rev. 1341030; thanks for contribution\n                \n> BaseParser.readUntilEndOfStream can stop too early, causing IOException on valid PDFs\n> -------------------------------------------------------------------------------------\n>\n>                 Key: PDFBOX-1299\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1299\n>             Project: PDFBox\n>          Issue Type: Bug\n>    Affects Versions: 1.6.0\n>            Reporter: Michael McCandless\n>            Assignee: Timo Boehme\n>             Fix For: 1.7.0\n>\n>         Attachments: PDFBOX-1299.patch, Tracey_Prather_31-Dec-2010_211843_2011Portfolio.pdf\n>\n>\n> The purpose of BaseParser.readUntilEndOfStream is to scan ahead,\n> copying bytes to the output, stopping once it sees \"endstream\".\n> The problem with this approach is sometimes the stream data itself\n> contains endstream causing readUntilEndOfStream to stop too early.\n> This can legitimately happen when the stream is an embedded PDF; I'll\n> attach a test PDF showing this.\n> However, the stream dict declares the stream length (in bytes)...  so\n> it seems like we should be respecting that length (if present) and\n> simply copy over that many bytes, instead of scanning the stream bytes\n> for endstream?  This should be a lot faster too...\n> I imagine we always scan so that we are more robust if the length is\n> missing/invalid?  Is that why this method was used?  (I don't know the\n> history here...).  If so, maybe we can have an option to use\n> the declared stream length if present.\n> I have a patch to use the declared stream length (if present), and it enables\n> at least this test PDF to correctly parse.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-05-21T14:08:41Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1299"
},{
  "_id": {
    "$oid": "5f27bbb7a9368823397d454c"
  },
  "message_id": "<JIRA.12692819.1391423088000.897.1524819960273@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27bb47a9368823397d2ab3"
  },
  "reference_ids": [
    {
      "$oid": "5f27bbaea9368823397d4266"
    },
    {
      "$oid": "5f27bbaea9368823397d4267"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27bbaea9368823397d4266"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "5bbe40b857674ee167452f7d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JENA-632) Generate JSON from SPARQL directly.",
  "body": "\n    [ https://issues.apache.org/jira/browse/JENA-632?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16456139#comment-16456139 ] \n\nASF GitHub Bot commented on JENA-632:\n-------------------------------------\n\nGithub user afs commented on a diff in the pull request:\n\n    https://github.com/apache/jena/pull/114#discussion_r184631546\n  \n    --- Diff: jena-arq/src/main/java/org/apache/jena/sparql/serializer/QuerySerializer.java ---\n    @@ -142,7 +144,19 @@ public void visitAskResultForm(Query query)\n             out.print(\"ASK\") ;\n             out.newline() ;\n         }\n    -    \n    +\n    +    @Override\n    +    public void visitJsonResultForm(Query query) {\n    +        out.print(\"JSON {\");\n    +        List<String> terms = new ArrayList<>();\n    +        for (Map.Entry<String, Object> entry : query.getJsonMapping().entrySet()) {\n    --- End diff --\n    \n    Is it ready to merge?  Any area to look at specially? I've used `qparse` on JSON queries and run a few test queries with all the cases I can think of.\n    \n\n\n\n> Generate JSON from SPARQL directly.\n> -----------------------------------\n>\n>                 Key: JENA-632\n>                 URL: https://issues.apache.org/jira/browse/JENA-632\n>             Project: Apache Jena\n>          Issue Type: Improvement\n>          Components: ARQ, Fuseki\n>            Reporter: Andy Seaborne\n>            Assignee: Bruno P. Kinoshita\n>            Priority: Minor\n>              Labels: java, javacc\n>          Time Spent: 0.5h\n>  Remaining Estimate: 0h\n>\n> The capability to generate JSON directly from a SPARQL (or extended SPARQL) query would enable the creation of JSON data API over published linked data.\n> This project would cover:\n> # Design and publication of a design.\n> # Refinement of design based on community feed\n> # Implementation, including testing.\n> # Refinement of implementation based on community feed\n> Skills required: Java, some parser work, design and discussion with the user community, basic understanding of HTTP and content negotiation.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-04-27T09:06:00Z"
  },
  "patches": [],
  "external_id": "JENA-632"
},{
  "_id": {
    "$oid": "5f27ccc9014d3531c6cbeb22"
  },
  "message_id": "<JIRA.12443108.1260543832000.483632.1513614480506@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [
    {
      "$oid": "5f27ccc9014d3531c6cbeb21"
    },
    {
      "$oid": "5f27ccc9014d3531c6cbeb20"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ccc9014d3531c6cbeb20"
  },
  "from_id": {
    "$oid": "596779baaff2204b3cbd1282"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JCR-2434) Occasional IndexingQueueTest failure",
  "body": "\n    [ https://issues.apache.org/jira/browse/JCR-2434?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16295204#comment-16295204 ] \n\nJulian Reschke commented on JCR-2434:\n-------------------------------------\n\ntrunk: [r889976|http://svn.apache.org/r889976\n\n> Occasional IndexingQueueTest failure\n> ------------------------------------\n>\n>                 Key: JCR-2434\n>                 URL: https://issues.apache.org/jira/browse/JCR-2434\n>             Project: Jackrabbit Content Repository\n>          Issue Type: Test\n>          Components: jackrabbit-core\n>    Affects Versions: 2.0-beta5\n>            Reporter: Marcel Reutegger\n>            Priority: Minor\n>             Fix For: 2.0-beta6\n>\n>\n> Usually the following assertion fails:\n> junit.framework.AssertionFailedError\n> \tat junit.framework.Assert.fail(Assert.java:47)\n> \tat junit.framework.Assert.assertTrue(Assert.java:20)\n> \tat junit.framework.Assert.assertTrue(Assert.java:27)\n> \tat org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:77)\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-12-18T16:28:00Z"
  },
  "patches": [],
  "external_id": "JCR-2434"
},{
  "_id": {
    "$oid": "5bbe12fd08955e10d507de48"
  },
  "message_id": "<145941499.1205216929791.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bbe124408955e10d507d458"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbe12db08955e10d507daaa"
  },
  "from_id": {
    "$oid": "5bbe12fd57674ee1679be52e"
  },
  "to_ids": [
    {
      "$oid": "58f6329a02ca40f8bf01b128"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (WSS-86) CryptoBase.splitAndTrim does not take into\n account the format of a DN constructed by different providers",
  "body": "\n     [ https://issues.apache.org/jira/browse/WSS-86?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRobert Egglestone updated WSS-86:\n---------------------------------\n\n    Attachment: remove_extra_escaping.patch\n\nAfter applying this patch, I've noticed that some encoded values get an extra slash added in by XML-Security and so still fail to match.\n\nI've submitted a query to XML-Security regarding this, however the attached patch (remove_extra_escaping.patch) provides a work-around on the WSS4J side in the mean time.\n\n> CryptoBase.splitAndTrim does not take into account the format of a DN constructed by different providers\n> --------------------------------------------------------------------------------------------------------\n>\n>                 Key: WSS-86\n>                 URL: https://issues.apache.org/jira/browse/WSS-86\n>             Project: WSS4J\n>          Issue Type: Bug\n>            Reporter: Christof Soehngen\n>            Priority: Minor\n>         Attachments: crypto.properties, DNTestCase.java, myKeystore.jks, name_tests.patch, remove_extra_escaping.patch, use_x500princ_for_names.patch\n>\n>\n> On some systems, different security providers are used to create the x509 certificate instances for the certificate in the soap message and for the certificates from the keystore.\n> Example would be one system where SOAP certificate is loaded with SUN provider, keystore with BC provider (although I have now idea how this is possible, given the fact that BC is not able to load JKS ...). This was checked at runtime/debug.\n> Merlin uses a splitAndTrim-Method to compare DNsin order to find certificates by issuer name.\n> If two different security provider are used the same certificates, they may result in different DNs:\n> org.bouncycastle.jce.provider.X509CertificateObject:\n> C=...,ST=...,L=...,O=...,OU=...,CN=...,E=...\n> sun.security.x509.X509CertImpl:\n> EMAILADDRESS=..., CN=..., OU=..., O=..., L=..., ST=..., C=...\n> Therefore, Merlin would treat theses certificates as different, even if the ... are equal.\n> A fix for this behaviour would be a modification of the splitAndTrim Method, replacing problematic attribute names like EMAILADDRESS\n> Something like:\n>     protected Vector splitAndTrim(String inString)\n>     {\n>         X509NameTokenizer nmTokens = new X509NameTokenizer(inString);\n>         Vector vr = new Vector();\n>         while (nmTokens.hasMoreTokens())\n>         {\n>             String tokenString = nmTokens.nextToken();\n>             \n>             // Try to split name/value pairs\n>             int positionOfEquals = tokenString.indexOf(\"=\");\n>             if (positionOfEquals >= 0)\n>             {\n>                 String name = tokenString.substring(0, positionOfEquals);\n>                 String value = tokenString.substring(positionOfEquals + 1);\n>                 \n>                 // Not mandatory, but may be possible problems, too \n>                 name = name.trim();\n>                 name = name.toUpperCase();\n>                 \n>                 // Fix certain deviations from standard names\n>                 if (name.equals(\"EMAILADDRESS\"))\n>                 {\n>                     name = \"E\";\n>                 }\n>                 \n>                 StringBuffer stringBuffer = new StringBuffer();\n>                 stringBuffer.append(name);\n>                 stringBuffer.append(\"=\");\n>                 stringBuffer.append(value);\n>                 \n>                 tokenString = stringBuffer.toString();\n>             }\n>             else\n>             {\n>                 // Ignore the token, if not parseable\n>             }\n>             \n>             vr.add(tokenString);\n>         }\n>         \n>         java.util.Collections.sort(vr);\n>         \n>         return vr;\n>     }\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: wss4j-dev-unsubscribe@ws.apache.org\nFor additional commands, e-mail: wss4j-dev-help@ws.apache.org\n\n",
  "date": {
    "$date": "2008-03-10T23:28:49Z"
  },
  "patches": [],
  "external_id": "WSS-86"
},{
  "_id": {
    "$oid": "60fac561d907ab79037f2497"
  },
  "message_id": "<16185931.1164640641115.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "body": "Selection in Schema\n-------------------\n\n                 Key: DIRSTUDIO-4\n                 URL: http://issues.apache.org/jira/browse/DIRSTUDIO-4\n             Project: Directory LDAP Studio\n          Issue Type: Bug\n          Components: ldapstudio-schemas-plugin\n            Reporter: Pierre-Arnaud Marcelot\n         Assigned To: Pierre-Arnaud Marcelot\n            Priority: Minor\n\n\nWhen trying to select the schema text (in code source mode) including the text that is not visible, the text is not scrolling when reaching the bottom of the window. \n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators: http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2006-11-27T07:17:21Z"
  },
  "from_id": {
    "$oid": "5f27c218af02e2d6de6ea2f8"
  },
  "subject": "[jira] Created: (DIRSTUDIO-4) Selection in Schema",
  "external_id": "DIRSTUDIO-4"
},{
  "_id": {
    "$oid": "5bea9d2c9e73d744d412b52a"
  },
  "message_id": "<30946712.1192649275636.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9c169e73d744d4127ec9"
  },
  "from_id": {
    "$oid": "5bea97a635e3ea2b7b4d6d98"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (DERBY-1823) Derby Developer's Guide -  Issues w/\n User authentication and authorization extended examples section/paragraph",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-1823?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKim Haase updated DERBY-1823:\n-----------------------------\n\n    Attachment: DERBY-1823.zip\n                DERBY-1823.diff\n\nI'm attaching a patch (DERBY-1823.diff and DERBY-1823.zip) that contains the following changes:\n\nM      src/devguide/rdevcsecure26537.dita\nA      src/devguide/rdevcsecureclientexample.dita\nM      src/devguide/derbydev.ditamap\n\nrdevcsecure26537.dita now contains an end-to-end embedded example. I've added a file with a client example, rdevcsecureclientexample.dita. The client example had to be split into two source files run in sequence, even though you don't have to shut down and restart the Network Server in between. \n\nI left the examples nested under their respective parent files (on client/server and embedded examples) in the TOC.\n\nPlease let me know if further changes are needed. I'm still waiting for approval of the fix to one of the parent files (DERBY-3109).\n\n\n> Derby Developer's Guide -  Issues w/ User authentication and authorization extended examples section/paragraph\n> --------------------------------------------------------------------------------------------------------------\n>\n>                 Key: DERBY-1823\n>                 URL: https://issues.apache.org/jira/browse/DERBY-1823\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Documentation\n>    Affects Versions: 10.1.1.0, 10.1.2.1, 10.1.3.1, 10.2.1.6, 10.2.2.0, 10.3.1.4\n>            Reporter: Francois Orsini\n>            Assignee: Kim Haase\n>            Priority: Minor\n>         Attachments: DERBY-1823.diff, DERBY-1823.zip\n>\n>\n> There is a couple of issues with the paragraph/section  \"User authentication and authorization extended examples\" in the developer's guide\n> http://db.apache.org/derby/docs/10.2/devguide/rdevcsecure26537.html\n> 1) The methods turnOnBuiltInUsers() & turnOffBuiltInUsers() do NOT shutdown and reboot the database for which the 'derby.connection.requireAuthentication' authentication database property is being set - as this last one is a derby static property, it will not be taken into account until the database is rebooted (or the whole derby engine instance). Hence, the 2 checks for \"Confirming requireAuthentication\" is misleading as the property value is changed _but_ the actual database authentication enabling/disabling has not changed since it was last booted. Database needs to be shutdown and rebooted after 'derby.connection.requireAuthentication' is set and then some negative testing of invalid user connection needs to be added to show that only valid users can connect (in the case, authentication is being enabled).\n> 2) Paragraph (extended examples section) also needs to be moved at the same level as the 2 above such as:\n>   \"User authentication example in a single-user, embedded environment\"\n>   http://db.apache.org/derby/docs/10.2/devguide/rdevcsecure125.html\n>   \"User authentication example in a client/server environment\"\n>   http://db.apache.org/derby/docs/10.2/devguide/rdevcsecure13713.html\n> since the extended examples (once fixed - see 1)) can be applied in both a client-server and embedded environments context.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2007-10-17T12:27:55Z"
  },
  "patches": [],
  "external_id": "DERBY-1823"
},{
  "_id": {
    "$oid": "5bbf2fa430623e2888ae147f"
  },
  "message_id": "<JIRA.12772918.1423206232000.276532.1423206274423@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf2f2430623e2888ae0cde"
    },
    {
      "$oid": "5bbf2f2430623e2888ae0cdf"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf2f2430623e2888ae0cde"
  },
  "from_id": {
    "$oid": "5bbdabda57674ee167d72cd5"
  },
  "to_ids": [
    {
      "$oid": "5bbf2ca057674ee16751d09e"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (LENS-283) Add latency metering for each resolver\n in cube query rewriter",
  "body": "Amareshwari Sriramadasu created LENS-283:\n--------------------------------------------\n\n             Summary: Add latency metering for each resolver in cube query rewriter\n                 Key: LENS-283\n                 URL: https://issues.apache.org/jira/browse/LENS-283\n             Project: Apache Lens\n          Issue Type: Improvement\n          Components: cube\n            Reporter: Amareshwari Sriramadasu\n             Fix For: 2.1\n\n\nSince cube query rewriter go through many resolvers sequentially, we want to find how much time is taken by each resolver in each phase.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-02-06T07:04:34Z"
  },
  "patches": [],
  "external_id": "LENS-283"
},{
  "_id": {
    "$oid": "5bbf0aefb79d666cbb225de9"
  },
  "message_id": "<JIRA.12980221.1466178033000.39644.1466372945460@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0aecb79d666cbb225d87"
    },
    {
      "$oid": "5bbf0aecb79d666cbb225d86"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0aecb79d666cbb225d86"
  },
  "from_id": {
    "$oid": "5bacc54f57674ee167debcb4"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-3861) Shrunk ISR before leader crash\n makes the partition unavailable",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-3861?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15338804#comment-15338804 ] \n\nMaysam Yabandeh commented on KAFKA-3861:\n----------------------------------------\n\nThanks [~wushujames]. I left a comment on KAFKA-3410.\n\n> Shrunk ISR before leader crash makes the partition unavailable\n> --------------------------------------------------------------\n>\n>                 Key: KAFKA-3861\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-3861\n>             Project: Kafka\n>          Issue Type: Bug\n>    Affects Versions: 0.10.0.0\n>            Reporter: Maysam Yabandeh\n>\n> We observed a case that the leader experienced a crash and lost its in-memory data and latest HW offsets. Normally Kafka should be safe and be able to make progress with a single node failure. However a few seconds before the crash the leader shrunk its ISR to itself, which is safe since min-in-sync-replicas is 2 and replication factor is 3 thus the troubled leader cannot accept new produce messages. After the crash however the controller could not name any of the of the followers as the new leader since as far as the controller knows they are not in ISR and could potentially be behind the last leader. Note that unclean-leader-election is disabled in this cluster since the cluster requires a very high degree of durability and cannot tolerate data loss.\n> The impact could get worse if the admin brings up the crashed broker in an attempt to make such partitions available again; this would take down even more brokers as the followers panic when they find their offset larger than HW offset in the leader:\n> {code}\n>     if (leaderEndOffset < replica.logEndOffset.messageOffset) {\n>       // Prior to truncating the follower's log, ensure that doing so is not disallowed by the configuration for unclean leader election.\n>       // This situation could only happen if the unclean election configuration for a topic changes while a replica is down. Otherwise,\n>       // we should never encounter this situation since a non-ISR leader cannot be elected if disallowed by the broker configuration.\n>       if (!LogConfig.fromProps(brokerConfig.originals, AdminUtils.fetchEntityConfig(replicaMgr.zkUtils,\n>         ConfigType.Topic, topicAndPartition.topic)).uncleanLeaderElectionEnable) {\n>         // Log a fatal error and shutdown the broker to ensure that data loss does not unexpectedly occur.\n>         fatal(\"Halting because log truncation is not allowed for topic %s,\".format(topicAndPartition.topic) +\n>           \" Current leader %d's latest offset %d is less than replica %d's latest offset %d\"\n>           .format(sourceBroker.id, leaderEndOffset, brokerConfig.brokerId, replica.logEndOffset.messageOffset))\n>         Runtime.getRuntime.halt(1)\n>       }\n> {code}\n> One hackish solution would be that the admin investigates the logs, determine that unclean-leader-election in this particular case would be safe and temporarily enables it (while the crashed node is down) until new leaders are selected for affected partitions, wait for the topics LEO advances far enough and then brings up the crashed node again. This manual process is however slow and error-prone and the cluster will suffer partial unavailability in the meanwhile.\n> We are thinking of having the controller make an exception for this case: if ISR size is less than min-in-sync-replicas and the new leader would be -1, then the controller does an RPC to all the replicas and inquire of the latest offset, and if all the replicas responded then chose the one with the largest offset as the leader as well as the new ISR. Note that the controller cannot do that if any of the non-leader replicas do not respond since there might be a case that the responding replicas have not been involved the last ISR and hence potentially behind the others (and the controller could not know that since it does not keep track of previous ISR).\n> Pros would be that kafka will be safely available when such cases occur and would not require any admin intervention. The cons however is that the controller talking to brokers inside the leader election function would break the existing pattern in the source code as currently the leader is elected locally without requiring any additional RPC.\n> Thoughts?\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-06-19T21:49:05Z"
  },
  "patches": [],
  "external_id": "KAFKA-3861"
},{
  "_id": {
    "$oid": "5bbe1010b1ffc5570d03fbf2"
  },
  "message_id": "<JIRA.12773012.1423232226000.281948.1423248635796@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e4eb1ffc5570d03b7b4"
  },
  "reference_ids": [
    {
      "$oid": "5bbe100fb1ffc5570d03fbe9"
    },
    {
      "$oid": "5bbe1010b1ffc5570d03fbea"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe100fb1ffc5570d03fbe9"
  },
  "from_id": {
    "$oid": "59bfa242f2a4565fe9fa1469"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (TIKA-1544) empty lines are not preserved",
  "body": "\n    [ https://issues.apache.org/jira/browse/TIKA-1544?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14309621#comment-14309621 ] \n\nTim Allison commented on TIKA-1544:\n-----------------------------------\n\nAh, ok.  I'll take a look.  I've been away from the RTF parser for a while.\n\n> empty lines are not preserved\n> -----------------------------\n>\n>                 Key: TIKA-1544\n>                 URL: https://issues.apache.org/jira/browse/TIKA-1544\n>             Project: Tika\n>          Issue Type: Bug\n>    Affects Versions: 1.6\n>         Environment: Windows 8, Java 1.8\n>            Reporter: mortee\n>            Priority: Minor\n>\n> I'm trying to extract the text content from RTF documents. The files contain empty lines (two or more consecutive paragraph-end marks), on which the further processing relies to tell apart different parts of the text. But unfortuantely Tika (with --text switch) eliminates all those empty lines, instead of preserving them.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-02-06T18:50:35Z"
  },
  "patches": [],
  "external_id": "TIKA-1544"
},{
  "_id": {
    "$oid": "58bfd0a515d83644fcc4e963"
  },
  "message_id": "<JIRA.12458594.1268175537302.167728.1398292335393@arcas>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfd0a515d83644fcc4e962"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfd0a515d83644fcc4e962"
  },
  "from_id": {
    "$oid": "58bfc8c402ca40f8bf1478a3"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (ZOOKEEPER-694) Add unit tester for Zookeeper",
  "body": "\n     [ https://issues.apache.org/jira/browse/ZOOKEEPER-694?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMichi Mutsuzaki updated ZOOKEEPER-694:\n--------------------------------------\n\n    Fix Version/s:     (was: 3.5.0)\n                   3.6.0\n\n> Add unit tester for Zookeeper\n> -----------------------------\n>\n>                 Key: ZOOKEEPER-694\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-694\n>             Project: ZooKeeper\n>          Issue Type: Improvement\n>          Components: c client, tests\n>            Reporter: David Rosenstrauch\n>            Priority: Minor\n>             Fix For: 3.6.0\n>\n>\n> It would be nice to have a mock/fake version of org.apache.zookeeper.Zookeeper that could be used for unit testing.  i.e., a single instance Zookeeper that operates completely in memory, with no network or disk I/O.\n> This would make it possible to pass one of the memory-only fake Zookeeper's into unit tests, while using a real Zookeeper in production code.\n> i.e., maybe something like this:\n> public interface ZooKeeperService {\n> ...\n> }\n> public class ZooKeeperTester implements ZooKeeperService {\n> ...\n> (stand-alone, in-memory, test implementation)\n> ...\n> }\n> public class ZooKeeper implements ZooKeeperService {\n> ...\n> (\"real\" implementation)\n> ...\n> }\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-04-23T22:32:15Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-694"
},{
  "_id": {
    "$oid": "5bacb2b5faaadd76f8aa0bc8"
  },
  "message_id": "<1954932682.54021.1351732992590.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb2b5faaadd76f8aa0bc7"
  },
  "from_id": {
    "$oid": "5bacb0ba57674ee167d4aa49"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PIG-2898) Parallel execution of e2e tests",
  "body": "\n     [ https://issues.apache.org/jira/browse/PIG-2898?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRohini Palaniswamy updated PIG-2898:\n------------------------------------\n\n    Resolution: Fixed\n        Status: Resolved  (was: Patch Available)\n\nPatch committed to 0.11 and trunk. Thanks Ivan.\n                \n> Parallel execution of e2e tests\n> -------------------------------\n>\n>                 Key: PIG-2898\n>                 URL: https://issues.apache.org/jira/browse/PIG-2898\n>             Project: Pig\n>          Issue Type: Improvement\n>          Components: e2e harness\n>    Affects Versions: 0.10.0\n>            Reporter: Andrey Klochkov\n>            Assignee: Ivan A. Veselovsky\n>              Labels: test\n>             Fix For: 0.11, 0.12\n>\n>         Attachments: PIG-2898-trunk-3.patch, PIG-2898-trunk-8.patch\n>\n>\n> Today it takes ~19 hours to run the full set of e2e tests in mapred mode. The bottleneck here is the client side, and per our observations it can help a lot if the e2e harness would be able to run tests in parallel threads.\n> We prototyped changes in e2e harness allowing to run tests in a configurable number of threads. Preliminary results show more than 6x reduction in execution time when using a small 3-nodes M/R cluster with modest configuration. Going to share a patch shortly.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-11-01T01:23:12Z"
  },
  "patches": [],
  "external_id": "PIG-2898"
},{
  "_id": {
    "$oid": "60fd7a6236c61279ee0b9244"
  },
  "message_id": "<CALiX4iYj+97pfXM7jp_mUTk9NFg0JCkAT9gMW7mR_zwjNxCqKg@mail.gmail.com>",
  "mailing_list_id": {
    "$oid": "60fd79fc36c61279ee0b865d"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "58c11ba602ca40f8bfb1f533"
  },
  "to_ids": [
    {
      "$oid": "60fd7a24f73e2aa390fe7ffa"
    }
  ],
  "cc_ids": [],
  "subject": "TopicTest failure (FINERACT-776)",
  "body": "Would anyone be willing to step up and fix the failing TopicTest?\n\nsee https://issues.apache.org/jira/browse/FINERACT-776\n\nand https://github.com/apache/fineract/pull/602\n\nJust ask if not clear / questions.\n\n_______________________\nMichael Vorburger\nhttp://www.vorburger.ch\n",
  "date": {
    "$date": "2019-07-03T23:27:10Z"
  },
  "patches": [],
  "external_id": "FINERACT-776"
},{
  "_id": {
    "$oid": "5f27c4bce2367af241b6d0c8"
  },
  "message_id": "<JIRA.12652225.1370968947126.99910.1370980040781@arcas>",
  "mailing_list_id": {
    "$oid": "5f27c38ce2367af241b67514"
  },
  "reference_ids": [
    {
      "$oid": "5f27c4bce2367af241b6d0c4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27c4bce2367af241b6d0c4"
  },
  "from_id": {
    "$oid": "58bfd09f02ca40f8bf148280"
  },
  "to_ids": [
    {
      "$oid": "58bfd014e4f89451f55ce17b"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (BIGTOP-1005) Create versionless symlinks in\n hadoop client directory",
  "body": "\n     [ https://issues.apache.org/jira/browse/BIGTOP-1005?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nSean Mackrory updated BIGTOP-1005:\n----------------------------------\n\n    Attachment: 0001-BIGTOP-1005.-Create-versionless-symlinks-in-hadoop-c.patch\n    \n> Create versionless symlinks in hadoop client directory\n> ------------------------------------------------------\n>\n>                 Key: BIGTOP-1005\n>                 URL: https://issues.apache.org/jira/browse/BIGTOP-1005\n>             Project: Bigtop\n>          Issue Type: Bug\n>            Reporter: Sean Mackrory\n>            Assignee: Sean Mackrory\n>            Priority: Minor\n>         Attachments: 0001-BIGTOP-1005.-Create-versionless-symlinks-in-hadoop-c.patch\n>\n>\n> In other directories in the hadoop package, we create versionless symlinks to the JARs. It would be convenient if we did the same for the /client/ directory.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-06-11T19:47:20Z"
  },
  "patches": [],
  "external_id": "BIGTOP-1005"
},{
  "_id": {
    "$oid": "5f27d66fa85ed7e944f89eb3"
  },
  "message_id": "<131492347.1137423781776.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27d65fa85ed7e944f89c4e"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "5f27d096af02e2d6de933700"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "body": "Improve SVN directory structure\n-------------------------------\n\n         Key: DOXIA-46\n         URL: http://jira.codehaus.org/browse/DOXIA-46\n     Project: doxia\n        Type: Improvement\n\n    Reporter: Grzegorz Slowikowski\n    Priority: Trivial\n\n\n1.\nI think that doxia-module-twiki should be moved to sandbox (this requires creating sandbox) because it is not a part of the build yet. On the other hand I find it inconsistent, that this module has its own subdirectory, and the others are all in core.\n2.\nThere should be \"site\" submodule. Now site \"src\" directory is a direct subdirectory of \"trunk\". It looks strange like it is now.\nAll other multiprojects have their site as a separate submodule.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-01-16T09:03:01Z"
  },
  "from_id": {
    "$oid": "5f27d1dcaf02e2d6de9b0f22"
  },
  "subject": "[jira] Created: (DOXIA-46) Improve SVN directory structure",
  "external_id": "DOXIA-46"
},{
  "_id": {
    "$oid": "5bc86fb957a11257de56ca5f"
  },
  "message_id": "<1525532089.80.1337875839569.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc86f4257a11257de56c634"
  },
  "from_id": {
    "$oid": "5bbe100357674ee16798fc99"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-1277) NPE when extracting image inside\n form",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1277?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAndreas Lehmkühler updated PDFBOX-1277:\n---------------------------------------\n\n    Fix Version/s:     (was: 1.7.0)\n    \n> NPE when extracting image inside form\n> -------------------------------------\n>\n>                 Key: PDFBOX-1277\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1277\n>             Project: PDFBox\n>          Issue Type: Bug\n>            Reporter: Daniel Bonniot de Ruisselet\n>         Attachments: in.pdf\n>\n>\n> Unsing current HEAD version:\n> java -cp app/target/pdfbox-app-1.7.0-SNAPSHOT.jar org.apache.pdfbox.ExtractImages /tmp/in.pdf \n> Exception in thread \"main\" java.lang.NullPointerException\n> \tat org.apache.pdfbox.ExtractImages.processResources(ExtractImages.java:166)\n> \tat org.apache.pdfbox.ExtractImages.processResources(ExtractImages.java:195)\n> \tat org.apache.pdfbox.ExtractImages.extractImages(ExtractImages.java:150)\n> \tat org.apache.pdfbox.ExtractImages.main(ExtractImages.java:64)\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n       \n",
  "date": {
    "$date": "2012-05-24T16:10:39Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1277"
},{
  "_id": {
    "$oid": "5bc869cf57a11257de569d04"
  },
  "message_id": "<JIRA.12718148.1401801093201.87613.1402157282050@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc869bb57a11257de569c67"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc869bb57a11257de569c67"
  },
  "from_id": {
    "$oid": "5bbe100357674ee16798fc99"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-2110) Font not found: CourierNew",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-2110?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAndreas Lehmkühler updated PDFBOX-2110:\n---------------------------------------\n\n    Affects Version/s: 1.8.5\n\n> Font not found: CourierNew\n> --------------------------\n>\n>                 Key: PDFBOX-2110\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-2110\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: FontBox\n>    Affects Versions: 1.8.5, 2.0.0\n>            Reporter: Juraj Lonc\n>            Assignee: Andreas Lehmkühler\n>             Fix For: 1.8.6, 2.0.0\n>\n>         Attachments: PDFBOX-2110_FontManager.diff, testpdf_monospace_DPH_032014.pdf\n>\n>\n> PDF uses non-embedded font \"CourierNew\".\n> OS contains font:\n> {code}/usr/share/fonts/truetype/msttcorefonts/Courier_New.ttf: Courier New:style=Regular,Normal,obyèejné,Standard,????????,Normaali,Normál,Normale,Standaard,Normal{code}\n> FontManager is not able to find it and warns:\n> {code}WARN  [org.apache.fontbox.util.FontManager] (http-0.0.0.0-80-6) Font not found: CourierNew{code}\n> It seems that the problem is in that space in font name \"CourierNew\" vs. \"Courier New\"\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-06-07T16:08:02Z"
  },
  "patches": [],
  "external_id": "PDFBOX-2110"
},{
  "_id": {
    "$oid": "60fac2edd907ab79037e693c"
  },
  "message_id": "<JIRA.13213219.1548984313000.221637.1549034580060@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac2d5d907ab79037e61e3"
    },
    {
      "$oid": "60fac2d5d907ab79037e61e4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac2d5d907ab79037e61e3"
  },
  "from_id": {
    "$oid": "60fac2d4f73e2aa390c8525c"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DIRSERVER-2264) missing schema type for NIS:\n nisMapName",
  "body": "\n    [ https://issues.apache.org/jira/browse/DIRSERVER-2264?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16758403#comment-16758403 ] \n\nPhilip Brown commented on DIRSERVER-2264:\n-----------------------------------------\n\nwhat’s the best way for me to get a working instance on my servers then?\n\nOn Fri, Feb 1, 2019 at 2:30 AM Emmanuel Lecharny (JIRA) <jira@apache.org>\n\n\n\n> missing schema type for NIS: nisMapName\n> ---------------------------------------\n>\n>                 Key: DIRSERVER-2264\n>                 URL: https://issues.apache.org/jira/browse/DIRSERVER-2264\n>             Project: Directory ApacheDS\n>          Issue Type: Bug\n>          Components: schema\n>    Affects Versions: 2.0.0.AM25\n>            Reporter: Philip Brown\n>            Priority: Major\n>\n> the directory shipps with a bunch of NIS schema objects.. but it is missing a crucial one:\n> olcAttributeTypes: ( 1.3.6.1.1.1.1.26 NAME 'nisMapName'\n>  DESC 'Name of a generic NIS map'\n>  EQUALITY caseIgnoreMatch\n>  SYNTAX 1.3.6.1.4.1.1466.115.121.1.15\\{64} )\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2019-02-01T15:23:00Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-2264"
},{
  "_id": {
    "$oid": "5bea9f749e73d744d41335a3"
  },
  "message_id": "<641935446.1142837172367.JavaMail.jira@ajax>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea9e8b9e73d744d412fe71"
  },
  "from_id": {
    "$oid": "5bea9e7735e3ea2b7b5e04b3"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (DERBY-1072) Misleading/wrong error message while\n dropping a view",
  "body": "    [ http://issues.apache.org/jira/browse/DERBY-1072?page=comments#action_12371041 ] \n\nMayuresh Nirhali commented on DERBY-1072:\n-----------------------------------------\n\nThanks for your response!\n\nDiffering changes ( to the other languages messages) is a convinient option for me, but before deciding that, I would like to understand who and how is this going to happen before release!\nDo I need to create any placeholder for this ??\nBefore release, is anyone going to identify if there are any inconsistencies between different language messages ??\nIs it okay If I just modify the english messages  for now ??\n\n\n> Misleading/wrong error message while dropping a view\n> ----------------------------------------------------\n>\n>          Key: DERBY-1072\n>          URL: http://issues.apache.org/jira/browse/DERBY-1072\n>      Project: Derby\n>         Type: Bug\n>   Components: Newcomer, SQL\n>     Versions: 10.1.2.2\n>  Environment: windows xp , jdk1.5\n>     Reporter: Manjula Kutty\n>     Assignee: Mayuresh Nirhali\n>     Priority: Minor\n\n>\n> while dropping a view which does not really exist on a database , the error message thrown by ij says ERROR X0X05: Table 'VIEW222' does not  exist. It looks like we should make the error message to say that  \"Table/View\" does not exist, instead of \"table\".\n> Look at other messages that are shared for both Table and View.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-03-20T06:46:12Z"
  },
  "patches": [],
  "external_id": "DERBY-1072"
},{
  "_id": {
    "$oid": "5bacb4b2faaadd76f8aa8399"
  },
  "message_id": "<925453977.1207041155185.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bacb4a3faaadd76f8aa8031"
  },
  "from_id": {
    "$oid": "5bacb47a57674ee167dbdf61"
  },
  "to_ids": [
    {
      "$oid": "59bfb5fef2a4565fe9250f45"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Issue Comment Edited: (PIG-161) Rework physical plan",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-161?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12584072#action_12584072 ] \n\nshravanmn edited comment on PIG-161 at 4/1/08 2:10 AM:\n----------------------------------------------------------------------------\n\nThank you for the input Pi. Responses in line...\n\n    * The proposal in wiki says \"Make the model entirely push-based\" but this implementation is pull-based. What is the real direction on this?\n[shrav] Hmm, Interesting observation. We never realized there could be such a grave error lurking in the documentation. The consensus was to have a single threaded, pull based model. The current model is push based. We need to change this.\n@Alan: I am not sure how this happened?!\n    * This comment is obvious. Those dummy type instances in POFilter can be made static. If they are used all over the place, we should move them to a more generic parent class.\n[shrav] Its a good idea. I think we can introduce these into PhysicalOperator as static dummy types. But we should also make this known to anyone implementing physical operators so that these can be used. I will move them.\n    * visit() should not throw ParseException. Look at PIG-169 for example. In some cases we use visit later in the processing after parsing. If we expect ParseException when parsing then we can just wrap in ParseException.\n[shrav] This is a side effect of the Operator's visit method throwing ParseException. But I think you are right. We should throw some other generic exception than a ParseException.\n@Alan: Could you please comment on this?\n    * Therefore doAllPredecessors() and doAllPredecessors() also should not throw ParseException\n[shrav] Same as the previous one.\n    * GreaterThanExpr, LessThanExpr, GTOrEqualToExpr, NotEqualToExpr + a few more are 90% the same. Is it possible to have a common abstract class for them?\n[shrav] Well, if you can think of a better way it would be great. The main thing that nfluenced this choice was that we wanted to minimize the branching constructs. The moment we introduce an abstract class, we would need them. This was done because the physical side assumes that the type checker will provide appropriate type information and we can use them inside expressions to avoid branching.\n    * This is subjective. The name depthFirst() for me is a bit mis-leading because this specific depthFirst() doesn't walk over already seen nodes.\n[shrav] I concur. It is depthFirst in a restricted sense. Again the same search can be done in both directions either from the top or from the bottom. If we choose to change this in OperatorPlan, I am fine with this.\n@Alan: Your comments needed.\n    * From this \"The input model assumes that it can either be taken from an operator or can be attached directly to this operator\" could you explain more what attachinput and input in PhysicalOperator do?\n[shrav] PhysicalOperator is the base class for all the operators. Since each operator needed the same input model, I just refactored it into the base class. So the input model is an artifact of the execution model. Since we decided have attribute plans, the model that we converged to was that the attribute plans be separated from the top level operator plan and scoped within the operator that it is an attribute of. So when the top level operator pulls tuples from its input operator, it attaches these tuples to the attribute plan and calls the getNext on the root operator of the plan. The attach of the attribute plan in turn uses attachInput fn the leaf operators. I hope I am clear. If not please let me know. I wil rephrase it.\n\n      was (Author: shravanmn):\n    Thank you for the input Pi. Responses in line...\n\n    * The proposal in wiki says \"Make the model entirely push-based\" but this implementation is pull-based. What is the real direction on this?\nHmm, Interesting observation. We never realized there could be such a grave error lurking in the documentation. The consensus was to have a single threaded, pull based model. The current model is push based. We need to change this.\n@Alan: I am not sure how this happened?!\n    * This comment is obvious. Those dummy type instances in POFilter can be made static. If they are used all over the place, we should move them to a more generic parent class.\nIts a good idea. I think we can introduce these into PhysicalOperator as static dummy types. But we should also make this known to anyone implementing physical operators so that these can be used. I will move them.\n    * visit() should not throw ParseException. Look at PIG-169 for example. In some cases we use visit later in the processing after parsing. If we expect ParseException when parsing then we can just wrap in ParseException.\nThis is a side effect of the Operator's visit method throwing ParseException. But I think you are right. We should throw some other generic exception than a ParseException.\n@Alan: Could you please comment on this?\n    * Therefore doAllPredecessors() and doAllPredecessors() also should not throw ParseException\nSame as the previous one.\n    * GreaterThanExpr, LessThanExpr, GTOrEqualToExpr, NotEqualToExpr + a few more are 90% the same. Is it possible to have a common abstract class for them?\nWell, if you can think of a better way it would be great. The main thing that nfluenced this choice was that we wanted to minimize the branching constructs. The moment we introduce an abstract class, we would need them. This was done because the physical side assumes that the type checker will provide appropriate type information and we can use them inside expressions to avoid branching.\n    * This is subjective. The name depthFirst() for me is a bit mis-leading because this specific depthFirst() doesn't walk over already seen nodes.\nI concur. It is depthFirst in a restricted sense. Again the same search can be done in both directions either from the top or from the bottom. If we choose to change this in OperatorPlan, I am fine with this.\n@Alan: Your comments needed.\n    * From this \"The input model assumes that it can either be taken from an operator or can be attached directly to this operator\" could you explain more what attachinput and input in PhysicalOperator do?\nPhysicalOperator is the base class for all the operators. Since each operator needed the same input model, I just refactored it into the base class. So the input model is an artifact of the execution model. Since we decided have attribute plans, the model that we converged to was that the attribute plans be separated from the top level operator plan and scoped within the operator that it is an attribute of. So when the top level operator pulls tuples from its input operator, it attaches these tuples to the attribute plan and calls the getNext on the root operator of the plan. The attach of the attribute plan in turn uses attachInput fn the leaf operators. I hope I am clear. If not please let me know. I wil rephrase it.\n  \n> Rework physical plan\n> --------------------\n>\n>                 Key: PIG-161\n>                 URL: https://issues.apache.org/jira/browse/PIG-161\n>             Project: Pig\n>          Issue Type: Sub-task\n>            Reporter: Alan Gates\n>            Assignee: Alan Gates\n>         Attachments: Phy_AbsClass.patch\n>\n>\n> This bug tracks work to rework all of the physical operators as described in http://wiki.apache.org/pig/PigTypesFunctionalSpec\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-04-01T02:12:35Z"
  },
  "patches": [],
  "external_id": "PIG-161"
},{
  "_id": {
    "$oid": "58bfd0c215d83644fcc4ef8a"
  },
  "message_id": "<JIRA.12688419.1389484207335.89461.1389650158088@arcas>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfd0ba15d83644fcc4edcc"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfd0ba15d83644fcc4edcc"
  },
  "from_id": {
    "$oid": "58bfc8dc02ca40f8bf1478f8"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (ZOOKEEPER-1861) ConcurrentHashMap isn't used\n properly in QuorumCnxManager",
  "body": "\n     [ https://issues.apache.org/jira/browse/ZOOKEEPER-1861?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nTed Yu updated ZOOKEEPER-1861:\n------------------------------\n\n    Attachment: zookeeper-1861-v2.txt\n\nPatch v2 addresses Michi's comments\n\n> ConcurrentHashMap isn't used properly in QuorumCnxManager\n> ---------------------------------------------------------\n>\n>                 Key: ZOOKEEPER-1861\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-1861\n>             Project: ZooKeeper\n>          Issue Type: Bug\n>            Reporter: Ted Yu\n>            Assignee: Ted Yu\n>            Priority: Minor\n>         Attachments: zookeeper-1861-v1.txt, zookeeper-1861-v2.txt\n>\n>\n> queueSendMap is a ConcurrentHashMap.\n> At line 210:\n> {code}\n>             if (!queueSendMap.containsKey(sid)) {\n>                 queueSendMap.put(sid, new ArrayBlockingQueue<ByteBuffer>(\n>                         SEND_CAPACITY));\n> {code}\n> By the time control enters if block, there may be another concurrent put with same sid to the ConcurrentHashMap.\n> putIfAbsent() should be used.\n> Similar issue occurs at line 307 as well.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-01-13T21:55:58Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-1861"
},{
  "_id": {
    "$oid": "5f27bec7a7dc6ca79d80f701"
  },
  "message_id": "<10114469.318291285084593313.JavaMail.jira@thor>",
  "mailing_list_id": {
    "$oid": "5f27bd66a7dc6ca79d809788"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c04"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "body": "cache should use both Last-Modified and ETag for validations when available\n---------------------------------------------------------------------------\n\n                 Key: HTTPCLIENT-998\n                 URL: https://issues.apache.org/jira/browse/HTTPCLIENT-998\n             Project: HttpComponents HttpClient\n          Issue Type: Improvement\n          Components: Cache\n    Affects Versions: 4.1 Alpha2\n            Reporter: Jonathan Moore\n         Attachments: use-all-validators.patch\n\nThis is a protocol recommendation:\n\n\"[HTTP/1.1 clients], if both an entity tag and a Last-Modified value have been provided by the origin server, SHOULD use both validators in cache-conditional requests. This allows both HTTP/1.0 and HTTP/1.1 caches to respond appropriately.\"\n\nhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.3.4\n\nThe current implementation only uses the ETag when conditionally validating an entry, so HTTP/1.0 caches can't currently reply to us with a 304 (Not Modified), even if that would be appropriate.\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@hc.apache.org\nFor additional commands, e-mail: dev-help@hc.apache.org\n\n",
  "date": {
    "$date": "2010-09-21T11:56:33Z"
  },
  "from_id": {
    "$oid": "59677cd7aff2204b3cbd16ae"
  },
  "subject": "[jira] Created: (HTTPCLIENT-998) cache should use both\n Last-Modified and ETag for validations when available",
  "external_id": "HTTPCLIENT-998"
},{
  "_id": {
    "$oid": "5bacb337faaadd76f8aa296c"
  },
  "message_id": "<227697599.61003.1323478959992.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5bacb31f57674ee167d93214"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PIG-2420) FLATTEN of null tuples produces\n unexpected number of fields",
  "body": "FLATTEN of null tuples produces unexpected number of fields\n-----------------------------------------------------------\n\n                 Key: PIG-2420\n                 URL: https://issues.apache.org/jira/browse/PIG-2420\n             Project: Pig\n          Issue Type: Bug\n    Affects Versions: 0.9.0, 0.8.1, 0.8.0, 0.9.1, 0.10, 0.9.2\n            Reporter: Dmitriy V. Ryaboy\n\n\nFlattening a null tuple results in a single column (with the value null) being produced.\n\nThat leads to all the columns after the flattened value shifting left by n-1 positions, where n is the number of expected fields in a tuple!\n\nConsider:\n\ngrunt> sh cat tmp/x\nfoo\t bar\na\t(b,c)\td\ngrunt> x = load 'tmp/x' as (a:chararray, b:(b:chararray, c:chararray), d:chararray);\ngrunt> projected = foreach x generate d;\ngrunt> dump projected   \n(bar)\n(d)\n\ngrunt> flattened = foreach x generate a, flatten(b) as (b, c), d;\ngrunt> dump flattened\n(foo,,bar)  -- NOTE THREE FIELDS INSTEAD OF EXPECTED 4\n(a,b,c,d)                           \ngrunt> projected = foreach flattened generate d;    \ngrunt> dump projected   \n()  -- NOTE WRONG VALUE \n(d)\ngrunt> projected = foreach flattened generate c;\n() -- NOTE THAT, INCONSISTENTLY, C is NULL! AS IS B.\n(c)\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-12-10T01:02:39Z"
  },
  "patches": [],
  "external_id": "PIG-2420"
},{
  "_id": {
    "$oid": "5bacaa7bc593cd2007765897"
  },
  "message_id": "<JIRA.13023647.1480316379000.375336.1480316398356@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5baca8d0c593cd200776058b"
  },
  "reference_ids": [
    {
      "$oid": "5bacaa7bc593cd2007765896"
    },
    {
      "$oid": "5bacaa7bc593cd2007765895"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacaa7bc593cd2007765895"
  },
  "from_id": {
    "$oid": "5baca91c57674ee167cde68e"
  },
  "to_ids": [
    {
      "$oid": "5baca91c57674ee167cde691"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (ZEPPELIN-1716) Error of interpreter not found is\n not propagated to frontend.",
  "body": "Jeff Zhang created ZEPPELIN-1716:\n------------------------------------\n\n             Summary: Error of interpreter not found is not propagated to frontend.\n                 Key: ZEPPELIN-1716\n                 URL: https://issues.apache.org/jira/browse/ZEPPELIN-1716\n             Project: Zeppelin\n          Issue Type: Bug\n    Affects Versions: 0.7.0\n            Reporter: Jeff Zhang\n            Assignee: Jeff Zhang\n\n\nIt is introduced in ZEPPELIN-1399, we call InterpreterFactory.getInterpreter before calling Paragraph.jobRun\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-11-28T06:59:58Z"
  },
  "patches": [],
  "external_id": "ZEPPELIN-1716"
},{
  "_id": {
    "$oid": "5bbe0ed6b1ffc5570d03c701"
  },
  "message_id": "<JIRA.13029274.1482185543000.3022.1504216620645@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e4eb1ffc5570d03b7b4"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0ec5b1ffc5570d03c4ef"
    },
    {
      "$oid": "5bbe0ec5b1ffc5570d03c4ee"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0ec5b1ffc5570d03c4ee"
  },
  "from_id": {
    "$oid": "59bfa282f2a4565fe9faaf80"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (TIKA-2219) CharsetDetector no longer detects\n windows-1252 charset",
  "body": "\n     [ https://issues.apache.org/jira/browse/TIKA-2219?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMatthew Caruana Galizia updated TIKA-2219:\n------------------------------------------\n    Attachment: test.txt\n\nThis file contains x92 characters which should force detection to Windows-1252.\n\n> CharsetDetector no longer detects windows-1252 charset\n> ------------------------------------------------------\n>\n>                 Key: TIKA-2219\n>                 URL: https://issues.apache.org/jira/browse/TIKA-2219\n>             Project: Tika\n>          Issue Type: Bug\n>          Components: parser\n>    Affects Versions: 1.14\n>         Environment: Any.\n>            Reporter: Pascal Essiembre\n>            Priority: Minor\n>             Fix For: 2.0, 1.15\n>\n>         Attachments: test.txt\n>\n>\n> Starting with Tika 1.14, windows-1252 is no longer detected, as ISO-8859-1 is always detected instead.  While not tested, this likely affects other windows-125* encodings as well.\n> I tracked it down to a change in the {{CharsetRecog_sbcs.CharsetRecog_8859_1#getName()}} method.  Now it always returns \"ISO-8859-1\" whereas before it was: {{return haveC1Bytes ? \"windows-1252\" : \"ISO-8859-1\";}}\n> Now that condition has been moved to the {{match(CharsetDetector det)}} method so that the returned CharsetMatch has the proper name.  The problem with that is {{CharsetDetector#detectAll()}} method overwrites the correct match with a new one that will return the value of {{#getName()}}  from the {{CharsetRecognizer}} instead (which is always \"ISO-8859-1\" in this case).\n> There might be legitimate reasons why the {{CharsetMatch}} instances in {{detectAll()}} method are replaced with new ones, but changing this code in that method appears to work for me:\n> // Remove this:\n> //                    CharsetMatch m = new CharsetMatch(this, csr, confidence);\n> //                    matches.add(m);\n> // Add this instead:\n>                     matches.add(charsetMatch);\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-08-31T21:57:00Z"
  },
  "patches": [],
  "external_id": "TIKA-2219"
},{
  "_id": {
    "$oid": "5bbef761748b2d53b76201ce"
  },
  "message_id": "<440323065.27041.1332913483345.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bbef648748b2d53b761c752"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbef755748b2d53b761ff1d"
  },
  "from_id": {
    "$oid": "5bbef75957674ee1676bdc3a"
  },
  "to_ids": [
    {
      "$oid": "5bbef67e57674ee1676a2fde"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DELTASPIKE-132) Discuss the concept of the\n exception chain / stack",
  "body": "\n    [ https://issues.apache.org/jira/browse/DELTASPIKE-132?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13240198#comment-13240198 ] \n\nJason Porter commented on DELTASPIKE-132:\n-----------------------------------------\n\nI believe as it currently stands in Seam Catch (Solder) all of these cases are handled. You can get a hold of the ExceptionStack via an event before it's processed and make any changes you desire.\n                \n> Discuss the concept of the exception chain / stack\n> --------------------------------------------------\n>\n>                 Key: DELTASPIKE-132\n>                 URL: https://issues.apache.org/jira/browse/DELTASPIKE-132\n>             Project: DeltaSpike\n>          Issue Type: Sub-task\n>          Components: ExceptionHandler-Module\n>            Reporter: Jason Porter\n>            Assignee: Gerhard Petracek\n>             Fix For: 0.2-incubating\n>\n>\n> Exceptions in Java are often a chain of exceptions, or a stack of exceptions.\n> A problem of many Java developers, especially newer developers is finding the actual root cause of an exception. The Exception Handling module should take this guess work out when it finds exception handlers and first look for a handler for the root cause. The exception stack should be completely unwrapped before looking for a handler.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-03-28T05:44:43Z"
  },
  "patches": [],
  "external_id": "DELTASPIKE-132"
},{
  "_id": {
    "$oid": "5bacc4d856f6a00b02094493"
  },
  "message_id": "<55158D23.6050209@gatech.edu>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc4d856f6a00b02094475"
    },
    {
      "$oid": "5bacc4d856f6a00b02094484"
    },
    {
      "$oid": "5bacc4d856f6a00b02094473"
    },
    {
      "$oid": "5bacc4d856f6a00b02094474"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc4d856f6a00b02094484"
  },
  "from_id": {
    "$oid": "5bacc47057674ee167dca9b2"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [jira] [Created] (MAHOUT-1659) Remove deprecated Lanczos solver\n from spectral clustering in mr-legacy",
  "body": "Honestly not sure, as I haven't had a chance to play around with the \nscala dsl much yet. Suneel suggested we save that for 0.10.1.\n\nOn 3/27/15 12:00 PM, Dmitriy Lyubimov wrote:\n> Shannon,\n>\n> How difficult would it be to port spectral clustering to our scala alg and\n> math? We have ssvd there as well.\n> On Mar 27, 2015 7:26 AM, \"Shannon Quinn (JIRA)\" <jira@apache.org> wrote:\n>\n>> Shannon Quinn created MAHOUT-1659:\n>> -------------------------------------\n>>\n>>               Summary: Remove deprecated Lanczos solver from spectral\n>> clustering in mr-legacy\n>>                   Key: MAHOUT-1659\n>>                   URL: https://issues.apache.org/jira/browse/MAHOUT-1659\n>>               Project: Mahout\n>>            Issue Type: Task\n>>            Components: Clustering, mrlegacy\n>>      Affects Versions: 0.9\n>>              Reporter: Shannon Quinn\n>>              Assignee: Shannon Quinn\n>>              Priority: Minor\n>>               Fix For: 0.10.0\n>>\n>>\n>> Spectral clustering still has the option of using either SSVD or the\n>> Lanczos solver for dimensionality reduction. Remove the latter entirely.\n>>\n>>\n>>\n>> --\n>> This message was sent by Atlassian JIRA\n>> (v6.3.4#6332)\n>>\n\n",
  "date": {
    "$date": "2015-03-27T13:02:27Z"
  },
  "patches": [],
  "external_id": "MAHOUT-1659"
},{
  "_id": {
    "$oid": "5c580623e078b00ec4e7581c"
  },
  "message_id": "<JIRA.13043516.1487227509000.16887.1487669744300@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c58060fe078b00ec4e75772"
    },
    {
      "$oid": "5c58060fe078b00ec4e75771"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c58060fe078b00ec4e75771"
  },
  "from_id": {
    "$oid": "5bbdf32f57674ee1677ee824"
  },
  "to_ids": [
    {
      "$oid": "5bbdf31357674ee1677dbf18"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (RANGER-1387) Remove unused SQL_CONNECTOR_JAR in\n install.properties of ranger plugin",
  "body": "\n     [ https://issues.apache.org/jira/browse/RANGER-1387?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nQiang Zhang updated RANGER-1387:\n--------------------------------\n    Attachment: Ranger900commit.log\n\n> Remove unused SQL_CONNECTOR_JAR in install.properties of ranger plugin\n> ----------------------------------------------------------------------\n>\n>                 Key: RANGER-1387\n>                 URL: https://issues.apache.org/jira/browse/RANGER-1387\n>             Project: Ranger\n>          Issue Type: Bug\n>          Components: plugins\n>            Reporter: Qiang Zhang\n>            Assignee: Qiang Zhang\n>            Priority: Minor\n>              Labels: patch\n>             Fix For: 1.0.0\n>\n>         Attachments: 0001-RANGER-1387-Remove-unused-SQL_CONNECTOR_JAR-in-insta.patch, Ranger900commit.log\n>\n>\n> There is configuration SQL_CONNECTOR_JAR in install.properties of ranger plugin(hdfs,hive,etc),\n> it is used to store audit data in database(mysql,oracle,etc) by ranger plugin.\n> As ranger does not support to store audit data in database any more,\n> please refer RANGER-900:Remove support for DB based auditing,\n> so SQL_CONNECTOR_JAR is useless when user install ranger plugin.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-21T09:35:44Z"
  },
  "patches": [],
  "external_id": "RANGER-1387"
},{
  "_id": {
    "$oid": "5bbf291630623e2888adc8a6"
  },
  "message_id": "<JIRA.13001214.1472540062000.230094.1478675999645@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf291630623e2888adc8a4"
    },
    {
      "$oid": "5bbf27e630623e2888adc19a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf27e630623e2888adc19a"
  },
  "from_id": {
    "$oid": "5bbdaabc57674ee167d03561"
  },
  "to_ids": [
    {
      "$oid": "5bbf261257674ee1674f913a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (LENS-1302) Add launch rejection reason to query\n status message",
  "body": "\n     [ https://issues.apache.org/jira/browse/LENS-1302?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nRajat Khandelwal resolved LENS-1302.\n------------------------------------\n    Resolution: Fixed\n\nFixed alongwith LENS-743\n\n> Add launch rejection reason to query status message\n> ---------------------------------------------------\n>\n>                 Key: LENS-1302\n>                 URL: https://issues.apache.org/jira/browse/LENS-1302\n>             Project: Apache Lens\n>          Issue Type: Improvement\n>            Reporter: Rajat Khandelwal\n>            Assignee: Rajat Khandelwal\n>\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-11-09T07:19:59Z"
  },
  "patches": [],
  "external_id": "LENS-1302"
},{
  "_id": {
    "$oid": "60fac373d907ab79037e970f"
  },
  "message_id": "<JIRA.12839896.1435079081000.1063.1435096002488@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [
    {
      "$oid": "60fac364d907ab79037e9252"
    },
    {
      "$oid": "60fac364d907ab79037e9251"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fac364d907ab79037e9251"
  },
  "from_id": {
    "$oid": "5f27c1d6af02e2d6de6e05b2"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (DIRSTUDIO-1060) Exported OpenLDAP schema has\n syntax errors",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSTUDIO-1060?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nStefan Seelmann updated DIRSTUDIO-1060:\n---------------------------------------\n    Fix Version/s: 2.0.0-M10\n\n> Exported OpenLDAP schema has syntax errors\n> ------------------------------------------\n>\n>                 Key: DIRSTUDIO-1060\n>                 URL: https://issues.apache.org/jira/browse/DIRSTUDIO-1060\n>             Project: Directory Studio\n>          Issue Type: Bug\n>          Components: studio-schemaeditor\n>    Affects Versions: 2.0.0-M9 (2.0.0.v20150606-M9)\n>         Environment: slapd 2.4.40 (Debian version)\n>            Reporter: Christoph Kling\n>              Labels: easyfix\n>             Fix For: 2.0.0-M10\n>\n>\n> If you export a schema to OpenLDAP schema format, you cannot import it to slapd because slaptest fails with:\n> # slaptest -f test.conf -F tmp/\n> 55898c8a drksuewo.schema: line 5 attributetype: Missing closing parenthesis before end of input\n> AttributeTypeDescription = \"(\" whsp\n>   numericoid whsp      ; AttributeType identifier\n>   [ \"NAME\" qdescrs ]             ; name used in AttributeType\n>   [ \"DESC\" qdstring ]            ; description\n>   [ \"OBSOLETE\" whsp ]\n>   [ \"SUP\" woid ]                 ; derived from this other\n>                                    ; AttributeType\n>   [ \"EQUALITY\" woid ]            ; Matching Rule name\n>   [ \"ORDERING\" woid ]            ; Matching Rule name\n>   [ \"SUBSTR\" woid ]              ; Matching Rule name\n>   [ \"SYNTAX\" whsp noidlen whsp ] ; see section 4.3\n>   [ \"SINGLE-VALUE\" whsp ]        ; default multi-valued\n>   [ \"COLLECTIVE\" whsp ]          ; default not collective\n>   [ \"NO-USER-MODIFICATION\" whsp ]; default user modifiable\n>   [ \"USAGE\" whsp AttributeUsage ]; default userApplications\n>                                    ; userApplications\n>                                    ; directoryOperation\n>                                    ; distributedOperation\n>                                    ; dSAOperation\n>   whsp \")\"\n> slaptest: bad configuration directory!\n> The problem is that the exported schema misses the required whitespace character in front of the closing parenthesis of each attributetype and objectclass element. The fix is easy, just the renderer must be corrected.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-06-23T21:46:42Z"
  },
  "patches": [],
  "external_id": "DIRSTUDIO-1060"
},{
  "_id": {
    "$oid": "5bbf0b8335bc964434819cad"
  },
  "message_id": "<JIRA.12984736.1467168547000.170.1467210089263@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf090e35bc964434814f5a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0b2535bc964434818dc6"
    },
    {
      "$oid": "5bbf0b2535bc964434818dc7"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0b2535bc964434818dc6"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdff0"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (STORM-1934) Race condition between\n sync-supervisor and sync-processes raises several strange issues",
  "body": "\n    [ https://issues.apache.org/jira/browse/STORM-1934?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15355314#comment-15355314 ] \n\nASF GitHub Bot commented on STORM-1934:\n---------------------------------------\n\nGithub user HeartSaVioR commented on the issue:\n\n    https://github.com/apache/storm/pull/1528\n  \n    @arunmahadevan \n    This works perfectly.\n    \n    - Writing new assignment\n    ```\n    6700 {:storm-id \"test-topology2-4-1467185073\", :executors ([7 7] [4 4] [1 1]), :resources [0.0 0.0 0.0]}, \n    6702 {:storm-id \"test-topology2-4-1467185073\", :executors ([6 6] [3 3]), :resources [0.0 0.0 0.0]}, \n    6701 {:storm-id \"test-topology2-4-1467185073\", :executors ([5 5] [2 2]), :resources [0.0 0.0 0.0]}\n    ```\n    \n    - Assigned executors:\n    ```\n    {6700 {:storm-id \"test-topology2-4-1467185073\", :executors [[7 7] [4 4] [1 1]], :resources #object[org.apache.storm.generated.WorkerResources 0x6dabc02d \"WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)\"]}, \n    6701 {:storm-id \"test-topology2-4-1467185073\", :executors [[5 5] [2 2]], :resources #object[org.apache.storm.generated.WorkerResources 0x6de46954 \"WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)\"]}, \n    6702 {:storm-id \"test-topology2-4-1467185073\", :executors [[6 6] [3 3]], :resources #object[org.apache.storm.generated.WorkerResources 0x60f1fbb5 \"WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)\"]}}\n    ```\n    \n    - Allocated:\n    ```\n    \"612eeee0-0ddc-4820-9062-c79276d56cf2\" [:disallowed {:time-secs 1467209575, :storm-id \"test-topology2-4-1467185073\", :executors [[2 2] [6 6] [-1 -1] [4 4]], :port 6702}], \n    \"9751163a-f7b8-46a5-9b9c-bcd1fd1446cd\" [:disallowed {:time-secs 1467209574, :storm-id \"test-topology2-4-1467185073\", :executors [[7 7] [3 3] [1 1] [-1 -1] [5 5]], :port 6701}]\n    ```\n    \n    Now three workers are running.\n\n\n> Race condition between sync-supervisor and sync-processes raises several strange issues\n> ---------------------------------------------------------------------------------------\n>\n>                 Key: STORM-1934\n>                 URL: https://issues.apache.org/jira/browse/STORM-1934\n>             Project: Apache Storm\n>          Issue Type: Bug\n>          Components: storm-core\n>    Affects Versions: 1.0.0, 2.0.0, 1.0.1\n>            Reporter: Jungtaek Lim\n>            Assignee: Jungtaek Lim\n>            Priority: Critical\n>\n> There're some strange issues including STORM-1933 and others (which I will file an issue soon) which are related to race condition in supervisor.\n> As I mentioned to STORM-1933, basically sync-supervisor relies on zk assignment, and sync-processes relies on local assignment and local workers directory, but in fact sync-supervisor also access local state and take some actions which affects sync-processes. And also Satish left the comment to STORM-1933 describing other issue related to race condition and idea to fix this which is same page on me.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-06-29T14:21:29Z"
  },
  "patches": [],
  "external_id": "STORM-1934"
},{
  "_id": {
    "$oid": "5f27d0f1532b7277349c57a0"
  },
  "message_id": "<949996484.1223888812658.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d0f0532b7277349c576a"
  },
  "from_id": {
    "$oid": "59bfaa46f2a4565fe9098844"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (AMQ-1971) ConcurrentModificationException in\n high volume broker",
  "body": "\n    [ https://issues.apache.org/activemq/browse/AMQ-1971?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=46393#action_46393 ] \n\nJayson Minard commented on AMQ-1971:\n------------------------------------\n\nHey Rob, thanks for making a fix.  A quick question about the TopicStorePrefetch descendant class.  It instantiates its own batchList which then hides the ancestor one.  IT looks like you removed the dispatched() method which eliminates the previous issue I fixed and makes that part irrelevant.  But should the private batchList still be there in TopicStorePrefetch since it now appears to be unused?\n\n> ConcurrentModificationException in high volume broker \n> ------------------------------------------------------\n>\n>                 Key: AMQ-1971\n>                 URL: https://issues.apache.org/activemq/browse/AMQ-1971\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: Broker\n>    Affects Versions: 5.2.0\n>            Reporter: Jayson Minard\n>            Assignee: Rob Davies\n>             Fix For: 5.3.0\n>\n>         Attachments: amq1971.patch\n>\n>\n> This is occurring in the thousands as messages come through.    We have a network of 5 brokers although they do most of their work internally (publisher/consumer for queue are only on same broker) and do less work crossing to a central broker.\n> java.util.ConcurrentModificationException\n> \tat java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:373)\n> \tat java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:392)\n> \tat java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:391)\n> \tat org.apache.activemq.broker.region.cursors.AbstractStoreCursor.next(AbstractStoreCursor.java:136)\n> \tat org.apache.activemq.broker.region.cursors.StoreQueueCursor.next(StoreQueueCursor.java:140)\n> \tat org.apache.activemq.broker.region.Queue.doPageIn(Queue.java:1178)\n> \tat org.apache.activemq.broker.region.Queue.pageInMessages(Queue.java:1290)\n> \tat org.apache.activemq.broker.region.Queue.iterate(Queue.java:1004)\n> \tat org.apache.activemq.thread.DeterministicTaskRunner.runTask(DeterministicTaskRunner.java:84)\n> \tat org.apache.activemq.thread.DeterministicTaskRunner$1.run(DeterministicTaskRunner.java:41)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n> \tat java.lang.Thread.run(Thread.java:619)\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-10-13T02:06:52Z"
  },
  "patches": [],
  "external_id": "AMQ-1971"
},{
  "_id": {
    "$oid": "60fd84f39445ff90d5e59632"
  },
  "message_id": "<JIRA.13309224.1591200661000.175204.1591200720251@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fd84bf9445ff90d5e5927e"
  },
  "reference_ids": [
    {
      "$oid": "60fd84f39445ff90d5e59630"
    },
    {
      "$oid": "60fd84f39445ff90d5e59631"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fd84f39445ff90d5e59630"
  },
  "from_id": {
    "$oid": "60fa83e7f73e2aa390bb1e8e"
  },
  "to_ids": [
    {
      "$oid": "5f27d08eaf02e2d6de9300d7"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (OWB-1325) Provide a spy flavor of\n ClassDefiningService",
  "body": "Romain Manni-Bucau created OWB-1325:\n---------------------------------------\n\n             Summary: Provide a spy flavor of ClassDefiningService\n                 Key: OWB-1325\n                 URL: https://issues.apache.org/jira/browse/OWB-1325\n             Project: OpenWebBeans\n          Issue Type: Task\n            Reporter: Romain Manni-Bucau\n            Assignee: Romain Manni-Bucau\n             Fix For: 2.0.17\n\n\nGoal is to enable tools to capture proxies easily, will need some enhancements later so this feature must be considered experimental for now.\n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.4#803005)\n",
  "date": {
    "$date": "2020-06-03T16:12:00Z"
  },
  "patches": [],
  "external_id": "OWB-1325"
},{
  "_id": {
    "$oid": "5bdc01d916772b6055c7f9df"
  },
  "message_id": "<JIRA.12645471.1367344195000.22944.1430323808797@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bdbff4616772b6055c7c144"
  },
  "reference_ids": [
    {
      "$oid": "5bdc003c16772b6055c7d5b7"
    },
    {
      "$oid": "5bdc018f16772b6055c7f400"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bdc003c16772b6055c7d5b7"
  },
  "from_id": {
    "$oid": "5bbd8ccd57674ee167ce03e0"
  },
  "to_ids": [
    {
      "$oid": "5bdc002135e3ea2b7bb8a8be"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (KNOX-64) Knox API Definition within Topology File",
  "body": "\n     [ https://issues.apache.org/jira/browse/KNOX-64?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nKevin Minder updated KNOX-64:\n-----------------------------\n    Fix Version/s: 0.7.0\n\n> Knox API Definition within Topology File\n> ----------------------------------------\n>\n>                 Key: KNOX-64\n>                 URL: https://issues.apache.org/jira/browse/KNOX-64\n>             Project: Apache Knox\n>          Issue Type: Bug\n>          Components: Server\n>    Affects Versions: 0.2.0\n>            Reporter: Larry McCay\n>              Labels: API, security\n>             Fix For: 0.7.0\n>\n>   Original Estimate: 72h\n>  Remaining Estimate: 72h\n>\n> Currently we have no way to create and publish APIs or Flows as we need for the Gatekeeper functionality.\n> I would like to see something like an <api> definition that includes the list providers that make up the API flow.\n> It would then be published to the deploy server as gateway/api-name.\n> We may also need a way to replace \"gateway\" in the endpoint with something else like \"gatekeeper\" or some arbitrary name.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-04-29T16:10:08Z"
  },
  "patches": [],
  "external_id": "KNOX-64"
},{
  "_id": {
    "$oid": "5bbe114db1ffc5570d042567"
  },
  "message_id": "<1659324260.20363.1304471223231.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bbe0e4eb1ffc5570d03b7b4"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbe114db1ffc5570d042564"
  },
  "from_id": {
    "$oid": "59bfb4a1f2a4565fe9227223"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Resolved] (TIKA-619) Error parsing GIF",
  "body": "\n     [ https://issues.apache.org/jira/browse/TIKA-619?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nNick Burch resolved TIKA-619.\n-----------------------------\n\n       Resolution: Fixed\n    Fix Version/s: 1.0\n\nPatch applied in r1099309.\n\n> Error parsing GIF\n> -----------------\n>\n>                 Key: TIKA-619\n>                 URL: https://issues.apache.org/jira/browse/TIKA-619\n>             Project: Tika\n>          Issue Type: Bug\n>          Components: parser\n>    Affects Versions: 1.0\n>            Reporter: Erik Hetzner\n>            Priority: Minor\n>             Fix For: 1.0\n>\n>         Attachments: TIKA-619.patch, icon_gadget_tools_dark.gif\n>\n>\n> I am getting an exception parsing the following GIF file, which opens OK in firefox, etc. Although since this is a problem in Sun/Oracle code, I would not expect it to be fixed anytime soon!\n> {noformat}\n> $ java -jar tika-app/target/tika-app-1.0-SNAPSHOT.jar http://sites.google.com/site/keepaesopen/_/rsrc/1271597999559/system/app/images/icon_gadget_tools_dark.gif\n> Exception in thread \"main\" org.apache.tika.exception.TikaException: image/gif parse error\n>         at org.apache.tika.parser.image.ImageParser.parse(ImageParser.java:91)\n>         at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)\n>         at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)\n>         at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)\n>         at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:107)\n>         at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:302)\n>         at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:91)\n> Caused by: javax.imageio.IIOException: Unexpected block type 0!\n>         at com.sun.imageio.plugins.gif.GIFImageReader.readMetadata(GIFImageReader.java:722)\n>         at com.sun.imageio.plugins.gif.GIFImageReader.getWidth(GIFImageReader.java:167)\n>         at org.apache.tika.parser.image.ImageParser.parse(ImageParser.java:75)\n>         ... 6 more\n> {noformat}\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-05-04T01:07:03Z"
  },
  "patches": [],
  "external_id": "TIKA-619"
},{
  "_id": {
    "$oid": "5bbdf7aee8113566f6650305"
  },
  "message_id": "<16594994.1148150910125.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bbdf7a2e8113566f66500b9"
  },
  "from_id": {
    "$oid": "5bbdf79057674ee1678b2435"
  },
  "to_ids": [
    {
      "$oid": "5bbdf69457674ee16789fd6d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Updated: (NUTCH-173) PerHost Crawling Policy (\n crawl.ignore.external.links )",
  "body": "     [ http://issues.apache.org/jira/browse/NUTCH-173?page=all ]\n\nStefan Neufeind updated NUTCH-173:\n----------------------------------\n\n    Attachment: patch08-new.patch\n\nHere is the 08-patch, corrected to work against nightly from 2006-05-20.\nAlso fromHost is now only generated if really needed and nutch-default.xml is patched as well. By the way: Where should a property for \"crawl\" be located in the config-file? In the \"fetcher\"-section? In that case please somebody move it up/down or rename the property before including it in the dev-tree.\n\nBut could somebody please review it quickly? I'm not sure it's 100% correct. Still investigating on my side ...\n\n> PerHost Crawling Policy ( crawl.ignore.external.links )\n> -------------------------------------------------------\n>\n>          Key: NUTCH-173\n>          URL: http://issues.apache.org/jira/browse/NUTCH-173\n>      Project: Nutch\n>         Type: New Feature\n\n>   Components: fetcher\n>     Versions: 0.7.1, 0.7, 0.8-dev\n>     Reporter: Philippe EUGENE\n>     Priority: Minor\n>  Attachments: patch.txt, patch08-new.patch, patch08.txt\n>\n> There is two major way of crawl in Nutch.\n> Intranet Crawl : forbidden all, allow somes few host\n> Whole-web crawl : allow all, forbidden few thinks\n> I propose a third type of crawl.\n> Directory Crawl : The purpose of this crawl is to manage few thousands of host wihtout managing rules pattern in UrlFilterRegexp.\n> I made two patch for : 0.7, 0.7.1 and 0.8-dev\n> I propose a new boolean property in nutch-site.xml : crawl.ignore.external.links, with false value at default.\n> By default this new feature don't modify the behavior of nutch crawler.\n> When you setup this property to true, the crawler don't fetch external links of the host.\n> So the crawl is limited to the host that you inject at the beginning at the crawl.\n> I know there is some proposal of new crawl policy using the CrawlDatum in 0.8-dev branch. \n> This feature colud be a easiest way to add quickly new crawl feature to nutch, waiting for a best way to improve crawl policy.\n> I post two patch.\n> Sorry for my very poor english \n> --\n> Philippe\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "date": {
    "$date": "2006-05-20T18:48:30Z"
  },
  "patches": [],
  "external_id": "NUTCH-173"
},{
  "_id": {
    "$oid": "5bbf0968b79d666cbb221e43"
  },
  "message_id": "<JIRA.13047100.1488317172000.23592.1488321825635@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf08d5b79d666cbb22045d"
    },
    {
      "$oid": "5bbf08d5b79d666cbb22045e"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf08d5b79d666cbb22045d"
  },
  "from_id": {
    "$oid": "5bbf070b57674ee167302376"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-4819) Expose states of active tasks to\n public API",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-4819?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15889006#comment-15889006 ] \n\nMatthias J. Sax commented on KAFKA-4819:\n----------------------------------------\n\n[~fhussonnois] Thanks for the JIRA an PR. All public API changes require a KIP: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals\n\n> Expose states of active tasks to public API\n> -------------------------------------------\n>\n>                 Key: KAFKA-4819\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-4819\n>             Project: Kafka\n>          Issue Type: New Feature\n>          Components: streams\n>    Affects Versions: 0.10.2.0\n>            Reporter: Florian Hussonnois\n>            Priority: Minor\n>              Labels: needs-kip\n>\n> In Kafka 0.10.1.0 the toString method of KafkaStreams class has been implemented mainly to ease topologies debugging. Also,  the streams Metrics has been exposed to public API.\n> But currently theres is no way to monitor kstreams tasks states, assignments or consumed offsets.\n> I propose to expose the states of active tasks to the public API KafkaStreams.\n> For instance, an application can expose a REST API to get the global state of a kstreams topology.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-28T22:43:45Z"
  },
  "patches": [],
  "external_id": "KAFKA-4819"
},{
  "_id": {
    "$oid": "5f27bd15a9368823397db1c1"
  },
  "message_id": "<985636333.60082.1306944527679.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27bb47a9368823397d2ab3"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "5f27bce2af02e2d6de5e0382"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "body": "Node.ANY is not detected as a variable\n--------------------------------------\n\n                 Key: JENA-70\n                 URL: https://issues.apache.org/jira/browse/JENA-70\n             Project: Jena\n          Issue Type: Improvement\n          Components: Jena\n            Reporter: Laurent Pellegrino\n\n\nIn the javadoc it is said \"A Node_ANY (there should be only one) is a meta-node that is used to stand *for any* other node in a query\". Hence, I was expecting that any call to isVariable, isLiteral, isURI, etc. return true.\n\nIam raising this issue because in OutputLangUtils it is not possible to output (serialize) a Node_ANY. Hence, I think there is two solutions to support it. Either we can add a small code to handle the issue in the output method from OutputLangUtils or we can correct the Node_ANY class to support what is stated by the javadoc (by overriding is* and getName, getURI, methods). \n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-06-01T16:08:47Z"
  },
  "from_id": {
    "$oid": "5f27bce1af02e2d6de5e02a4"
  },
  "subject": "[jira] [Created] (JENA-70) Node.ANY is not detected as a variable",
  "external_id": "JENA-70"
},{
  "_id": {
    "$oid": "5f27ce3b442ab9b9860f18fd"
  },
  "message_id": "<JIRA.12945914.1456859163000.160168.1469728580471@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ce1f442ab9b9860f0fec"
    },
    {
      "$oid": "5f27cd70442ab9b9860eddde"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cd70442ab9b9860eddde"
  },
  "from_id": {
    "$oid": "5bacb0ba57674ee167d4aa38"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (OOZIE-2473) Connection pool for SMTP connection",
  "body": "\n     [ https://issues.apache.org/jira/browse/OOZIE-2473?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nSatish Subhashrao Saley updated OOZIE-2473:\n-------------------------------------------\n    Attachment:     (was: OOZIE-2473-2.patch)\n\n> Connection pool for SMTP connection\n> -----------------------------------\n>\n>                 Key: OOZIE-2473\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-2473\n>             Project: Oozie\n>          Issue Type: Bug\n>            Reporter: Satish Subhashrao Saley\n>            Assignee: Satish Subhashrao Saley\n>         Attachments: OOZIE-2473-1.patch, OOZIE-2473-1.patch\n>\n>\n> Currently, to send an email we setup new connection every time which seems costly. It would be good to have a connection pool.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-07-28T17:56:20Z"
  },
  "patches": [],
  "external_id": "OOZIE-2473"
},{
  "_id": {
    "$oid": "5bbe12d008955e10d507d950"
  },
  "message_id": "<18253642.1180964378602.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "5bbe124408955e10d507d458"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "58f6329a02ca40f8bf01b128"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "date": {
    "$date": "2007-06-04T06:39:38Z"
  },
  "body": "add more details to the error message when a signature verification failes\n--------------------------------------------------------------------------\n\n                 Key: WSS-78\n                 URL: https://issues.apache.org/jira/browse/WSS-78\n             Project: WSS4J\n          Issue Type: Improvement\n            Reporter: Armin Häberling\n            Assignee: Davanum Srinivas\n\n\nIn org.apache.ws.security.processor.SignatureProcessor.verifyXMLSignature() a WSSecurityException with the message \"The signature verification failed\" is thrown when the signature verification fails.\n\nThe error message should be enhanced with more specific info about the error, for example that one of the referenced digests didn't match.\nSee also http://marc.info/?l=xml-security-dev&m=112557776332544&w=2\n\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: wss4j-dev-unsubscribe@ws.apache.org\nFor additional commands, e-mail: wss4j-dev-help@ws.apache.org\n\n",
  "subject": "[jira] Created: (WSS-78) add more details to the error message when\n a signature verification failes",
  "from_id": {
    "$oid": "58c11e2502ca40f8bfb1f714"
  },
  "external_id": "WSS-78"
},{
  "_id": {
    "$oid": "5bbe0ea2272f7b1f6830d4a5"
  },
  "message_id": "<JIRA.13043341.1487185109000.90056.1487185122218@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbe0e12272f7b1f6830bd59"
  },
  "reference_ids": [
    {
      "$oid": "5bbe0ea2272f7b1f6830d4a3"
    },
    {
      "$oid": "5bbe0ea2272f7b1f6830d4a4"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbe0ea2272f7b1f6830d4a3"
  },
  "from_id": {
    "$oid": "5bbe0e2c57674ee167917b1d"
  },
  "to_ids": [
    {
      "$oid": "5bbe0e2757674ee1679173c5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (PARQUET-881) C++: Update Arrow hash to 0.2.0-rc2",
  "body": "Uwe L. Korn created PARQUET-881:\n-----------------------------------\n\n             Summary: C++: Update Arrow hash to 0.2.0-rc2\n                 Key: PARQUET-881\n                 URL: https://issues.apache.org/jira/browse/PARQUET-881\n             Project: Parquet\n          Issue Type: Task\n          Components: parquet-cpp\n            Reporter: Uwe L. Korn\n            Assignee: Uwe L. Korn\n\n\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.15#6346)\n",
  "date": {
    "$date": "2017-02-15T18:58:42Z"
  },
  "patches": [],
  "external_id": "PARQUET-881"
},{
  "_id": {
    "$oid": "5f27d33046816ce7cf506b5c"
  },
  "message_id": "<19243916.1120595834688.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d33046816ce7cf506b5b"
  },
  "from_id": {
    "$oid": "5f27d24baf02e2d6de9c22be"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Commented: (MPJDIFF-6) Support SVN, not just CVS for retrieving base API definition",
  "body": "    [ http://jira.codehaus.org/browse/MPJDIFF-6?page=comments#action_42419 ] \n\nPhil Steitz commented on MPJDIFF-6:\n-----------------------------------\n\nTo make this work correctly for svn, I think the maven.jdiff.old.tag and maven.jdiff.new.tag properties will either have to be interpreted as URLs or new properties added.  Probably the second is better, so that what gets checked out is, e.g.\n${maven.jdiff.old.uri}/${maven.jdiff.old.tag}.   It would be too limiting to assume that the old and new are based on the same root URI.\n\nI have started playing with a patch for this, but am not sure which way to go.   The simplest way to just get svn support in would seem to do as above.  Where is the ant svn task documented?  I was using ant exec.\n\nI am thinking though that it might be better to use the scm plugin checkout goal instead, as this would bring along other supported scm systems as well.  \n\n\n> Support SVN, not just CVS for retrieving base API definition\n> ------------------------------------------------------------\n>\n>          Key: MPJDIFF-6\n>          URL: http://jira.codehaus.org/browse/MPJDIFF-6\n>      Project: maven-jdiff-plugin\n>         Type: Improvement\n>     Versions: 1.4\n>  Environment: Maven 1.0, WinXP, J2SE 1.4.2\n>     Reporter: John Fallows\n\n>\n>\n> In addition to CVS, please also support Subversion http://subversion.tigris.org to retrieve the base API definition from version control.\n> Ant tasks for Subversion are available at http://subclipse.tigris.org/svnant.html.\n> Those of us using Maven and Subversion together cannot current use the maven-jdiff-plugin due to its currently hardcoded dependency on CVS.\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2005-07-05T15:37:14Z"
  },
  "patches": [],
  "external_id": "MPJDIFF-6"
},{
  "_id": {
    "$oid": "58bfd15615d83644fcc50e9f"
  },
  "message_id": "<820495053.32248.1354105738494.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "58bfd14515d83644fcc50b28"
  },
  "from_id": {
    "$oid": "58bfd14502ca40f8bf1483af"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-1578)\n org.apache.zookeeper.server.quorum.Zab1_0Test failed due to hard code with\n 33556 port",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-1578?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13505405#comment-13505405 ] \n\nLi Ping Zhang commented on ZOOKEEPER-1578:\n------------------------------------------\n\nThis 1 UT failure org.apache.zookeeper.server.quorum.Zab1_0Test is not only existed in Open JDK, but also exsited in SUN JDK.\n\nThis is due to this test uses a lot of hard-wired port numbers. Most or all other tests automatically assign a free port.\n\nThe resolvement is to modify org.apache.zookeeper.server.quorum.Zab1_0Test, specifying a different port instead of the 33556 or just change it to PortAssignment.unique() and add    \"import org.apache.zookeeper.PortAssignment;\". \n                \n> org.apache.zookeeper.server.quorum.Zab1_0Test failed due to hard code with 33556 port\n> -------------------------------------------------------------------------------------\n>\n>                 Key: ZOOKEEPER-1578\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-1578\n>             Project: ZooKeeper\n>          Issue Type: Bug\n>    Affects Versions: 3.4.3\n>            Reporter: Li Ping Zhang\n>              Labels: patch\n>   Original Estimate: 24h\n>  Remaining Estimate: 24h\n>\n> org.apache.zookeeper.server.quorum.Zab1_0Test was failed both with SUN JDK and open JDK.\n>     [junit] Running org.apache.zookeeper.server.quorum.Zab1_0Test\n>     [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 18.334 sec\n>     [junit] Test org.apache.zookeeper.server.quorum.Zab1_0Test FAILED \n> Zab1_0Test log:\n> Zab1_0Test log:\n> 2012-07-11 23:17:15,579 [myid:] - INFO  [main:Leader@427] - Shutdown called\n> java.lang.Exception: shutdown Leader! reason: end of test\n>         at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:427)\n>         at org.apache.zookeeper.server.quorum.Zab1_0Test.testLastAcceptedEpoch(Zab1_0Test.java:211)\n>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n>         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48)\n> 2012-07-11 23:17:15,584 [myid:] - ERROR [main:Leader@139] - Couldn't bind to port 33556\n> java.net.BindException: Address already in use\n>         at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:402)\n>         at java.net.ServerSocket.bind(ServerSocket.java:328)\n>         at java.net.ServerSocket.bind(ServerSocket.java:286)\n>         at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:137)\n>         at org.apache.zookeeper.server.quorum.Zab1_0Test.createLeader(Zab1_0Test.java:810)\n>         at org.apache.zookeeper.server.quorum.Zab1_0Test.testLeaderInElectingFollowers(Zab1_0Test.java:224)\n>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n> 2012-07-11 23:17:20,202 [myid:] - ERROR [LearnerHandler-bdvm039.svl.ibm.com/9.30.122.48:40153:LearnerHandler@559] - Unex\n> pected exception causing shutdown while sock still open\n> java.net.SocketTimeoutException: Read timed out\n>         at java.net.SocketInputStream.read(SocketInputStream.java:129)\n>         at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n>         at java.io.BufferedInputStream.read(BufferedInputStream.java:237)\n>         at java.io.DataInputStream.readInt(DataInputStream.java:370)\n>         at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)\n>         at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)\n>         at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)\n>         at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:291)\n> 2012-07-11 23:17:20,203 [myid:] - WARN  [LearnerHandler-bdvm039.svl.ibm.com/9.30.122.48:40153:LearnerHandler@569] - ****\n> *** GOODBYE bdvm039.svl.ibm.com/9.30.122.48:40153 ********\n> 2012-07-11 23:17:20,204 [myid:] - INFO  [Thread-20:Leader@421] - Shutting down\n> 2012-07-11 23:17:20,204 [myid:] - INFO  [Thread-20:Leader@427] - Shutdown called\n> java.lang.Exception: shutdown Leader! reason: lead ended\n> this failure seems 33556 port is already used, but it is not in use with command check in fact. There is a hard code in unit test, we can improve it with code patch.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-11-28T12:28:58Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-1578"
},{
  "_id": {
    "$oid": "5f27cd9d014d3531c6cc2a93"
  },
  "message_id": "<466389955.43137.1309167407585.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27cd81014d3531c6cc2251"
  },
  "from_id": {
    "$oid": "59bf9349f2a4565fe9e90df3"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JCR-3005) Make it possible to get multiple\n nodes in one call via davex",
  "body": "\n    [ https://issues.apache.org/jira/browse/JCR-3005?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13055424#comment-13055424 ] \n\nLukas Kahwe Smith commented on JCR-3005:\n----------------------------------------\n\n* I think it would be a nice touch and probably fairly cheap to add some metadata to the result, \"total_nodes\" would be one obvious one. not sure if it makes sense to add other metadata like duplicates, missing or stuff like that, at any rate it would be good to prepare the structure returned for this so that we can add anything else that makes sense later without BC breaks\n* I think it should only be once, actually I think the nodes should be a hash map with path: node pairs\n* Missing nodes should just be ignored (see also http://java.net/jira/browse/JSR_333-38)\n* If any nodes can be found, imho they should be returned, errors should if at all be returned as part of the metadata (see above)\n* Like I said, I would prefer a hash map including the path to identify the node in the nodes list\n\n> Make it possible to get multiple nodes in one call via davex\n> ------------------------------------------------------------\n>\n>                 Key: JCR-3005\n>                 URL: https://issues.apache.org/jira/browse/JCR-3005\n>             Project: Jackrabbit Content Repository\n>          Issue Type: New Feature\n>          Components: jackrabbit-jcr-server\n>            Reporter: Christian Stocker\n>         Attachments: patch_commit_d0596cade79b.patch\n>\n>\n> I'm working on this currently\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-06-27T09:36:47Z"
  },
  "patches": [],
  "external_id": "JCR-3005"
},{
  "_id": {
    "$oid": "5f27d01f532b7277349c2012"
  },
  "message_id": "<964753654.7358.1308156827446.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "5f27d01faf02e2d6de90be98"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (AMQ-3368) Patch to disable all endpoints",
  "body": "Patch to disable all endpoints \n-------------------------------\n\n                 Key: AMQ-3368\n                 URL: https://issues.apache.org/jira/browse/AMQ-3368\n             Project: ActiveMQ\n          Issue Type: Improvement\n          Components: JCA Container\n         Environment: n/a\n            Reporter: Jeremy Levy\n            Priority: Minor\n         Attachments: ActiveMQResourceAdapter_Disable_All_Endpoints.txt\n\nFor packaging scenarios where MDB endpoints are deployed along with EJB's and WARs in an EAR but you don't want the queues to be activated on some servers this patch provides a system property that allows you to disabled endpoint activation.\n\nSetting the system property 'activemq.endpoints.disabled' to true will stop queues from activating.   For example: \n\njava -Dactivemq.endpoints.disabled=true -jar myjar.jar \n\nThis is useful if you want to keep your deployment archive the same across multiple instances but reserve some servers for queue processing.  The exclusive consumer functionality isn't sufficient incase you want to have multiple MDBs active.\n\nSee email thread titled \"Patch to disable all Queues\" on users@activemq.apache.org from 6/1/2011 for more details.\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-06-15T16:53:47Z"
  },
  "patches": [],
  "external_id": "AMQ-3368"
},{
  "_id": {
    "$oid": "5f27cd32014d3531c6cc0b7f"
  },
  "message_id": "<JIRA.12693445.1391636189784.47853.1391680331770@arcas>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [
    {
      "$oid": "5f27cd31014d3531c6cc0b7a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cd31014d3531c6cc0b7a"
  },
  "from_id": {
    "$oid": "59bf92f3f2a4565fe9e6dbb0"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (JCR-3723) Add support for observation\n statistics to RepositoryStatistics",
  "body": "\n    [ https://issues.apache.org/jira/browse/JCR-3723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13893218#comment-13893218 ] \n\nAlex Parvulescu commented on JCR-3723:\n--------------------------------------\n\nbq. QUERY_AVERAGE(true)\nI can't remember exactly, I think it was because the query run times can go pretty high and some outlier could blowup the entire stats sequence.\n\nBut looking at the code, it seems that the averages are not affected by this flag at all, they just compute dynamic values out of 2 stats [0], whereas the TimeSeriesRecorder handles the resets on the normal (count & duration) stats.\n\n\n\n[0] http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-jcr-commons/src/main/java/org/apache/jackrabbit/stats/TimeSeriesAverage.java\n[1] http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-jcr-commons/src/main/java/org/apache/jackrabbit/stats/TimeSeriesRecorder.java\n\n> Add support for observation statistics to RepositoryStatistics\n> --------------------------------------------------------------\n>\n>                 Key: JCR-3723\n>                 URL: https://issues.apache.org/jira/browse/JCR-3723\n>             Project: Jackrabbit Content Repository\n>          Issue Type: Improvement\n>          Components: jackrabbit-core\n>            Reporter: Michael Dürig\n>            Assignee: Michael Dürig\n>         Attachments: JCR-3723.patch\n>\n>\n> I suggest to add support for observation statistics to RepositoryStatistics for tracking the total number of observation events and the time spent handling observation events as TimeSeries.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-06T09:52:11Z"
  },
  "patches": [],
  "external_id": "JCR-3723"
},{
  "_id": {
    "$oid": "5bea989d9e73d744d411cb48"
  },
  "message_id": "<JIRA.12658736.1374233842679.99809.1384886600319@arcas>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [
    {
      "$oid": "5bea989d9e73d744d411cb47"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bea989d9e73d744d411cb47"
  },
  "from_id": {
    "$oid": "5bea981135e3ea2b7b4e616e"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (DERBY-6298) Documentation implies column NOT NULL\n constraint cannot be named, but it can be.",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-6298?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nDag H. Wanvik closed DERBY-6298.\n--------------------------------\n\n\n> Documentation implies column NOT NULL constraint cannot be named, but it can be.\n> --------------------------------------------------------------------------------\n>\n>                 Key: DERBY-6298\n>                 URL: https://issues.apache.org/jira/browse/DERBY-6298\n>             Project: Derby\n>          Issue Type: Bug\n>          Components: Documentation\n>    Affects Versions: 10.10.1.1\n>            Reporter: Dag H. Wanvik\n>            Assignee: Kim Haase\n>             Fix For: 10.8.3.1, 10.9.2.2, 10.10.1.3, 10.11.0.0\n>\n>         Attachments: DERBY-6298-2.diff, DERBY-6298.diff, rrefsqlj16095.html, rrefsqlj16095.html\n>\n>\n> Cf syntax description in ref/rrefsqlj16095.html:\n> Column-level-constraint\n> {\n>     NOT NULL |\n>     [ [CONSTRAINT constraint-Name]\n>     {\n>         CHECK (searchCondition) |\n>         {\n>             PRIMARY KEY |\n>             UNIQUE |\n>             REFERENCES clause\n>         } \n>     }\n> }\n> but actually this works:\n> create table t(i int constraint foo not null); The curly braces around primary, unique and references are redundant as well.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1#6144)\n",
  "date": {
    "$date": "2013-11-19T18:43:20Z"
  },
  "patches": [],
  "external_id": "DERBY-6298"
},{
  "_id": {
    "$oid": "5bbf2c0130623e2888ade45f"
  },
  "message_id": "<JIRA.12929487.1452604398000.275803.1454415159903@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf259230623e2888adbbb0"
  },
  "reference_ids": [
    {
      "$oid": "5bbf2bff30623e2888ade41c"
    },
    {
      "$oid": "5bbf2bff30623e2888ade41b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf2bff30623e2888ade41b"
  },
  "from_id": {
    "$oid": "5bbdabda57674ee167d72cd5"
  },
  "to_ids": [
    {
      "$oid": "5bbf261257674ee1674f913a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (LENS-920) Error marshalling LensAPIResult to json",
  "body": "\n     [ https://issues.apache.org/jira/browse/LENS-920?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAmareshwari Sriramadasu updated LENS-920:\n-----------------------------------------\n    Status: Patch Available  (was: In Progress)\n\n> Error marshalling LensAPIResult to json\n> ---------------------------------------\n>\n>                 Key: LENS-920\n>                 URL: https://issues.apache.org/jira/browse/LENS-920\n>             Project: Apache Lens\n>          Issue Type: Bug\n>          Components: api, server\n>            Reporter: Amruth S\n>            Assignee: Amareshwari Sriramadasu\n>             Fix For: 2.5\n>\n>         Attachments: LENS-920.6.patch\n>\n>\n> Rest apis should be able to work with both application/xml and application/json  accept headers.\n> But some APIs that return LensAPIResult<?> does not work with application/json accept header.\n> To reproduce\n> 1) Hit the /lensapi/queryapi/queries with one more additional header \n>    Accept - application/json\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-02-02T12:12:39Z"
  },
  "patches": [],
  "external_id": "LENS-920"
},{
  "_id": {
    "$oid": "5bbdf2b6e8113566f6647602"
  },
  "message_id": "<JIRA.13171354.1531301379000.21086.1531301400259@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbdf24fe8113566f664747a"
  },
  "reference_ids": [
    {
      "$oid": "5bbdf2b6e8113566f6647601"
    },
    {
      "$oid": "5bbdf2b5e8113566f6647600"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbdf2b5e8113566f6647600"
  },
  "from_id": {
    "$oid": "5bbdf2ad57674ee16779d239"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd1e4f89451f55cdfd2"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (NUTCH-2618) protocol-okhttp not to use\n http.timeout for max duration to fetch document",
  "body": "Sebastian Nagel created NUTCH-2618:\n--------------------------------------\n\n             Summary: protocol-okhttp not to use http.timeout for max duration to fetch document\n                 Key: NUTCH-2618\n                 URL: https://issues.apache.org/jira/browse/NUTCH-2618\n             Project: Nutch\n          Issue Type: Bug\n          Components: protocol\n    Affects Versions: 1.15\n            Reporter: Sebastian Nagel\n             Fix For: 1.15\n\n\nProtocol-okhttp (NUTCH-2576) uses the HTTP network timeout ({{http.timeout}}) as time limit for the max duration to fetch a document. The timeout value (default = 10 sec.) is usually to small to fetch larger documents. The max fetch duration should be separately configurable, e.g., by a property {{http.time.limit}} (similar to {{http.content.limit}}).\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-07-11T09:30:00Z"
  },
  "patches": [],
  "external_id": "NUTCH-2618"
},{
  "_id": {
    "$oid": "58bfd08b15d83644fcc4e3c4"
  },
  "message_id": "<JIRA.12721148.1402678631976.130908.1402702022145@arcas>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfcf8915d83644fcc4ae74"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfcf8915d83644fcc4ae74"
  },
  "from_id": {
    "$oid": "58bfd08b02ca40f8bf14825f"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-1940) Integrate with Docker.",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-1940?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14031328#comment-14031328 ] \n\nDavid Medinets commented on ZOOKEEPER-1940:\n-------------------------------------------\n\n~rtice - I looked at the signalfuse project but was unable to make it work. I haven't studied Python and that project did not provide batch files to simplify startup. Naively building and running their image resulted in the following message. The maestro-ng approach might be better than using pipework, but I wasn't able to use the project with my current knowledge.\n\n{noformat}\n$ docker run -i -t docker/zookeeper:3.4.5\nTraceback (most recent call last):\n  File \"/opt/zookeeper-3.4.5/.docker/run.py\", line 25, in <module>\n    ZOOKEEPER_NODE_LIST = get_node_list(get_service_name(),\n  File \"/usr/local/lib/python2.7/dist-packages/maestro/guestutils.py\", line 27, in get_service_name\n    raise MaestroEnvironmentError('Service name was not defined')\nmaestro.guestutils.MaestroEnvironmentError: Service name was not defined\n{noformat}\n\n> Integrate with Docker.\n> ----------------------\n>\n>                 Key: ZOOKEEPER-1940\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-1940\n>             Project: ZooKeeper\n>          Issue Type: Wish\n>            Reporter: David Medinets\n>            Priority: Trivial\n>\n> Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications.\n> It's become quite popular and I'd like to see the zookeeper community suggest a standard way to run zookeeper inside docker containers. To get the conversation started, I have a working example at:\n> https://github.com/medined/docker-zookeeper\n> I hope there is a better technique that I used. And if there is please make suggestions.\n> The difficulty, I think, posed by Docker, is that the images are started before the bridge network is created. This means, again I think, that zookeeper is running inside the container with no way to communicate with the ensemble for some non-trivial amount of time. \n> My resolution to this was to force each each to wait 30 seconds before  starting zookeeper. I still see connection errors in the logs, but eventually the cluster settles and everything seems to work.\n> I'm hoping that someone which more networking experience than I can find a way to eliminate that 30 second delay and the connection errors during startup.\n> Thanks for reading this far.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-06-13T23:27:02Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-1940"
},{
  "_id": {
    "$oid": "5bc85e486e373d4fe81c6b32"
  },
  "message_id": "<1297220666.10870.1341521794837.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc85e486e373d4fe81c6b31"
  },
  "from_id": {
    "$oid": "5bacb30257674ee167d8fa53"
  },
  "to_ids": [
    {
      "$oid": "5bc85ac857674ee167cc9912"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (FLUME-1316) AvroSink should be configurable for\n connect-timeout and request-timeout",
  "body": "\n     [ https://issues.apache.org/jira/browse/FLUME-1316?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMike Percy updated FLUME-1316:\n------------------------------\n\n    Attachment: FLUME-1316-4.patch\n    \n> AvroSink should be configurable for connect-timeout and request-timeout\n> -----------------------------------------------------------------------\n>\n>                 Key: FLUME-1316\n>                 URL: https://issues.apache.org/jira/browse/FLUME-1316\n>             Project: Flume\n>          Issue Type: Bug\n>          Components: Sinks+Sources\n>    Affects Versions: v1.2.0\n>            Reporter: Will McQueen\n>            Assignee: Mike Percy\n>             Fix For: v1.3.0\n>\n>         Attachments: FLUME-1316-4.patch\n>\n>\n> The Flume SDK provides connect-timeout and request-timeout props, but similar props are not currently exposed in AvroSink.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-07-05T20:56:34Z"
  },
  "patches": [],
  "external_id": "FLUME-1316"
},{
  "_id": {
    "$oid": "58bfcf4f15d83644fcc4a1ec"
  },
  "message_id": "<JIRA.12995391.1470441911000.320512.1471370902188@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "58bfcd7815d83644fcc47fed"
  },
  "reference_ids": [
    {
      "$oid": "58bfcf4615d83644fcc4a009"
    },
    {
      "$oid": "58bfcf4615d83644fcc4a008"
    }
  ],
  "in_reply_to_id": {
    "$oid": "58bfcf4615d83644fcc4a008"
  },
  "from_id": {
    "$oid": "58bfcf2002ca40f8bf147fe4"
  },
  "to_ids": [
    {
      "$oid": "58bfcbbbe4f89451f55cdf88"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (ZOOKEEPER-2505) Use shared library instead of\n static library in C client unit test",
  "body": "\n    [ https://issues.apache.org/jira/browse/ZOOKEEPER-2505?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15423171#comment-15423171 ] \n\nChris Nauroth commented on ZOOKEEPER-2505:\n------------------------------------------\n\n[~hanm], thank you for working on this.  I would love to get a complete build working on OS X too.\n\nHave you tried this patch on a Solaris VM too?  Last time I checked, there were some finicky differences in linker flags on Solaris.\n\n> Use shared library instead of static library in C client unit test\n> ------------------------------------------------------------------\n>\n>                 Key: ZOOKEEPER-2505\n>                 URL: https://issues.apache.org/jira/browse/ZOOKEEPER-2505\n>             Project: ZooKeeper\n>          Issue Type: Improvement\n>          Components: c client\n>    Affects Versions: 3.5.2\n>            Reporter: Michael Han\n>            Assignee: Michael Han\n>            Priority: Minor\n>             Fix For: 3.5.3\n>\n>         Attachments: ZOOKEEPER-2505.patch\n>\n>\n> Currently we are statically linking c unit tests to ZK client library - we should use shared library instead as there seems no particular reason to stick to static library, plus one benefit of using shared library is that would allow us to use overrides function calls from standard libraries at link time so we can simulate the wrap option for ld linker on os x. \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-08-16T18:08:22Z"
  },
  "patches": [],
  "external_id": "ZOOKEEPER-2505"
},{
  "_id": {
    "$oid": "5bbd8d175f06ec75d575f4fd"
  },
  "message_id": "<JIRA.13031371.1483176926000.620821.1483176958338@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbd8ca05f06ec75d575d7bb"
  },
  "reference_ids": [
    {
      "$oid": "5bbd8d175f06ec75d575f4fb"
    },
    {
      "$oid": "5bbd8d175f06ec75d575f4fc"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbd8d175f06ec75d575f4fb"
  },
  "from_id": {
    "$oid": "5bbd8d1757674ee167ce9799"
  },
  "to_ids": [
    {
      "$oid": "5bbd8cb057674ee167cdc69a"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (CALCITE-1558) Wrong field mapping of top level\n aggregate for cases when groupKey is used in aggregate function",
  "body": "Zhenghua Gao created CALCITE-1558:\n-------------------------------------\n\n             Summary: Wrong field mapping of top level aggregate for cases when groupKey is used in aggregate function\n                 Key: CALCITE-1558\n                 URL: https://issues.apache.org/jira/browse/CALCITE-1558\n             Project: Calcite\n          Issue Type: Bug\n            Reporter: Zhenghua Gao\n            Assignee: Julian Hyde\n\n\nIn AggregateExpandDistinctAggregatesRule.convertSingletonDistinct,\nif the groupKey is also used in some aggregate function, \nthe field mappting of top level aggregate would be messy.\nBad cases are:\n\n        // Equivalent SQL:\n        //   SELECT deptno, COUNT(deptno), SUM(DISTINCT sal)\n        //   FROM emp\n        //   GROUP BY deptno\n\n        //   SELECT deptno, SUM(cnt), SUM(sal)\n        //   FROM\n        //     SELECT deptno, COUNT(deptno) AS cnt, sal\n        //     FROM emp\n        //     GROUP BY deptno, sal\n        //   GROUP BY deptno\n\nor a more complex case:\n\n        // Equivalent SQL:\n        //   SELECT deptno, SUM(deptno), SUM(DISTINCT sal), MAX(deptno), MAX(comm)\n        //   FROM emp\n        //   GROUP BY deptno\n\n        //   SELECT deptno, SUM(sumOfInnerComm), SUM(sal), MAX(maxOfInnerDeptno), MAX(maxOfInnerComm)\n        //   FROM\n        //     SELECT deptno, sal, SUM(deptno) as sumOfInnerDeptno, MAX(deptno) as maxOfInnerDeptno, MAX(comm) AS maxOfInnerComm\n        //     FROM emp\n        //     GROUP BY deptno, sal\n        //   GROUP BY deptno\n\nI have fixed these cases, and will provide a patch later after more tests.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-12-31T09:35:58Z"
  },
  "patches": [],
  "external_id": "CALCITE-1558"
},{
  "_id": {
    "$oid": "5bea97c99e73d744d411a135"
  },
  "message_id": "<JIRA.12675322.1382557946000.178841.1469932687508@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [
    {
      "$oid": "5bea97c99e73d744d411a117"
    },
    {
      "$oid": "5bea97c99e73d744d411a118"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bea97c99e73d744d411a117"
  },
  "from_id": {
    "$oid": "5bea97bf35e3ea2b7b4da4ee"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DERBY-6391) remove unneeded object creation in\n newException() calls in releases > 10.10",
  "body": "\n    [ https://issues.apache.org/jira/browse/DERBY-6391?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15400921#comment-15400921 ] \n\nDanoja Dias commented on DERBY-6391:\n------------------------------------\n\nYes. But would it arise problems if we commit the attached patch? \n\nIf not, There will be more new String() object creations in other folders too.\n\nShouldn't we check them?\n\n> remove unneeded object creation in newException() calls in releases > 10.10\n> ---------------------------------------------------------------------------\n>\n>                 Key: DERBY-6391\n>                 URL: https://issues.apache.org/jira/browse/DERBY-6391\n>             Project: Derby\n>          Issue Type: Improvement\n>          Components: SQL, Store\n>    Affects Versions: 10.11.1.1\n>            Reporter: Mike Matrigali\n>            Assignee: Danoja Dias\n>              Labels: derby_backport_reject_10_10\n>         Attachments: Derby-6391.diff\n>\n>\n> In releases after 10.10 the code has been converted to use new \n> java language features.  One of the benefits I just noticed is that\n> arguments to StandardException.newException() no longer have\n> to be Objects.  I believe this is due to reimplementation using varargs.\n> As an example old code use to have to be written as:\n> throw StandardException.newException(\n>                     SQLState.FILE_BAD_CHECKSUM,\n>                     id,\n>                     new Long(checksum.getValue()),\n>                     new Long(onDiskChecksum),\n>                     pagedataToHexDump(pageData));\n> The only reason for the new Long() calls was to make them Objects so\n> that the call would match up to a hard coded N Object arg version of\n> the newException call.  I believe these conversions to Objects are no\n> longer needed (but formatting of the args might change).\n> There may be code size savings to be had by doing this code\n> rototil.\n> Anyone see a downside to changing the code in this way?\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-07-31T02:38:07Z"
  },
  "patches": [],
  "external_id": "DERBY-6391"
},{
  "_id": {
    "$oid": "5f27b9ed641061285052da3e"
  },
  "message_id": "<JIRA.12736638.1409002534182.26596.1409172419348@arcas>",
  "mailing_list_id": {
    "$oid": "5f27b6ac641061285051d39d"
  },
  "reference_ids": [
    {
      "$oid": "5f27b9d7641061285052d337"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27b9d7641061285052d337"
  },
  "from_id": {
    "$oid": "58bfd0f802ca40f8bf14830f"
  },
  "to_ids": [
    {
      "$oid": "5f27b732af02e2d6de506b8d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PHOENIX-1208) Check for existence of views\n doesn't take into account the fact that SYSTEM.CATALOG could be split\n across regions",
  "body": "\n    [ https://issues.apache.org/jira/browse/PHOENIX-1208?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14112805#comment-14112805 ] \n\nJeffrey Zhong commented on PHOENIX-1208:\n----------------------------------------\n\n[~jfernando_sfdc] In which case did you see SYSTEM.CATALOG is split? \n\n[~giacomotaylor] If SYSTEM.CATALOG is split, we have other issues because we relies on multiple rows update in transaction fashion against SYSTEM.CATALOG and MetaDataRegionObserver won't be singleton anymore. Should we enforce system.catalog not splittable or any other suggestion? \n\n> Check for existence of views doesn't take into account the fact that SYSTEM.CATALOG could be split across regions\n> -----------------------------------------------------------------------------------------------------------------\n>\n>                 Key: PHOENIX-1208\n>                 URL: https://issues.apache.org/jira/browse/PHOENIX-1208\n>             Project: Phoenix\n>          Issue Type: Bug\n>    Affects Versions: 5.0.0, 3.1, 4.1\n>            Reporter: Jan Fernando\n>            Priority: Minor\n>         Attachments: PHOENIX-1208.patch\n>\n>\n> It is possible that when SYSTEM.CATALOG gets very large that it will be split across multiple regions. The parent table metadata is guaranteed via MetaDataSplitPolicy to be in the same region. However, child tenant specific views could end split across multiple regions. The hasViews() method, when checking for the existence of any views, scans only the region the parent table metadata is located in. We should detect whether the views span multiple regions and scan across them all.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-08-27T20:46:59Z"
  },
  "patches": [],
  "external_id": "PHOENIX-1208"
},{
  "_id": {
    "$oid": "5c5804bfe078b00ec4e7473f"
  },
  "message_id": "<JIRA.13081548.1498077772000.191230.1499406120040@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5c58011ce078b00ec4e72770"
  },
  "reference_ids": [
    {
      "$oid": "5c58049ce078b00ec4e7460e"
    },
    {
      "$oid": "5c58049ce078b00ec4e7460d"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5c58049ce078b00ec4e7460d"
  },
  "from_id": {
    "$oid": "5c580148621a9a77b3bd673c"
  },
  "to_ids": [
    {
      "$oid": "5bbdf31357674ee1677dbf18"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (RANGER-1661) Default policy for KMS audits is\n pointing to incorrect location",
  "body": "\n    [ https://issues.apache.org/jira/browse/RANGER-1661?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16077628#comment-16077628 ] \n\nbhavik patel commented on RANGER-1661:\n--------------------------------------\n\n[~abajwa] I have checked on *master* as well as on *ranger-0.7* branches the default policy is pointing to \"/ranger/audit/kms\".\n\n> Default policy for KMS audits is pointing to incorrect location\n> ---------------------------------------------------------------\n>\n>                 Key: RANGER-1661\n>                 URL: https://issues.apache.org/jira/browse/RANGER-1661\n>             Project: Ranger\n>          Issue Type: Bug\n>          Components: Ranger\n>    Affects Versions: 0.7.0\n>            Reporter: Ali Bajwa\n>             Fix For: 1.0.0, 0.7.2\n>\n>         Attachments: Screen Shot 2017-06-21 at 12.01.26 PM.png, Screen Shot 2017-06-21 at 12.01.43 PM.png\n>\n>\n> After installing Ranger, there is policy for KMS audits already created in HDFS policies...but it seems to be pointing incorrect location. It points to /ranger/kms/audit but Ambari defaults xasecure.audit.destination.hdfs.dir to hdfs://myhost:8020/ranger/audit\n> I believe the default policy should also point to /ranger/audit/kms\n> Env used: HDP 2.6.1.0-129\n> $ rpm -qa | grep ranger\n> ranger_2_6_1_0_129-admin-0.7.0.2.6.1.0-129.x86_64\n> $ rpm -qa | grep ambari\n> ambari-agent-2.5.0.3-7.x86_64\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-07-07T05:42:00Z"
  },
  "patches": [],
  "external_id": "RANGER-1661"
},{
  "_id": {
    "$oid": "5bea9f509e73d744d4132c14"
  },
  "message_id": "<1317271324.1127331814614.JavaMail.jira@ajax.apache.org>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "from_id": {
    "$oid": "5bea9e7135e3ea2b7b5df5e7"
  },
  "subject": "[jira] Created: (DERBY-583) networkserver does not return error message strings in native encoding to ij on zseries",
  "date": {
    "$date": "2005-09-21T21:43:34Z"
  },
  "body": "networkserver does not return error message strings in native encoding to ij on zseries\n---------------------------------------------------------------------------------------\n\n         Key: DERBY-583\n         URL: http://issues.apache.org/jira/browse/DERBY-583\n     Project: Derby\n        Type: Bug\n    Versions: 10.1.1.0    \n Environment: OS/390 (zseries) 1.06, ibm 1.4.2 jre\n    Reporter: Myrna van Lunteren\n     Fix For: 10.2.0.0\n\n\nWhen running ij on an OS/390 without specifying any encoding or locale properties, and starting network server without any locale properties, various error messages do not get converted into the native encoding.\n\nThus the messages are unreadable and not helpful.\n\nThe problem is not limited to ij, just easy to reproduce with ij.\n\nSome errors return readable message strings are returned with jcc, but not with derbyclient, for instance the test lang/forupdate.sql has this line in it:\n  select i from t1, t2 for update;\nwhich on linux & windows results in an error message 42Y90, \"FOR UPDATE is not permitted in this type of statement.\" On OS/390 this error message is readable with jcc, but with derbyclient, only the string 'ERROR' is readable. Other messages - especially those with indirection in it like error 42X04 are equally unreadable with both clients.\n\nTo reproduce 1 situation:\n- start networkserver \n- in another shell window, use the following string to run the in.sql file & save the output for jcc-client and derbyclient respectively:\n----------\njava -Dij.database='jdbc:derby:net://localhost:1527/wombat3;create=true' -Dij.retrieveMessagesFromServerOnGetMessage=true -Dij.protocol=jdbc:derby:net://localhost:1527/ -Dij.driver=com.ibm.db2.jcc.DB2Driver -Dij.user=APP -Dij.password=APP org.apache.derby.tools.ij  in.sql > jcc.out 2>&1\n----------\njava -Dij.database='jdbc:derby://localhost:1527/wombat2;create=true' -Dij.protocol=jdbc:derby://localhost:1527/ -Dij.driver=org.apache.derby.jdbc.ClientDriver -Dij.user=APP -Dij.password=APP org.apache.derby.tools.ij in.sql > client.out 2>&1\n----------\n\nin.sql contains:\n----------\ndrop table a;\ncreate table a (a int);\ninsert into a values (2342323423);\ndisconnect;\nexit;\n----------\n\nI'll attach a jar with the .out files and in.sql. This means that the out files are in EBCDIC, to read them, one needs to run them through native2ascii -encoding Cp1047.\n\n\n\n\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://issues.apache.org/jira/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n",
  "external_id": "DERBY-583"
},{
  "_id": {
    "$oid": "60fd7ad336c61279ee0ba883"
  },
  "message_id": "<JIRA.13089490.1500897837000.314704.1500897840282@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "60fd79fc36c61279ee0b865d"
  },
  "reference_ids": [
    {
      "$oid": "60fd7ad336c61279ee0ba881"
    },
    {
      "$oid": "60fd7ad336c61279ee0ba882"
    }
  ],
  "in_reply_to_id": {
    "$oid": "60fd7ad336c61279ee0ba881"
  },
  "from_id": {
    "$oid": "60fd7ac5f73e2aa3900008cf"
  },
  "to_ids": [
    {
      "$oid": "5f27c3fcaf02e2d6de73a42d"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (FINERACT-505) Loan Tranche Details should be\n captured in Bulk JLG loan application.",
  "body": "Santosh Math created FINERACT-505:\n-------------------------------------\n\n             Summary: Loan Tranche Details should be captured in Bulk JLG loan application.\n                 Key: FINERACT-505\n                 URL: https://issues.apache.org/jira/browse/FINERACT-505\n             Project: Apache Fineract\n          Issue Type: Improvement\n          Components: Loan\n            Reporter: Santosh Math\n            Assignee: Markus Geiss\n\n\nPresently, there is no fields to capture multi-tranche details in 'Bulk JLG loan application' . Therefore, Bulk JLG loan application doesn't support multi-tranche loan products. The current temperory fix is, the loan products with multi-tranche aren't displaying in drop-down menu of  'Bulk JLG loan application'.\n\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.4.14#64029)\n",
  "date": {
    "$date": "2017-07-24T12:04:00Z"
  },
  "patches": [],
  "external_id": "FINERACT-505"
},{
  "_id": {
    "$oid": "5bc86ed457a11257de56c1fd"
  },
  "message_id": "<JIRA.12643892.1366620656637.200693.1366620795978@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc86e6857a11257de56c063"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc86e6857a11257de56c063"
  },
  "from_id": {
    "$oid": "5bc86d8757674ee167d5d6ba"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-1575) PDFTextStripper sometimes adds\n spaces after a detached one word",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1575?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nVitalie Bureanu updated PDFBOX-1575:\n------------------------------------\n\n    Attachment: example.pdf\n    \n> PDFTextStripper sometimes adds spaces after a detached one word\n> ---------------------------------------------------------------\n>\n>                 Key: PDFBOX-1575\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1575\n>             Project: PDFBox\n>          Issue Type: Bug\n>          Components: Text extraction\n>    Affects Versions: 1.8.1\n>         Environment: Linux 64bit\n>            Reporter: Vitalie Bureanu\n>              Labels: pdfbox, space, whitespace,\n>         Attachments: example.pdf\n>\n>   Original Estimate: 2h\n>  Remaining Estimate: 2h\n>\n> Hello dear developers,\n> I noticed that PDFTextStripper sometimes adds spaces after a completely detached words...\n> For example - if you make text extraction for attached file you will se that PDFTextStripper adds one space after words: \"Qty \" and \"Unit Price \" but not adds after \"Description\" and \"Line Total\".\n> I think this is a bug, because after words \"Qty \" and \"Unit Price \" should not be present the whitespace.\n> Can you please fix it?\n> (see attach)\n> Thank you very much,\n> Vitalie\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-04-22T08:53:15Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1575"
},{
  "_id": {
    "$oid": "5f27d2fa46816ce7cf505ca1"
  },
  "message_id": "<14000373.1127522592771.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d2fa46816ce7cf505c95"
  },
  "from_id": {
    "$oid": "5f27d1d9af02e2d6de9b0784"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Closed: (MPJAVA-40) Make a report with compiler output",
  "body": "     [ http://jira.codehaus.org/browse/MPJAVA-40?page=all ]\n     \nCarlos Sanchez closed MPJAVA-40:\n--------------------------------\n\n     Resolution: Fixed\n    Fix Version: 1.6\n\n> Make a report with compiler output\n> ----------------------------------\n>\n>          Key: MPJAVA-40\n>          URL: http://jira.codehaus.org/browse/MPJAVA-40\n>      Project: maven-java-plugin\n>         Type: New Feature\n>     Reporter: Carlos Sanchez\n>     Assignee: Carlos Sanchez\n>      Fix For: 1.6\n\n>\n>\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2005-09-23T19:43:12Z"
  },
  "patches": [],
  "external_id": "MPJAVA-40"
},{
  "_id": {
    "$oid": "5c57f392149eba7f3821922e"
  },
  "message_id": "<1688476580.15143.1337839000690.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5c57eed8149eba7f3821534a"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "59bfb257f2a4565fe91c1718"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "body": "Shinichiro Abe created CONNECTORS-477:\n-----------------------------------------\n\n             Summary: Support for fullWidthSpace against url\n                 Key: CONNECTORS-477\n                 URL: https://issues.apache.org/jira/browse/CONNECTORS-477\n             Project: ManifoldCF\n          Issue Type: Improvement\n          Components: Web connector\n            Reporter: Shinichiro Abe\n            Priority: Minor\n             Fix For: ManifoldCF 0.6\n\n\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "subject": "[jira] [Created] (CONNECTORS-477) Support for fullWidthSpace\n against url",
  "date": {
    "$date": "2012-05-24T05:56:40Z"
  },
  "from_id": {
    "$oid": "59bfa29ef2a4565fe9fb4894"
  },
  "external_id": "CONNECTORS-477"
},{
  "_id": {
    "$oid": "5bbf0b9835bc96443481a05a"
  },
  "message_id": "<JIRA.12969902.1463372882000.194161.1463372892851@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf090e35bc964434814f5a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0b5c35bc96443481964a"
    },
    {
      "$oid": "5bbf0b5c35bc964434819649"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0b5c35bc964434819649"
  },
  "from_id": {
    "$oid": "5bbf0b8657674ee167398ef1"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdff0"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (STORM-1838) [storm-kafka-client] the resumed\n OffsetEntry goes different",
  "body": "Jifeng Yin created STORM-1838:\n---------------------------------\n\n             Summary: [storm-kafka-client] the resumed OffsetEntry goes different\n                 Key: STORM-1838\n                 URL: https://issues.apache.org/jira/browse/STORM-1838\n             Project: Apache Storm\n          Issue Type: Bug\n            Reporter: Jifeng Yin\n             Fix For: 1.0.1\n\n\nThere are no more new messages after consumer rejoins the group. And it turns out the old OffsetEntry instance kept commitOffsetsForAckedTuples failing.\n\nthe comment \"leave the acked offsets as they were to resume where it left off\" doesn't work for me, but the ackedMsgs goes different.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-05-16T04:28:12Z"
  },
  "patches": [],
  "external_id": "STORM-1838"
},{
  "_id": {
    "$oid": "5f27ccf6442ab9b9860eba6d"
  },
  "message_id": "<JIRA.13191379.1539415452000.92955.1582203540363@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27ccf6442ab9b9860eba6a"
    },
    {
      "$oid": "5f27ccf6442ab9b9860eba6b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27ccf6442ab9b9860eba6a"
  },
  "from_id": {
    "$oid": "5f27ccf0af02e2d6de79b1b9"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Closed] (OOZIE-3368) [fluent-job] CredentialsRetrying\n example does not compile",
  "body": "\n     [ https://issues.apache.org/jira/browse/OOZIE-3368?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nAndras Salamon closed OOZIE-3368.\n---------------------------------\n\nClosing issue; Oozie 5.2.0 is released\n\n> [fluent-job] CredentialsRetrying example does not compile\n> ---------------------------------------------------------\n>\n>                 Key: OOZIE-3368\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-3368\n>             Project: Oozie\n>          Issue Type: Bug\n>          Components: examples, fluent-job\n>    Affects Versions: 5.1.0\n>            Reporter: Daniel Becker\n>            Assignee: Daniel Becker\n>            Priority: Minor\n>             Fix For: 5.2.0\n>\n>         Attachments: OOZIE-3368.1.patch, OOZIE-3368.2.patch\n>\n>\n> The file for org.apache.oozie.example.fluentjob.CredentialsRetrying contains an unused import, com.google.common.collect.Lists, which also causes that the example cannot be compiled the way the readme ([https://github.com/apache/oozie/blob/master/docs/src/site/markdown/DG_FluentJobAPI.md)] describes.\n> If the unused import is removed, the example compiles, though produces an invalid workflow xml.\n\n\n\n--\nThis message was sent by Atlassian Jira\n(v8.3.4#803005)\n",
  "date": {
    "$date": "2020-02-20T12:59:00Z"
  },
  "patches": [],
  "external_id": "OOZIE-3368"
},{
  "_id": {
    "$oid": "5f27d46546816ce7cf50a728"
  },
  "message_id": "<17967568.1087440744537.JavaMail.orion@beaver.codehaus.org>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d46546816ce7cf50a71b"
  },
  "from_id": {
    "$oid": "58c123f102ca40f8bfb1fc2a"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Closed: (MAVEN-1301) rar plugin",
  "body": "Message:\n\n   The following issue has been closed.\n\n   Resolver: dion gillard\n       Date: Wed, 16 Jun 2004 10:52 PM\n\nInitial version committed\n---------------------------------------------------------------------\nView the issue:\n  http://jira.codehaus.org/browse/MAVEN-1301\n\nHere is an overview of the issue:\n---------------------------------------------------------------------\n        Key: MAVEN-1301\n    Summary: rar plugin\n       Type: New Feature\n\n     Status: Closed\n   Priority: Major\n Resolution: FIXED\n\n Original Estimate: Unknown\n Time Spent: Unknown\n  Remaining: Unknown\n\n    Project: maven\n Components: \n             core\n   Versions:\n             1.0\n\n   Assignee: dion gillard\n   Reporter: David Jencks\n\n    Created: Sun, 30 May 2004 6:37 PM\n    Updated: Wed, 16 Jun 2004 10:52 PM\n\nDescription:\nrar plugin.\n\nThis is designed to be using in the same project as j2ee connector source code.  The goal rar:install executed after jar:install or jar:jar will package the projects jar file, the ra.xml deployment descriptors and any other app-server specific deployment descriptors from src/rar/META-INF, and any jars marked with a rar.bundle property into a rar file and install it into the local repository under the rars type.\n\nxdoc goals.xml, navigation.xml and properties.xml can be generated using the appropriate maven commands.  I'm not sure if these are usually checked into cvs since they can be generated.\n\nThis is heavily cribbed from the  ear plugin.\n\nI'd consider writing a test if someone would point out how they work and can be run.\n\nI'd appreciate it if this could be added soon since it is needed in several other projects and I would rather not check it into those projects.\n\n\n---------------------------------------------------------------------\nJIRA INFORMATION:\nThis message is automatically generated by JIRA.\n\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n\nIf you want more information on JIRA, or have a bug to report see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2004-06-16T22:52:24Z"
  },
  "patches": [],
  "external_id": "MAVEN-1301"
},{
  "_id": {
    "$oid": "5bc85dc56e373d4fe81c67b6"
  },
  "message_id": "<161650105.126372.1348622047653.JavaMail.jiratomcat@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84ef46e373d4fe81c0e7f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bc85dc56e373d4fe81c67a7"
  },
  "from_id": {
    "$oid": "58bfd04702ca40f8bf1481e7"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1da"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (FLUME-1611) LogUtils regex can be precompiled",
  "body": "\n    [ https://issues.apache.org/jira/browse/FLUME-1611?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13463442#comment-13463442 ] \n\nBrock Noland commented on FLUME-1611:\n-------------------------------------\n\nGoogle is the most authoritative source! j/k\n                \n> LogUtils regex can be precompiled\n> ---------------------------------\n>\n>                 Key: FLUME-1611\n>                 URL: https://issues.apache.org/jira/browse/FLUME-1611\n>             Project: Flume\n>          Issue Type: Bug\n>          Components: Channel\n>    Affects Versions: v1.3.0\n>            Reporter: Hari Shreedharan\n>            Assignee: Hari Shreedharan\n>             Fix For: v1.3.0\n>\n>         Attachments: FLUME-1611.patch\n>\n>\n\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2012-09-26T12:14:07Z"
  },
  "patches": [],
  "external_id": "FLUME-1611"
},{
  "_id": {
    "$oid": "5bea99a99e73d744d411fe85"
  },
  "message_id": "<1969209316.18559.1338412283899.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5bea96ab9e73d744d41195a1"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5bea995b9e73d744d411ef23"
  },
  "from_id": {
    "$oid": "5bea97f835e3ea2b7b4e2a04"
  },
  "to_ids": [
    {
      "$oid": "59bfa5f3f2a4565fe9018715"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Assigned] (DERBY-5793) Document new\n SYSCS_INVALIDATE_ALL_STATEMENTS procedure",
  "body": "\n     [ https://issues.apache.org/jira/browse/DERBY-5793?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nMamta A. Satoor reassigned DERBY-5793:\n--------------------------------------\n\n    Assignee: Mamta A. Satoor  (was: Kim Haase)\n    \n> Document new SYSCS_INVALIDATE_ALL_STATEMENTS procedure\n> ------------------------------------------------------\n>\n>                 Key: DERBY-5793\n>                 URL: https://issues.apache.org/jira/browse/DERBY-5793\n>             Project: Derby\n>          Issue Type: Sub-task\n>          Components: Documentation\n>            Reporter: Mamta A. Satoor\n>            Assignee: Mamta A. Satoor\n>\n> We should document the procedure that will get added as part of DERBY-5578. The new procedure will invalidate all the stored prepared statements. When such an invalid stored prepared statement is executed next time, it will get recompiled at that point thus generating a new plan for it. This can be a very useful tool from support point of view, especially after upgrades.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-05-30T21:11:23Z"
  },
  "patches": [],
  "external_id": "DERBY-5793"
},{
  "_id": {
    "$oid": "5bbf0c07b79d666cbb2289d0"
  },
  "message_id": "<JIRA.12786485.1427591615000.137223.1453102179819@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf09f8b79d666cbb223678"
    },
    {
      "$oid": "5bbf09f8b79d666cbb223679"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf09f8b79d666cbb223678"
  },
  "from_id": {
    "$oid": "5bbf0a2757674ee16736120d"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-2066) Replace FetchRequest /\n FetchResponse with their org.apache.kafka.common.requests equivalents",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-2066?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15104883#comment-15104883 ] \n\nDavid Jacot commented on KAFKA-2066:\n------------------------------------\n\nIn the current implementation, FetchResponse contains a map of FetchResponsePartitionData where each FetchResponsePartitionData contains a MessageSet. Then, FetchResponseSend is used to directly transfers bytes of each MessageSet to the channel. This is where the \"zero-copy\" happens.\n\nWhereas, in the new FetchResponse, messages are represented as a ByteBuffer which brings all bytes in the user space and therefore breaks the \"zero-copy\". In order to keep it, I propose to use Records instead of a ByteBuffer in FetchResponse and extend the serialization mechanism to support channels. I need to work out the details but, roughly, It would require to:\n\n1) update FetchResponse (and ProduceRequest to stay consistent);\n2) introduce a new Type for Records;\n3) add a new method in Type which work a the channel level with a default implementation which uses the current way; and\n4) add some methods in Records interface for the serialization (basically what is in MessageSet)\n4) make MessageSet inherit from Records to make them interoperable (serialization wise).\n\n[~ijuma], [~granthenke] What do you think?\n\n> Replace FetchRequest / FetchResponse with their org.apache.kafka.common.requests equivalents\n> --------------------------------------------------------------------------------------------\n>\n>                 Key: KAFKA-2066\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-2066\n>             Project: Kafka\n>          Issue Type: Sub-task\n>            Reporter: Gwen Shapira\n>            Assignee: David Jacot\n>\n> Replace FetchRequest / FetchResponse with their org.apache.kafka.common.requests equivalents.\n> Note that they can't be completely removed until we deprecate the SimpleConsumer API (and it will require very careful patchwork for the places where core modules actually use the SimpleConsumer API).\n> This also requires a solution on how to stream from memory-mapped files (similar to what existing code does with FileMessageSet. \n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2016-01-18T07:29:39Z"
  },
  "patches": [],
  "external_id": "KAFKA-2066"
},{
  "_id": {
    "$oid": "5bbf07a4b79d666cbb21b7aa"
  },
  "message_id": "<JIRA.13141914.1519940770000.309774.1519940820146@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf07a4b79d666cbb21b7a9"
    },
    {
      "$oid": "5bbf07a4b79d666cbb21b7a8"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf07a4b79d666cbb21b7a8"
  },
  "from_id": {
    "$oid": "5bbf070c57674ee167302589"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Created] (KAFKA-6604) ReplicaManager should not remove\n partitions on the log dirctory from high watermark checkpoint file",
  "body": "Dong Lin created KAFKA-6604:\n-------------------------------\n\n             Summary: ReplicaManager should not remove partitions on the log dirctory from high watermark checkpoint file\n                 Key: KAFKA-6604\n                 URL: https://issues.apache.org/jira/browse/KAFKA-6604\n             Project: Kafka\n          Issue Type: Bug\n            Reporter: Dong Lin\n            Assignee: Dong Lin\n\n\nCurrently a broker may truncate a partition to log start offset in the following scenario:\n\n- Broker A is restarted after shutdown\n- Controller knows that broker A is started.\n- Som event (e.g. topic deletion) triggered controller to send LeaderAndIsrRequest for partition P1.\n- Broker A receives LeaderAndIsrRequest for partition P1. After the broker receives the first LeaderAndIsrRequest, it will overwrite the HW checkpoint file with all its leader partitions and follower partitions. The checkpoint file will contain only the HW for partition P1.\n- Controller sends broker A a LeaderAndIsrRequest for all its leader and follower partitions.\n- Broker creates ReplicaFetcherThread for its follower partitions, truncates the log to HW, which will be zero for all partitions except P1.\n\nWhen this happens, potentially all logs in the broker will be truncated to log start offset and then the cluster will run with reduced availability for a long time.\n\nThe right solution is to keep the partitions in the high watermark checkpoint file if the partition exists in LogManager.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v7.6.3#76005)\n",
  "date": {
    "$date": "2018-03-01T21:47:00Z"
  },
  "patches": [],
  "external_id": "KAFKA-6604"
},{
  "_id": {
    "$oid": "5bacc52556f6a00b02095b36"
  },
  "message_id": "<E80DCA4D-D43F-4E12-959C-F9F46A871FCA@occamsmachete.com>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc52556f6a00b02095b35"
    },
    {
      "$oid": "5bacc52456f6a00b02095aff"
    },
    {
      "$oid": "5bacc52556f6a00b02095b34"
    },
    {
      "$oid": "5bacc4d156f6a00b0209423a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc52556f6a00b02095b35"
  },
  "from_id": {
    "$oid": "59bfdf5bf2a4565fe93d986a"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    },
    {
      "$oid": "58bfcc62e4f89451f55ce0f1"
    }
  ],
  "cc_ids": [],
  "subject": "Mahout on Spark",
  "body": "New name for a new thread.\n\nA lot of the discussion on MAHOUT-1464 has been around integrating that feature with the Scala DSL. As Saikat says this is of general interest since people seem to agree that this is a good place to integrate efforts.\n\nI’m interested in what I think Dmitriy called data frames. Being a complete noob on Spark I may have gotten this wrong but let me take a shot so he can correct me.\n\nThere are a lot of problems that require a pipeline. The text input pipeline is an example, but almost any input to Mahout requires at least an id translation step. What I though Dmitriy was suggesting was that by avoiding the disk write + read between steps we might get significant speedups. This has many implications, I’m sure.\n\nFor one I think it means the non-serialized objects are being used by multiple parts of the pipeline and so are not subject to “translation”.\n\nDmitriy can you explain more? You mentioned a talk you have given, do you have slides somewhere or a PDF?\n\n\nOn Mar 26, 2014, at 7:15 AM, Ted Dunning <ted.dunning@gmail.com> wrote:\n\nIt would be great to have you.\n\n\n(go ahead and start new threads when appropriate ... better than hijacking)\n\n\nOn Wed, Mar 26, 2014 at 6:00 AM, Hardik Pandya <smarty.juice@gmail.com>wrote:\n\n> Sorry to hijack the thread,\n> \n> this seems like first steps of mahout geeting it to work on spark\n> \n> there are similar efforts going on with R+Spark aka Spark R\n> \n> not sure if this helpos, played with spark ec2 scripts and it brings up\n> multinode cluster using mesos and its configurable - willing to contribute\n> donations for mahout-dev\n> \n> \n> \n> \n> \n> On Sun, Mar 23, 2014 at 11:22 PM, Saikat Kanjilal (JIRA) <jira@apache.org\n>> wrote:\n> \n>> \n>>    [\n>> \n> https://issues.apache.org/jira/browse/MAHOUT-1464?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13944710#comment-13944710\n> ]\n>> \n>> Saikat Kanjilal commented on MAHOUT-1464:\n>> -----------------------------------------\n>> \n>> +1 on Andrew's suggestion on using AWS to do this.  Andrew is it possible\n>> to have a shared account so mahout contributors can use this, I 'd even\n> be\n>> willing to chip in donations :) to have a shared AWS account\n>> \n>>> RowSimilarityJob on Spark\n>>> -------------------------\n>>> \n>>>                Key: MAHOUT-1464\n>>>                URL: https://issues.apache.org/jira/browse/MAHOUT-1464\n>>>            Project: Mahout\n>>>         Issue Type: Improvement\n>>>         Components: Collaborative Filtering\n>>>   Affects Versions: 0.9\n>>>        Environment: hadoop, spark\n>>>           Reporter: Pat Ferrel\n>>>             Labels: performance\n>>>            Fix For: 1.0\n>>> \n>>>        Attachments: MAHOUT-1464.patch, MAHOUT-1464.patch,\n>> MAHOUT-1464.patch\n>>> \n>>> \n>>> Create a version of RowSimilarityJob that runs on Spark. Ssc has a\n>> prototype here: https://gist.github.com/sscdotopen/8314254. This should\n>> be compatible with Mahout Spark DRM DSL so a DRM can be used as input.\n>>> Ideally this would extend to cover MAHOUT-1422 which is a feature\n>> request for RSJ on two inputs to calculate the similarity of rows of one\n>> DRM with those of another. This cross-similarity has several applications\n>> including cross-action recommendations.\n>> \n>> \n>> \n>> --\n>> This message was sent by Atlassian JIRA\n>> (v6.2#6252)\n>> \n> \n\n",
  "date": {
    "$date": "2014-03-26T09:05:02Z"
  },
  "patches": [],
  "external_id": "MAHOUT-1464"
},{
  "_id": {
    "$oid": "60fac4ddd907ab79037f0174"
  },
  "message_id": "<1906126718.1207086804344.JavaMail.jira@brutus>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac4cfd907ab79037efda7"
  },
  "from_id": {
    "$oid": "59bfaa3af2a4565fe90974cd"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Resolved: (DIRSERVER-1103) New human readable normalizers\n need to intelligently figure out how to handle byte[]s (Review needed)",
  "body": "\n     [ https://issues.apache.org/jira/browse/DIRSERVER-1103?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nEmmanuel Lecharny resolved DIRSERVER-1103.\n------------------------------------------\n\n    Resolution: Won't Fix\n\nAfter having reviewed this code, I don't think we should change it. The NormalizerInterceptor should handle such cases.\n\n> New human readable normalizers need to intelligently figure out how to handle byte[]s (Review needed)\n> -----------------------------------------------------------------------------------------------------\n>\n>                 Key: DIRSERVER-1103\n>                 URL: https://issues.apache.org/jira/browse/DIRSERVER-1103\n>             Project: Directory ApacheDS\n>          Issue Type: Bug\n>    Affects Versions: bigbang\n>            Reporter: Alex Karasulu\n>            Assignee: Emmanuel Lecharny\n>             Fix For: 1.5.2\n>\n>\n> Looks like some of the new normalizers like this one do not know how to handle byte[]s.  This is very bad and compare operations will all fail because these normalizers must be passed a byte[] by the compare handling of the server.  Note that CompareContext uses a byte[] for the value being compared regardless of the human readibility of the attributeType's syntax.\n> Example referred to above:\n> http://svn.apache.org/viewvc?view=rev&revision=595845\n> Emmanuel can you take a look at this one for me or at least comment on it - I figure you know best the normalizer you added a while back.  But however I wanted to also ask if there is a better way to solve this than I did above.  I guess I can make the compare operation figure out what to give the normalizer.  Anyways I think we're going to be in a great possition soon when we do all this Value and ServerEntry stuff - these kinds of problems will never happen again.\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2008-04-01T14:53:24Z"
  },
  "patches": [],
  "external_id": "DIRSERVER-1103"
},{
  "_id": {
    "$oid": "5bc8704857a11257de56cffc"
  },
  "message_id": "<255155301.9930.1305286247567.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [],
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "patches": [],
  "subject": "[jira] [Created] (PDFBOX-1010) Image printed with colors inverted",
  "from_id": {
    "$oid": "5bc870ae57674ee167d80b7e"
  },
  "body": "Image printed with colors inverted\n----------------------------------\n\n                 Key: PDFBOX-1010\n                 URL: https://issues.apache.org/jira/browse/PDFBOX-1010\n             Project: PDFBox\n          Issue Type: Bug\n          Components: Utilities\n    Affects Versions: 1.6.0\n         Environment: Windows XP using latest snapshot\n            Reporter: Laurent Chane\n\n\nWhen printing a test PDF page, the colors of one embedded image has its colors inverted (black and white)\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-05-13T11:30:47Z"
  },
  "external_id": "PDFBOX-1010"
},{
  "_id": {
    "$oid": "60fac425d907ab79037ecc98"
  },
  "message_id": "<132507461.46695.1302370385723.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "60fac239d907ab79037e53b7"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "60fac406d907ab79037ec414"
  },
  "from_id": {
    "$oid": "60fac41af73e2aa390cd679f"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c02"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (DIRSTUDIO-732) apache directory studio don't\n support OID Macros",
  "body": "\n    [ https://issues.apache.org/jira/browse/DIRSTUDIO-732?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13017922#comment-13017922 ] \n\nHoward Chu commented on DIRSTUDIO-732:\n--------------------------------------\n\nvery specific to OpenLDAP only.\nto be a way to get incompatible LDAP data in the long run.\nmanagement of internal schemas (like configuration or such), but this is only\ngood for people dealing with elements which *won'* be exposed.\n\nI added OID macros to OpenLDAP because the original X.500 spec uses OID \nmacros, and I view their omission from LDAP as a bug in the LDAP specs. And \nyes, of course they're actually useful too...\n\n-- \n   -- Howard Chu\n   CTO, Symas Corp.           http://www.symas.com\n   Director, Highland Sun     http://highlandsun.com/hyc/\n   Chief Architect, OpenLDAP  http://www.openldap.org/project/\n\n\n> apache directory studio don't support OID Macros\n> ------------------------------------------------\n>\n>                 Key: DIRSTUDIO-732\n>                 URL: https://issues.apache.org/jira/browse/DIRSTUDIO-732\n>             Project: Directory Studio\n>          Issue Type: Bug\n>          Components: studio-schemaeditor\n>    Affects Versions: 1.5.3\n>         Environment: windows XP; Open LDAP 2.4 directory\n>            Reporter: Francois PICHOUD\n>             Fix For: 1.5.4\n>\n>\n> I have 2 attribute that have macro ID : x.y.z:1 and x.y.z:2  \n> ADS just saw the first one. \n> It had been solve on shared project : DIRSHARED-10\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2011-04-09T17:33:05Z"
  },
  "patches": [],
  "external_id": "DIRSTUDIO-732"
},{
  "_id": {
    "$oid": "5f27cf0c442ab9b9860f54ed"
  },
  "message_id": "<JIRA.12697493.1393457257655.113425.1393457365149@arcas>",
  "mailing_list_id": {
    "$oid": "5f27ccc9442ab9b9860eb88a"
  },
  "reference_ids": [
    {
      "$oid": "5f27cf05442ab9b9860f5301"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5f27cf05442ab9b9860f5301"
  },
  "from_id": {
    "$oid": "5f27cefeaf02e2d6de88aa5b"
  },
  "to_ids": [
    {
      "$oid": "58bfd074e4f89451f55ce196"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Assigned] (OOZIE-1713) Avoid creating dummy input file for\n each launcher jobs.",
  "body": "\n     [ https://issues.apache.org/jira/browse/OOZIE-1713?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\npurshotam shah reassigned OOZIE-1713:\n-------------------------------------\n\n    Assignee: purshotam shah\n\n> Avoid creating dummy input file for each launcher jobs.\n> -------------------------------------------------------\n>\n>                 Key: OOZIE-1713\n>                 URL: https://issues.apache.org/jira/browse/OOZIE-1713\n>             Project: Oozie\n>          Issue Type: Bug\n>            Reporter: purshotam shah\n>            Assignee: purshotam shah\n>\n> In current approach we create a dummy input file for each  launcher jobs, which puts extra load on NN.\n> Cleaner approach is to define a Oozie launcher input format which can works without input file.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160)\n",
  "date": {
    "$date": "2014-02-26T23:29:25Z"
  },
  "patches": [],
  "external_id": "OOZIE-1713"
},{
  "_id": {
    "$oid": "5bacb178faaadd76f8a9c15b"
  },
  "message_id": "<JIRA.12835478.1433441140000.58108.1434132360590@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [
    {
      "$oid": "5bacb178faaadd76f8a9c158"
    },
    {
      "$oid": "5bacb178faaadd76f8a9c157"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacb178faaadd76f8a9c157"
  },
  "from_id": {
    "$oid": "5bacb17857674ee167d612b0"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PIG-4599) tar.gz compression doesn't produce\n correct output",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-4599?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14583805#comment-14583805 ] \n\nRavi Prakash commented on PIG-4599:\n-----------------------------------\n\nActually https://stackoverflow.com/questions/10185007/how-to-load-a-tarball-to-pig seems to indicate that *.gz is handled. So the PigTextInputFormat is correctly gunzipping the file. However I don't know if pig handles tar , so the output is the content of the tar file. If tar isn't supported this is expected behavior and we should close this JIRA as invalid\n\n> tar.gz compression doesn't produce correct output\n> -------------------------------------------------\n>\n>                 Key: PIG-4599\n>                 URL: https://issues.apache.org/jira/browse/PIG-4599\n>             Project: Pig\n>          Issue Type: Bug\n>    Affects Versions: 0.12.1\n>            Reporter: Tomas Hudik\n>              Labels: compression, easytest\n>\n> I'm not completely sure whether this is the right place to put this issue since Pig is involved, however, Pig leave decompression of tar.gz to   hadoop-common.\n> How to reproduce the issue: \n> # simple file (file1) with arbitrary text lines put into in1 in HDFS\n> # same file (file1) compressed by tar -cvzf file1.tar.gz file put into in2 in HDFS\n> # issue simple pig commands in pig:\n> {quote}\n> raw = load 'in1/' USING TextLoader AS (line: bytearray);\n> dump raw;\n> {quote}\n> run for both (compressed and uncompressed file)\n> # in case of compressed version you will get strange 1st line\n> {quote}\n> a0000644000570000001440000000002512534073736011260 0ustar loadhadoopusersa\n> ...\n> {quote}\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-06-12T18:06:00Z"
  },
  "patches": [],
  "external_id": "PIG-4599"
},{
  "_id": {
    "$oid": "5f27bd0fa9368823397dafdf"
  },
  "message_id": "<845247067.12540.1317044305403.JavaMail.tomcat@hel.zones.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27bb47a9368823397d2ab3"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27bcf0a9368823397da683"
  },
  "from_id": {
    "$oid": "58bfc90602ca40f8bf147945"
  },
  "to_ids": [
    {
      "$oid": "5f27bce2af02e2d6de5e0382"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (JENA-117) A pure Java version of tdbloader2",
  "body": "\n     [ https://issues.apache.org/jira/browse/JENA-117?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nPaolo Castagna updated JENA-117:\n--------------------------------\n\n    Summary: A pure Java version of tdbloader2  (was: tdbloader2: Java external sorting using binary files vs. UNIX sort over text files)\n\n> A pure Java version of tdbloader2\n> ---------------------------------\n>\n>                 Key: JENA-117\n>                 URL: https://issues.apache.org/jira/browse/JENA-117\n>             Project: Jena\n>          Issue Type: Improvement\n>          Components: TDB\n>            Reporter: Paolo Castagna\n>            Assignee: Paolo Castagna\n>              Labels: performance, tdbloader2\n>         Attachments: TDB_JENA-117_r1171714.patch\n>\n>\n> There is probably a significant performance improvement for tdbloader2 in replacing the UNIX sort over text files with an external sorting pure Java implementation.\n> Since JENA-99 we now have a SortedDataBag which does exactly that.\n>     ThresholdPolicyCount<Tuple<Long>> policy = new ThresholdPolicyCount<Tuple<Long>>(1000000);\n>     SerializationFactory<Tuple<Long>> serializerFactory = new TupleSerializationFactory();\n>     Comparator<Tuple<Long>> comparator = new TupleComparator();\n>     SortedDataBag<Tuple<Long>> sortedDataBag = new SortedDataBag<Tuple<Long>>(policy, serializerFactory, comparator);\n> TupleSerializationFactory greates TupleInputStream|TupleOutputStream which are wrappers around DataInputStream|DataOutputStream. TupleComparator is trivial.\n> Preliminary results seems promising and show that the Java implementation can be faster than UNIX sort since it uses smaller binary files (instead of text files) and it does comparisons of long values rather than strings.\n> An example of ExternalSort which compare SortedDataBag vs. UNIX sort is available here:\n> https://github.com/castagna/tdbloader3/blob/hadoop-0.20.203.0/src/main/java/com/talis/labs/tdb/tdbloader3/dev/ExternalSort.java\n> A further advantage in doing the sorting with Java rather than UNIX sort is that we could stream results directly into the BPlusTreeRewriter rather than on disk and then reading them from disk into the BPlusTreeRewriter.\n> I've not done an experiment yet to see if this is actually a significant improvement.\n> Using compression for intermediate files might help, but more experiments are necessary to establish if it is worthwhile or not.\n\n--\nThis message is automatically generated by JIRA.\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2011-09-26T13:38:25Z"
  },
  "patches": [],
  "external_id": "JENA-117"
},{
  "_id": {
    "$oid": "5bc86c5b57a11257de56aa6c"
  },
  "message_id": "<JIRA.12701170.1394690985729.57501.1394691104172@arcas>",
  "mailing_list_id": {
    "$oid": "5bc84f1e57a11257de55e818"
  },
  "reference_ids": [
    {
      "$oid": "5bc86c5a57a11257de56aa6a"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bc86c5a57a11257de56aa6a"
  },
  "from_id": {
    "$oid": "5bbe0f9657674ee1679862f7"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7c0f"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (PDFBOX-1982) Standardise AcroForm Fields",
  "body": "\n     [ https://issues.apache.org/jira/browse/PDFBOX-1982?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nJohn Hewson updated PDFBOX-1982:\n--------------------------------\n\n    Component/s: AcroForm\n\n> Standardise AcroForm Fields\n> ---------------------------\n>\n>                 Key: PDFBOX-1982\n>                 URL: https://issues.apache.org/jira/browse/PDFBOX-1982\n>             Project: PDFBox\n>          Issue Type: Improvement\n>          Components: AcroForm\n>    Affects Versions: 2.0.0\n>            Reporter: John Hewson\n>\n> While working on adding the patch in PDFBOX-1847 I noticed that the digital signature form field, PDSignature is deprecated, having been replaced by PDSignatureField.\n> Currently some aspects of the fields do not correspond with the PDF specification, in particular the hierarchy of the fields and their naming. There are currently 43 open issues for the AcroForm component and no issues have been closed since 2011, so I've attempted some basic refactoring to give us a clean slate for adding new features and fixing old bugs.\n> Here's the current hierarchy of fields in PDFBox:\n> PDField\n>     PDChoiceButton\n>         PDCheckbox\n>         PDRadioCollection\n>     PDPushButton\n>     PDSignatureField\n>     PDUnknownField\n>     PDVariableText\n>         PDChoiceField\n>         PDTextbox\n> And here's the actual hierarchy from the PDF specification:\n> Field\n>     Button\n>         Check Box\n>         Radio Button\n>         Pushbutton\n>     Text\n>     Choice\n>         List Box\n>         Combo Box\n>     Signature\n> Note that PDPushButton is in the wrong place in the hierarchy and List Box and Combo Box are missing.\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.2#6252)\n",
  "date": {
    "$date": "2014-03-13T06:11:44Z"
  },
  "patches": [],
  "external_id": "PDFBOX-1982"
},{
  "_id": {
    "$oid": "5f27cfdd532b7277349c0def"
  },
  "message_id": "<186669300.39436.1340295823033.JavaMail.jiratomcat@issues-vm>",
  "mailing_list_id": {
    "$oid": "5f27cd4a532b7277349b648f"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27cfdd532b7277349c0dee"
  },
  "from_id": {
    "$oid": "5f27cfbdaf02e2d6de8eabaf"
  },
  "to_ids": [
    {
      "$oid": "58c11a87e4f89451f51d7bff"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (AMQ-3892) NACKed Messages Are Not Available Later",
  "body": "\n     [ https://issues.apache.org/jira/browse/AMQ-3892?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nGuy Allard updated AMQ-3892:\n----------------------------\n\n    Attachment: snack.rb\n\nCode to recreate this problem.\n\n                \n> NACKed Messages Are Not Available Later\n> ---------------------------------------\n>\n>                 Key: AMQ-3892\n>                 URL: https://issues.apache.org/jira/browse/AMQ-3892\n>             Project: ActiveMQ\n>          Issue Type: Bug\n>          Components: stomp\n>    Affects Versions: 5.6.0\n>         Environment: Linux tjjackson 3.0.0-21-generic #35-Ubuntu SMP Fri May 25 17:57:41 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux\n> Ruby stomp client - various versions\n>            Reporter: Guy Allard\n>         Attachments: snack.rb\n>\n>\n> connect\n> send a message\n> Subscribe with ackmode client.\n> Receive a message.\n> NACK it.\n> close connection\n> reestablish connection\n> resubscribe\n> attempt receive - message not available (and it should be)\n> I will attach example code.\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n        \n",
  "date": {
    "$date": "2012-06-21T16:23:43Z"
  },
  "patches": [],
  "external_id": "AMQ-3892"
},{
  "_id": {
    "$oid": "5f27d22146816ce7cf501ebf"
  },
  "message_id": "<72070916.1136939821684.JavaMail.haus-jira@codehaus01.managed.contegix.com>",
  "mailing_list_id": {
    "$oid": "5f27cdca46816ce7cf4f0546"
  },
  "reference_ids": [],
  "in_reply_to_id": {
    "$oid": "5f27d22046816ce7cf501e77"
  },
  "from_id": {
    "$oid": "5bbe410157674ee1674bf19b"
  },
  "to_ids": [
    {
      "$oid": "58bfcbd9e4f89451f55cdfee"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Closed: (MNG-1860) maven.* project references no longer\n resolve in ant tasks.",
  "body": "     [ http://jira.codehaus.org/browse/MNG-1860?page=all ]\n     \nBrett Porter closed MNG-1860:\n-----------------------------\n\n     Resolution: Cannot Reproduce\n    Fix Version:     (was: 2.0.2)\n\n> maven.* project references no longer resolve in ant tasks.\n> ----------------------------------------------------------\n>\n>          Key: MNG-1860\n>          URL: http://jira.codehaus.org/browse/MNG-1860\n>      Project: Maven 2\n>         Type: Bug\n\n>   Components: Ant tasks\n>     Versions: 2.0.1\n>     Reporter: John Casey\n>     Assignee: John Casey\n>     Priority: Blocker\n\n>\n>\n> from original email:\n> The following reference in ant run used to work. Now it resolves to\n> null.null. Did something fundamental change or is this a bug?\n>               <unzip src=\"${maven.build.finalName}.${maven.packaging}\"\n> dest=\"target/classes\"></unzip>       \n>   \n> From follow-up email:\n> changing to pom.* fixes the problem\n\n-- \nThis message is automatically generated by JIRA.\n-\nIf you think it was sent incorrectly contact one of the administrators:\n   http://jira.codehaus.org/secure/Administrators.jspa\n-\nFor more information on JIRA, see:\n   http://www.atlassian.com/software/jira\n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: dev-unsubscribe@maven.apache.org\nFor additional commands, e-mail: dev-help@maven.apache.org\n\n",
  "date": {
    "$date": "2006-01-10T18:37:01Z"
  },
  "patches": [],
  "external_id": "MNG-1860"
},{
  "_id": {
    "$oid": "5bbf0ce3b79d666cbb22acbd"
  },
  "message_id": "<JIRA.12858817.1440557218000.87582.1443423484416@Atlassian.JIRA>",
  "mailing_list_id": {
    "$oid": "5bbf05aab79d666cbb21959a"
  },
  "reference_ids": [
    {
      "$oid": "5bbf0c95b79d666cbb22a01c"
    },
    {
      "$oid": "5bbf0c95b79d666cbb22a01b"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbf0c95b79d666cbb22a01b"
  },
  "from_id": {
    "$oid": "58bfceb002ca40f8bf147f15"
  },
  "to_ids": [
    {
      "$oid": "58bfcbc9e4f89451f55cdfb5"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (KAFKA-2474) Add caching for converted Copycat\n schemas in JSONConverter",
  "body": "\n    [ https://issues.apache.org/jira/browse/KAFKA-2474?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14910109#comment-14910109 ] \n\nASF GitHub Bot commented on KAFKA-2474:\n---------------------------------------\n\nGitHub user ewencp opened a pull request:\n\n    https://github.com/apache/kafka/pull/250\n\n    KAFKA-2474: Add caching of JSON schema conversions to JsonConverter.\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/ewencp/kafka kafka-2474-cache-json-schema-conversions\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/kafka/pull/250.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #250\n    \n----\ncommit b828c60ee1905c8b98ffef1bc48a429439be09fd\nAuthor: Ewen Cheslack-Postava <me@ewencp.org>\nDate:   2015-09-27T23:28:00Z\n\n    KAFKA-2474: Add caching of JSON schema conversions to JsonConverter.\n\n----\n\n\n> Add caching for converted Copycat schemas in JSONConverter\n> ----------------------------------------------------------\n>\n>                 Key: KAFKA-2474\n>                 URL: https://issues.apache.org/jira/browse/KAFKA-2474\n>             Project: Kafka\n>          Issue Type: Sub-task\n>          Components: copycat\n>            Reporter: Ewen Cheslack-Postava\n>            Assignee: Ewen Cheslack-Postava\n>             Fix For: 0.9.0.0\n>\n>\n> From discussion of KAFKA-2367:\n> bq. Caching of conversion of schemas. In the JSON implementation we're including, we're probably being pretty wasteful right now since every record has to translate both the schema and data to JSON. We should definitely be doing some caching here. I think an LRU using an IdentityHashMap should be fine. However, this does assume that connectors are good about reusing schemas (defining them up front, or if they are dynamic they should have their own cache of schemas and be able to detect when they can be reused).\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n",
  "date": {
    "$date": "2015-09-28T06:58:04Z"
  },
  "patches": [],
  "external_id": "KAFKA-2474"
},{
  "_id": {
    "$oid": "5bbef729748b2d53b761f4e4"
  },
  "message_id": "<JIRA.12597912.1341687741933.323934.1368628277098@arcas>",
  "mailing_list_id": {
    "$oid": "5bbef648748b2d53b761c752"
  },
  "reference_ids": [
    {
      "$oid": "5bbef6c5748b2d53b761dce0"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bbef6c5748b2d53b761dce0"
  },
  "from_id": {
    "$oid": "5bbef66f57674ee16769f69b"
  },
  "to_ids": [
    {
      "$oid": "5bbef67e57674ee1676a2fde"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Updated] (DELTASPIKE-228) Make @MessageBundle annotated\n type available via EL",
  "body": "\n     [ https://issues.apache.org/jira/browse/DELTASPIKE-228?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]\n\nGerhard Petracek updated DELTASPIKE-228:\n----------------------------------------\n\n    Fix Version/s:     (was: 0.4-incubating)\n                   0.5\n    \n> Make @MessageBundle annotated type available via EL \n> ----------------------------------------------------\n>\n>                 Key: DELTASPIKE-228\n>                 URL: https://issues.apache.org/jira/browse/DELTASPIKE-228\n>             Project: DeltaSpike\n>          Issue Type: New Feature\n>          Components: I18n-Module, JSF-Module\n>    Affects Versions: 0.2-incubating\n>            Reporter: Thomas Herzog\n>            Assignee: Mark Struberg\n>             Fix For: 0.5\n>\n>\n> After you defined an MessageBundle type, you wanna use it in the views as well without wrapping the type into a @Named annotated cdi bean to be available to use it via EL.\n> It would be fine if the implementation would be created and registrered as an cdi bean at deployment time and therefore available via EL in the views.\n> I think the main usage for the messages is in the views, at least in our usacases.\n> Therefore it would also nice to define the name of the created cdi bean via maybe @MessageContextConfig annotation and default should be the name of the type, but the name of the type could be same, just placed in different packages.\n> If this will be done the developer only has to define his MessageBundle type with the getter for the messages and configuration via annotation if necessary, and use it in the views right away.\n> Regarding to issue DELTASPIKE-223 it would be necessary to think about follwing possible issues.\n> If there would be multiple choices for the convention of the getter methods for the messages defined in the MessageBundle type, there could occur follwing problems.\n> 1. String welcomeTo(); // Key: welcome_to\n> 2. String getWelcomeTo();  // Key: welcome_to with get prefix\n> 3  String getWelcomeTo();  // Key: get_welcome_to\n> @1\n> How will EL resolve the method if called via #{type.welcomeTo} ?\n> As far as i know EL would try to invoke getWelcomeTo() method which could not be found in this case !!\n> @2 and 3\n> How will it be distiguished if get prefix is part of the key or just the start of the getter method? \n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-05-15T14:31:17Z"
  },
  "patches": [],
  "external_id": "DELTASPIKE-228"
},{
  "_id": {
    "$oid": "5f27cdd7014d3531c6cc3b0e"
  },
  "message_id": "<425651536.122471265643327992.JavaMail.jira@brutus.apache.org>",
  "mailing_list_id": {
    "$oid": "5f27cc26014d3531c6cbc3a8"
  },
  "reference_ids": [],
  "from_id": {
    "$oid": "59bf930ff2a4565fe9e7925c"
  },
  "to_ids": [
    {
      "$oid": "58c11930e4f89451f51d7b63"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] Created: (JCR-2487)\n WorkspaceItemStateFactory#createItemStates throws ClassCastException",
  "body": "WorkspaceItemStateFactory#createItemStates throws ClassCastException\n--------------------------------------------------------------------\n\n                 Key: JCR-2487\n                 URL: https://issues.apache.org/jira/browse/JCR-2487\n             Project: Jackrabbit Content Repository\n          Issue Type: Bug\n          Components: jackrabbit-jcr2spi\n    Affects Versions: 2.1.0\n            Reporter: Michael Dürig\n            Assignee: Michael Dürig\n\n\nWhen the first item in the ItemInfo iterator returned by the RepositoryService is a PropertyInfo instead a NodeInfo, a ClassCastException is thrown. This should rather be a ItemNotFoundException.\n\n\n-- \nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n",
  "date": {
    "$date": "2010-02-08T15:35:27Z"
  },
  "patches": [],
  "external_id": "JCR-2487"
},{
  "_id": {
    "$oid": "5bacc64256f6a00b0209ad67"
  },
  "message_id": "<4CA4CE0F.3010100@gatech.edu>",
  "mailing_list_id": {
    "$oid": "5bacc3dc56f6a00b020924d2"
  },
  "reference_ids": [
    {
      "$oid": "5bacc64256f6a00b0209ad66"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacc64256f6a00b0209ad66"
  },
  "from_id": {
    "$oid": "5bacc47057674ee167dca9b2"
  },
  "to_ids": [
    {
      "$oid": "58bfd14ee4f89451f55ce1d9"
    }
  ],
  "cc_ids": [],
  "subject": "Re: [jira] Commented: (MAHOUT-363) Proposal for GSoC 2010 (EigenCuts\n clustering algorithm for Mahout)",
  "body": "  Forgot to mention - Jeff's patch looks good to me. Thanks for tidying \nit up :)\n\nOn 9/30/2010 1:49 PM, Shannon Quinn (JIRA) wrote:\n>      [ https://issues.apache.org/jira/browse/MAHOUT-363?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12916574#action_12916574 ]\n>\n> Shannon Quinn commented on MAHOUT-363:\n> --------------------------------------\n>\n> (apologies for the delay)\n>\n> While I would love to have this patch included in 0.4, I'm not convinced it's ready. There is no form of output just yet; it was something I purposely held back on, since there was so much discussion surrounding the standardization of Mahout's classification/clustering formats, so rather than put in something along the lines of KMeans that would have been pulled out anyway, I essentially left it blank.\n>\n> Also, there's still a logic bug somewhere in the Eigencuts algorithm; the clustering isn't yet correct. I'm rotating this semester with Dr. Chennubhotla, and this is one of our priorities.\n>\n> I will certainly yield to and be 100% behind whatever the community thinks on this issue, but my $0.02 is that pushing it back to 0.5 might be better.\n>\n>> Proposal for GSoC 2010 (EigenCuts clustering algorithm for Mahout)\n>> ------------------------------------------------------------------\n>>\n>>                  Key: MAHOUT-363\n>>                  URL: https://issues.apache.org/jira/browse/MAHOUT-363\n>>              Project: Mahout\n>>           Issue Type: Task\n>>           Components: Clustering\n>>     Affects Versions: 0.3\n>>             Reporter: Shannon Quinn\n>>             Assignee: Shannon Quinn\n>>              Fix For: 0.4\n>>\n>>          Attachments: MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363.patch, MAHOUT-363JE.patch, MAHOUT-363JE.patch\n>>\n>>\n>> Proposal Title: EigenCuts spectral clustering implementation on map/reduce for Apache Mahout (addresses issue Mahout-328)\n>> Student Name: Shannon Quinn\n>> Student E-mail: magsol@gmail.com\n>> Organization/Project:Assigned Mentor:\n>> Proposal Abstract:\n>> Clustering algorithms are advantageous when the number of classes are not known a priori. However, most techniques still require an explicit K to be chosen, and most spectral algorithms' use of piecewise constant approximation of eigenvectors breaks down when the clusters are tightly coupled. EigenCuts[1] solves both these problems by choosing an eigenvector to create a new cluster boundary and iterating until no more edges are cut.\n>> Detailed Description\n>> Clustering techniques are extremely useful unsupervised methods, particularly within my field of computational biology, for situations where the number (and often the characteristics as well) of classes expressed in the data are not known a priori. K-means is a classic technique which, given some K, attempts to label data points within a cluster as a function of their distance (e.g. Euclidean) from the cluster's mean, iterating to convergence.\n>> Another approach is spectral clustering, which models the data as a weighted, undirected graph in some n-dimensional space, and creates a matrix M of transition probabilities between nodes. By computing the eigenvalues and eigenvectors of this matrix, most spectral clustering techniques take advantage of the fact that, for data with loosely coupled clusters, the K leading eigenvectors will identify the roughly piecewise constant regions in the data that correspond to clusters.\n>> However, these techniques all suffer from drawbacks, the two most significant of which are having to choose an arbitrary K a priori, and in the situation of tightly coupled clusters where the piecewise constant approximation on the eigenvectors no longer holds.\n>> The EigenCuts algorithm addresses both these issues. As a type of spectral clustering algorithm it works by constructing a Markov chain representation of the data and computing the eigenvectors and eigenvalues of the transition matrix. Eigenflows, or flow of probability by eigenvector, have an associated half life of flow decay called eigenflow. By perturbing the weights between nodes, it can be observed where bottlenecks exist in the eigenflow's halflife, allowing for the identification of boundaries between clusters. Thus, this algorithm iterates until no more cuts between clusters need to be made, eliminating the need for an a prior K, and conferring the ability to separate tightly coupled clusters.\n>> The only disadvantage of EigenCuts is the need to recompute eigenvectors and eigenvalues at each iterative step, incurring a large computational overhead. This problem can be adequately addressed within the map/reduce framework and on a Hadoop cluster by parallelizing the computation of each eigenvector and its associated eigenvalue. Apache Hama in particular, with its specializations in graph and matrix data, will be crucial in parallelizing the computations of transition matrices and their corresponding eigenvalues and eigenvectors at each iteration.\n>> Since Dr Chennubhotla is currently a member of the faculty at the University of Pittsburgh, I have been in contact with him for the past few weeks, and we both envision and eagerly anticipate continued collaboration over the course of the summer and this project's implementation. His guidance in highlighting the finer points of the underlying theory, coupled with my experience in and knowledge of software engineering, makes this is a project we are both extremely excited about implementing.\n>> Timeline\n>> At the end of each sprint, there should be a concrete, functional deliverable. It may not do much, but what it does will work and have full coverage accompanying unit tests.\n>> Sprint 0 (April 26 - May 23): Work with mentor on any pre-coding tasks - familiarization with and dev deployments of Hadoop and Mahout; reading up on documentation, fine-tuning the project plan and requirements. This part will kick into high gear after May 6 (my last final exam and final academic obligation, prior to the actual graduation ceremony), but likely nothing before April 29 (the day of my thesis defense).\n>> Sprint 1 (2 weeks; May 24 - June 6): Implement basic k-means clustering algorithm and parallelize on Mahout over Hadoop. Preliminary interface allows for dataset selection and visualization of resulting clusters.\n>> Sprint 2 (3 weeks; June 7 - 27): Modify k-means algorithm to spectral clustering. Integrate map/reduce framework via Mahout and take advantage of existing core calculation of transition matrices and associated eigenvectors and eigenvalues.\n>> Sprint 3 (3 weeks; June 28 - July 18): Augment spectral clustering to EigenCuts. Fully parallelize with Mahout. Also, mid-term evaluations.\n>> Sprint 4 (3 weeks; July 19 - August 8): Fix any remaining issues with EigenCuts. Finalize interface for running the algorithm, selecting datasets and visualizing results.\n>> Sprint 5 (1 week; August 9 - 15): Tidy up documentation, write final unit tests, fix outstanding bugs.\n>> Other Information\n>> I am finishing up my last semester as a master's student in computational biology at Carnegie Mellon, prior to beginning the PhD program in CompBio at Carnegie Mellon this coming fall. I have worked extensively with clustering techniques as a master's student, as a significant amount of my work has involved bioimage analysis within Dr Robert Murphy's lab. Previous work has involved using HMMs to detect patterns in tuberculosis genomes and use CLUSTALW to cluster those patterns according to geographic location. My master's thesis involves use of matrix completion to infer linear transformations of proteins and their associated subcellular locations across varying cell conditions (drugs, cell lines, etc).\n>> I unfortunately have limited experience with Apache Mahout and Hadoop, but with an undergraduate computer science degree from Georgia Tech, and after an internship with IBM ExtremeBlue, I feel I am extremely adept at picking up new frameworks quickly.\n>> References\n>> [1] Chakra Chennubhotla and Allan D. Jepson. Half-Lives of EigenFlows for Spectral Clustering. NIPS 2002.\n\n",
  "date": {
    "$date": "2010-09-30T13:51:11Z"
  },
  "patches": [],
  "external_id": "MAHOUT-363"
},{
  "_id": {
    "$oid": "5bacb284faaadd76f8aa0055"
  },
  "message_id": "<JIRA.12623297.1355181881955.212910.1366765813867@arcas>",
  "mailing_list_id": {
    "$oid": "5bacb03efaaadd76f8a99172"
  },
  "reference_ids": [
    {
      "$oid": "5bacb245faaadd76f8a9f195"
    }
  ],
  "in_reply_to_id": {
    "$oid": "5bacb245faaadd76f8a9f195"
  },
  "from_id": {
    "$oid": "5bacb13457674ee167d592f0"
  },
  "to_ids": [
    {
      "$oid": "58bfc94202ca40f8bf147995"
    }
  ],
  "cc_ids": [],
  "subject": "[jira] [Commented] (PIG-3088) Add a builtin udf which removes\n prefixes",
  "body": "\n    [ https://issues.apache.org/jira/browse/PIG-3088?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13639925#comment-13639925 ] \n\nPrashant Kommireddi commented on PIG-3088:\n------------------------------------------\n\nI agree this might be a special case but is extremely common in our production scripts.\nA lot of our jobs run off of data exported from db tables, and a lot of these have common fields (Id, CreatedBy, CreatedDate ...). Just wanted to highlight one needs to be careful in making this the default behavior.\n\n\n                \n> Add a builtin udf which removes prefixes\n> ----------------------------------------\n>\n>                 Key: PIG-3088\n>                 URL: https://issues.apache.org/jira/browse/PIG-3088\n>             Project: Pig\n>          Issue Type: New Feature\n>            Reporter: Jonathan Coveney\n>            Assignee: Jonathan Coveney\n>             Fix For: 0.12\n>\n>         Attachments: PIG-3088-0.patch\n>\n>\n> This is something that I always hear people complaining about. Note that this depends on the FlattenOutput annotation.\n> This UDF supports the following.\n> {code}\n> a = load 'a' as (x1, y1, z1);\n> b = load 'a' as (x2, y2, z2);\n> c = join a by x1, b by x2;\n> describe c;\n> --c: {a::x1: bytearray,a::y1: bytearray,a::z1: bytearray,b::x2: bytearray,b::y2: bytearray,b::z2: bytearray}\n> d = foreach c generate RemovePrefix(*);\n> describe d;\n> --d: {x1: bytearray,y1: bytearray,z1: bytearray,x2: bytearray,y2: bytearray,z2: bytearray}\n> {code}\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n",
  "date": {
    "$date": "2013-04-24T01:10:13Z"
  },
  "patches": [],
  "external_id": "PIG-3088"
}]